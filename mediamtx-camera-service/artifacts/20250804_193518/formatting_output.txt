--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/src/camera_service/main.py	2025-08-04 02:18:43.477278+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/src/camera_service/main.py	2025-08-04 15:35:19.302344+00:00
@@ -32,136 +32,139 @@
 
 
 class ServiceCoordinator:
     """
     Coordinates service lifecycle with proper error handling and cleanup.
-    
+
     Manages startup sequence, graceful shutdown, and cleanup of partially
     initialized state on failure.
     """
-    
+
     def __init__(self):
         self.service_manager: Optional[ServiceManager] = None
         self.logger: Optional[logging.Logger] = None
         self._shutdown_requested = asyncio.Event()
-    
+
     async def startup(self) -> None:
         """
         Initialize all service components with proper error handling.
-        
+
         Raises:
             SystemExit: On unrecoverable startup failure
         """
         try:
             # Step 1: Load configuration
             config = load_config()
-            
+
             # Step 2: Setup logging
             setup_logging(config.logging)
             self.logger = logging.getLogger(__name__)
-            
+
             service_version = get_version()
             self.logger.info(f"Starting MediaMTX Camera Service v{service_version}")
-            
+
             # Step 3: Create service manager
             self.service_manager = ServiceManager(config)
-            
+
             # Step 4: Setup signal handlers for graceful shutdown
             self._setup_signal_handlers()
-            
+
             # Step 5: Start services
             await self.service_manager.start()
-            
+
             self.logger.info("Camera service started successfully")
-            
+
         except KeyboardInterrupt:
             self.logger.info("Received keyboard interrupt during startup")
             raise
         except Exception as e:
             if self.logger:
                 self.logger.error(f"Fatal startup error: {e}", exc_info=True)
             else:
                 # Fallback logging if setup_logging failed
-                print(f"Fatal startup error (logging not available): {e}", file=sys.stderr)
-            
+                print(
+                    f"Fatal startup error (logging not available): {e}", file=sys.stderr
+                )
+
             # Cleanup any partially initialized state
             await self._cleanup_partial_state()
             raise SystemExit(1) from e
-    
+
     async def shutdown(self) -> None:
         """Gracefully shutdown all service components."""
         if self.logger:
             self.logger.info("Initiating graceful shutdown")
-        
+
         if self.service_manager:
             try:
                 await self.service_manager.stop()
                 if self.logger:
                     self.logger.info("Service manager stopped successfully")
             except Exception as e:
                 if self.logger:
-                    self.logger.error(f"Error during service manager shutdown: {e}", exc_info=True)
-        
+                    self.logger.error(
+                        f"Error during service manager shutdown: {e}", exc_info=True
+                    )
+
         if self.logger:
             self.logger.info("Camera service stopped")
-    
+
     async def wait_for_shutdown(self) -> None:
         """Wait for shutdown signal or service completion."""
         if not self.service_manager:
             raise RuntimeError("Service not started")
-        
+
         # Wait for either shutdown signal or service completion
         shutdown_task = asyncio.create_task(self._shutdown_requested.wait())
         service_task = asyncio.create_task(self.service_manager.wait_for_shutdown())
-        
+
         try:
             done, pending = await asyncio.wait(
-                [shutdown_task, service_task],
-                return_when=asyncio.FIRST_COMPLETED
+                [shutdown_task, service_task], return_when=asyncio.FIRST_COMPLETED
             )
-            
+
             # Cancel any remaining tasks
             for task in pending:
                 task.cancel()
                 try:
                     await task
                 except asyncio.CancelledError:
                     pass
-                    
+
         except Exception as e:
             if self.logger:
                 self.logger.error(f"Error waiting for shutdown: {e}", exc_info=True)
-    
+
     def _setup_signal_handlers(self) -> None:
         """Setup signal handlers for graceful shutdown."""
-        if sys.platform == 'win32':
+        if sys.platform == "win32":
             # Windows doesn't support signal handlers in asyncio
             if self.logger:
                 self.logger.warning("Signal handlers not supported on Windows")
             return
-        
+
         def signal_handler(signum: int) -> None:
             """Handle shutdown signals."""
             signal_name = signal.Signals(signum).name
             if self.logger:
                 self.logger.info(f"Received {signal_name} signal")
-            
+
             # Signal shutdown event (thread-safe)
             self._shutdown_requested.set()
-        
+
         try:
             loop = asyncio.get_event_loop()
             for sig in (signal.SIGTERM, signal.SIGINT):
                 loop.add_signal_handler(sig, signal_handler, sig)
-                
+
             if self.logger:
                 self.logger.debug("Signal handlers configured for SIGTERM and SIGINT")
-                
+
         except Exception as e:
             if self.logger:
                 self.logger.warning(f"Failed to setup signal handlers: {e}")
-    
+
     async def _cleanup_partial_state(self) -> None:
         """Cleanup any partially initialized state on startup failure."""
         if self.service_manager:
             try:
                 await self.service_manager.stop()
@@ -171,18 +174,18 @@
 
 
 async def main() -> None:
     """Main application entry point with robust error handling."""
     coordinator = ServiceCoordinator()
-    
+
     try:
         # Initialize and start all services
         await coordinator.startup()
-        
+
         # Wait for shutdown signal or service completion
         await coordinator.wait_for_shutdown()
-        
+
     except KeyboardInterrupt:
         # Handle Ctrl+C gracefully
         if coordinator.logger:
             coordinator.logger.info("Received keyboard interrupt")
     except SystemExit:
@@ -205,6 +208,6 @@
             else:
                 print(f"Shutdown error: {e}", file=sys.stderr)
 
 
 if __name__ == "__main__":
-    asyncio.run(main())
\ No newline at end of file
+    asyncio.run(main())
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/src/camera_service/logging_config.py	2025-08-04 02:43:19.837459+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/src/camera_service/logging_config.py	2025-08-04 15:35:19.408450+00:00
@@ -20,233 +20,247 @@
 
 
 class CorrelationIdFilter(logging.Filter):
     """
     Thread-local correlation ID filter for request tracking.
-    
+
     Automatically adds correlation IDs to log records for distributed
     tracing and debugging support across request boundaries.
     """
-    
+
     def __init__(self):
         super().__init__()
         self._local = threading.local()
-    
+
     def filter(self, record: logging.LogRecord) -> bool:
         """Add correlation ID to log record if available."""
         correlation_id = self.get_correlation_id()
         if correlation_id:
             record.correlation_id = correlation_id
         return True
-    
+
     def set_correlation_id(self, correlation_id: str) -> None:
         """Set correlation ID for the current thread."""
         self._local.correlation_id = correlation_id
-    
+
     def get_correlation_id(self) -> Optional[str]:
         """Get correlation ID for the current thread."""
-        return getattr(self._local, 'correlation_id', None)
+        return getattr(self._local, "correlation_id", None)
 
 
 class JsonFormatter(logging.Formatter):
     """
     JSON formatter for structured logging in production environments.
-    
+
     Outputs log records as JSON objects suitable for log aggregation
     systems and automated processing per architecture AD-8 specification.
     """
-    
+
     def format(self, record: logging.LogRecord) -> str:
         """Format log record as JSON."""
         log_entry = {
-            'timestamp': self.formatTime(record, self.datefmt),
-            'level': record.levelname,
-            'logger': record.name,
-            'message': record.getMessage(),
-            'module': record.module,
-            'function': record.funcName,
-            'line': record.lineno
+            "timestamp": self.formatTime(record, self.datefmt),
+            "level": record.levelname,
+            "logger": record.name,
+            "message": record.getMessage(),
+            "module": record.module,
+            "function": record.funcName,
+            "line": record.lineno,
         }
-        
+
         # Add correlation ID if present
-        if hasattr(record, 'correlation_id'):
-            log_entry['correlation_id'] = record.correlation_id
-        
+        if hasattr(record, "correlation_id"):
+            log_entry["correlation_id"] = record.correlation_id
+
         # Add exception info if present
         if record.exc_info:
-            log_entry['exception'] = self.formatException(record.exc_info)
-        
+            log_entry["exception"] = self.formatException(record.exc_info)
+
         # Add any extra fields
         for key, value in record.__dict__.items():
-            if key not in ['name', 'msg', 'args', 'levelname', 'levelno', 
-                          'pathname', 'filename', 'module', 'lineno', 
-                          'funcName', 'created', 'msecs', 'relativeCreated',
-                          'thread', 'threadName', 'processName', 'process',
-                          'exc_info', 'exc_text', 'stack_info', 'correlation_id']:
+            if key not in [
+                "name",
+                "msg",
+                "args",
+                "levelname",
+                "levelno",
+                "pathname",
+                "filename",
+                "module",
+                "lineno",
+                "funcName",
+                "created",
+                "msecs",
+                "relativeCreated",
+                "thread",
+                "threadName",
+                "processName",
+                "process",
+                "exc_info",
+                "exc_text",
+                "stack_info",
+                "correlation_id",
+            ]:
                 log_entry[key] = value
-        
+
         return json.dumps(log_entry, default=str)
 
 
 class ConsoleFormatter(logging.Formatter):
     """
     Human-readable formatter for development environments.
-    
+
     Provides clear, readable log output suitable for console viewing
     during development and debugging per architecture AD-8 specification.
     """
-    
+
     def format(self, record: logging.LogRecord) -> str:
         """Format log record for console display."""
         # Add correlation ID to the format if present
         correlation_part = ""
-        if hasattr(record, 'correlation_id'):
+        if hasattr(record, "correlation_id"):
             correlation_part = f"[{record.correlation_id}] "
-        
+
         # Format: timestamp - logger - level - [correlation_id] message
         formatted = super().format(record)
         if correlation_part:
             # Insert correlation ID after the level
-            parts = formatted.split(' - ', 3)
+            parts = formatted.split(" - ", 3)
             if len(parts) >= 4:
                 formatted = f"{parts[0]} - {parts[1]} - {parts[2]} - {correlation_part}{parts[3]}"
-        
+
         return formatted
 
 
 def _parse_file_size(size_str: str) -> int:
     """
     Parse file size string to bytes.
-    
+
     Args:
         size_str: Size string like '10MB', '500KB', '1GB'
-        
+
     Returns:
         Size in bytes
-        
+
     Raises:
         ValueError: If size string format is invalid
     """
     # TODO: MEDIUM: Add comprehensive size parsing validation [Story:S14]
     size_str = size_str.upper().strip()
-    
+
     # Extract number and unit
-    match = re.match(r'^(\d+(?:\.\d+)?)\s*([KMGT]?B?)$', size_str)
+    match = re.match(r"^(\d+(?:\.\d+)?)\s*([KMGT]?B?)$", size_str)
     if not match:
         raise ValueError(f"Invalid file size format: {size_str}")
-    
+
     number, unit = match.groups()
     number = float(number)
-    
+
     # Convert to bytes
     multipliers = {
-        'B': 1,
-        'KB': 1024,
-        'MB': 1024 ** 2,
-        'GB': 1024 ** 3,
-        'TB': 1024 ** 4,
-        '': 1  # No unit means bytes
+        "B": 1,
+        "KB": 1024,
+        "MB": 1024**2,
+        "GB": 1024**3,
+        "TB": 1024**4,
+        "": 1,  # No unit means bytes
     }
-    
+
     return int(number * multipliers.get(unit, 1))
 
 
 def setup_logging(config: LoggingConfig, development_mode: bool = None) -> None:
     """
     Initialize logging configuration for the camera service.
-    
+
     Args:
         config: Logging configuration object containing all settings
         development_mode: If True, use console-friendly logging.
                          If False, use structured JSON logging.
                          If None, determine from environment or config.
-    
+
     This function configures the root logger and sets up appropriate
     formatters, handlers, and filters based on the environment per AD-8.
     """
     # Determine logging mode
     if development_mode is None:
         development_mode = (
-            os.getenv('CAMERA_SERVICE_ENV', 'production').lower() == 'development'
-            or os.getenv('LOG_FORMAT', 'json').lower() == 'console'
+            os.getenv("CAMERA_SERVICE_ENV", "production").lower() == "development"
+            or os.getenv("LOG_FORMAT", "json").lower() == "console"
         )
-    
+
     # Clear any existing handlers
     root_logger = logging.getLogger()
     for handler in root_logger.handlers[:]:
         root_logger.removeHandler(handler)
-    
+
     # Set logging level with graceful fallback
     try:
         level = getattr(logging, config.level.upper(), logging.INFO)
     except AttributeError:
         level = logging.INFO
-        
+
     root_logger.setLevel(level)
-    
+
     # Create correlation ID filter
     correlation_filter = CorrelationIdFilter()
-    
+
     # Setup console handler
     console_handler = logging.StreamHandler(sys.stdout)
     console_handler.addFilter(correlation_filter)
-    
+
     if development_mode:
         # Development: Human-readable console format
         console_formatter = ConsoleFormatter(
-            fmt=config.format,
-            datefmt='%Y-%m-%d %H:%M:%S'
+            fmt=config.format, datefmt="%Y-%m-%d %H:%M:%S"
         )
     else:
         # Production: JSON format for structured logging
-        console_formatter = JsonFormatter(datefmt='%Y-%m-%dT%H:%M:%S')
-    
+        console_formatter = JsonFormatter(datefmt="%Y-%m-%dT%H:%M:%S")
+
     console_handler.setFormatter(console_formatter)
     root_logger.addHandler(console_handler)
-    
+
     # Setup file handler with rotation if enabled
     if config.file_enabled and config.file_path:
         # Ensure log directory exists
         log_path = Path(config.file_path)
         log_path.parent.mkdir(parents=True, exist_ok=True)
-        
+
         # Create rotating file handler with configuration-based rotation
         try:
             max_bytes = _parse_file_size(config.max_file_size)
             file_handler = logging.handlers.RotatingFileHandler(
-                config.file_path,
-                maxBytes=max_bytes,
-                backupCount=config.backup_count
+                config.file_path, maxBytes=max_bytes, backupCount=config.backup_count
             )
         except (ValueError, AttributeError) as e:
             # Fallback to basic file handler if rotation config is invalid
             # TODO: LOW: Log rotation configuration validation warning [Story:S14]
             file_handler = logging.FileHandler(config.file_path)
-            
+
         file_handler.addFilter(correlation_filter)
-        
+
         # Use JSON format for file logging in production, console for development
         if development_mode:
             file_formatter = ConsoleFormatter(
-                fmt=config.format,
-                datefmt='%Y-%m-%d %H:%M:%S'
+                fmt=config.format, datefmt="%Y-%m-%d %H:%M:%S"
             )
         else:
-            file_formatter = JsonFormatter(datefmt='%Y-%m-%dT%H:%M:%S')
-        
+            file_formatter = JsonFormatter(datefmt="%Y-%m-%dT%H:%M:%S")
+
         file_handler.setFormatter(file_formatter)
         file_handler.setLevel(level)
         root_logger.addHandler(file_handler)
 
 
 def get_correlation_filter() -> Optional[CorrelationIdFilter]:
     """
     Get the correlation ID filter from the root logger.
-    
+
     Returns:
         The CorrelationIdFilter instance if found, None otherwise.
-        
+
     This function allows other parts of the application to access
     the correlation filter for setting request-specific correlation IDs.
     """
     root_logger = logging.getLogger()
     for handler in root_logger.handlers:
@@ -257,14 +271,14 @@
 
 
 def set_correlation_id(correlation_id: str) -> bool:
     """
     Set correlation ID for the current thread across all handlers.
-    
+
     Args:
         correlation_id: Correlation ID to set for current thread
-        
+
     Returns:
         True if correlation filter was found and ID was set, False otherwise
     """
     correlation_filter = get_correlation_filter()
     if correlation_filter:
@@ -274,13 +288,13 @@
 
 
 def get_correlation_id() -> Optional[str]:
     """
     Get the current thread's correlation ID.
-    
+
     Returns:
         The correlation ID for the current thread, or None if not set
     """
     correlation_filter = get_correlation_filter()
     if correlation_filter:
         return correlation_filter.get_correlation_id()
-    return None
\ No newline at end of file
+    return None
﻿--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/src/camera_service/config.py	2025-08-04 02:26:31.219650+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/src/camera_service/config.py	2025-08-04 15:35:20.402788+00:00
@@ -15,34 +15,38 @@
 from pathlib import Path
 
 # Optional dependencies
 try:
     import jsonschema
+
     HAS_JSONSCHEMA = True
 except ImportError:
     HAS_JSONSCHEMA = False
 
 try:
     from watchdog.observers import Observer
     from watchdog.events import FileSystemEventHandler
+
     HAS_WATCHDOG = True
 except ImportError:
     HAS_WATCHDOG = False
 
 
 @dataclass
 class ServerConfig:
     """Server configuration settings."""
+
     host: str = "0.0.0.0"
     port: int = 8002
     websocket_path: str = "/ws"
     max_connections: int = 100
 
 
 @dataclass
 class MediaMTXConfig:
     """MediaMTX integration configuration."""
+
     host: str = "localhost"
     api_port: int = 9997
     rtsp_port: int = 8554
     webrtc_port: int = 8889
     hls_port: int = 8888
@@ -52,20 +56,22 @@
 
 
 @dataclass
 class CameraConfig:
     """Camera detection and monitoring configuration."""
+
     poll_interval: float = 0.1
     detection_timeout: float = 1.0
     device_range: List[int] = field(default_factory=lambda: list(range(10)))
     enable_capability_detection: bool = True
     auto_start_streams: bool = False
 
 
 @dataclass
 class LoggingConfig:
     """Logging configuration settings."""
+
     level: str = "INFO"
     format: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
     file_enabled: bool = False
     file_path: str = "/var/log/camera-service/camera-service.log"
     max_file_size: str = "10MB"
@@ -73,28 +79,31 @@
 
 
 @dataclass
 class RecordingConfig:
     """Recording configuration settings."""
+
     auto_record: bool = False
     format: str = "mp4"
     quality: str = "medium"
     max_duration: int = 3600
     cleanup_after_days: int = 30
 
 
 @dataclass
 class SnapshotConfig:
     """Snapshot configuration settings."""
+
     format: str = "jpg"
     quality: int = 85
     cleanup_after_days: int = 7
 
 
 @dataclass
 class Config:
     """Complete service configuration."""
+
     server: ServerConfig = field(default_factory=ServerConfig)
     mediamtx: MediaMTXConfig = field(default_factory=MediaMTXConfig)
     camera: CameraConfig = field(default_factory=CameraConfig)
     logging: LoggingConfig = field(default_factory=LoggingConfig)
     recording: RecordingConfig = field(default_factory=RecordingConfig)
@@ -102,44 +111,44 @@
 
 
 class ConfigManager:
     """
     Configuration manager with environment overrides, validation, and hot reload.
-    
+
     Provides robust configuration loading with graceful fallback behavior,
     comprehensive validation, and safe hot reload functionality.
     """
-    
+
     def __init__(self):
         self._logger = logging.getLogger(__name__)
         self._config: Optional[Config] = None
         self._config_path: Optional[str] = None
         self._update_callbacks: List[Callable[[Config], None]] = []
         self._observer: Optional[Observer] = None
         self._lock = threading.Lock()
         self._default_config = Config()  # Fallback configuration
-        
+
     def load_config(self, config_path: str = None) -> Config:
         """
         Load configuration with environment variable overrides and validation.
-        
+
         Handles missing or malformed configuration files gracefully by falling back
         to default configuration values. Invalid environment overrides are logged
         but do not crash the service.
-        
+
         Args:
             config_path: Path to YAML configuration file
-            
+
         Returns:
             Validated configuration object
-            
+
         Raises:
             ValueError: If configuration validation fails after all fallbacks
         """
         with self._lock:
             config_data = {}
-            
+
             # Try to find and load configuration file
             if config_path is None:
                 try:
                     config_path = self._find_config_file()
                     self._config_path = config_path
@@ -148,440 +157,522 @@
                         "No configuration file found in standard locations, using defaults"
                     )
                     config_path = None
             else:
                 self._config_path = config_path
-            
+
             # Load YAML configuration with fallback
             if config_path:
                 config_data = self._load_yaml_config_safe(config_path)
             else:
                 self._logger.info("Using default configuration")
-            
+
             # Apply environment variable overrides (with error tolerance)
             config_data = self._apply_environment_overrides_safe(config_data)
-            
+
             # Ensure all required sections exist with defaults
             config_data = self._ensure_complete_config(config_data)
-            
+
             # Validate configuration with comprehensive error reporting
             validation_errors = self._validate_config_comprehensive(config_data)
             if validation_errors:
-                error_msg = f"Configuration validation failed:\n" + "\n".join(validation_errors)
+                error_msg = f"Configuration validation failed:\n" + "\n".join(
+                    validation_errors
+                )
                 self._logger.error(error_msg)
                 raise ValueError(error_msg)
-            
+
             # Create configuration object
             self._config = self._create_config_object(config_data)
-            
-            self._logger.info(f"Configuration loaded successfully from {config_path or 'defaults'}")
+
+            self._logger.info(
+                f"Configuration loaded successfully from {config_path or 'defaults'}"
+            )
             return self._config
-    
+
     def update_config(self, updates: Dict[str, Any]) -> Config:
         """
         Update configuration at runtime with validation and safe rollback.
-        
+
         Args:
             updates: Dictionary of configuration updates
-            
+
         Returns:
             Updated configuration object
-            
+
         Raises:
             ValueError: If configuration validation fails
             RuntimeError: If no configuration is currently loaded
         """
         with self._lock:
             if not self._config:
                 raise RuntimeError("Configuration not loaded")
-            
+
             # Create backup of current config for rollback
             backup_data = asdict(self._config)
-            
+
             try:
                 # Apply updates to current config data
                 current_data = asdict(self._config)
                 updated_data = self._merge_config_updates(current_data, updates)
-                
+
                 # Validate updated configuration
                 validation_errors = self._validate_config_comprehensive(updated_data)
                 if validation_errors:
-                    error_msg = f"Configuration update validation failed:\n" + "\n".join(validation_errors)
+                    error_msg = (
+                        f"Configuration update validation failed:\n"
+                        + "\n".join(validation_errors)
+                    )
                     raise ValueError(error_msg)
-                
+
                 # Create new configuration object
                 new_config = self._create_config_object(updated_data)
-                
+
                 # Update current configuration
                 old_config = self._config
                 self._config = new_config
-                
+
                 # Notify callbacks of configuration change
                 self._notify_config_updated(old_config, new_config)
-                
+
                 self._logger.info("Configuration updated successfully")
                 return self._config
-                
+
             except Exception as e:
                 # Rollback to backup configuration on failure
                 self._config = self._create_config_object(backup_data)
                 self._logger.error(f"Configuration update failed, rolled back: {e}")
                 raise ValueError(f"Configuration update failed: {e}")
-    
+
     def start_hot_reload(self) -> None:
         """
         Start hot reload monitoring for configuration file changes.
-        
+
         Uses file system monitoring to detect changes and reload configuration
         with proper validation and rollback on failure.
         """
         if not HAS_WATCHDOG:
-            self._logger.warning("Hot reload not available - watchdog dependency missing")
+            self._logger.warning(
+                "Hot reload not available - watchdog dependency missing"
+            )
             return
-            
+
         if not self._config_path:
             self._logger.warning("Hot reload not started - no configuration file path")
             return
-            
+
         if self._observer:
             self._logger.warning("Hot reload already started")
             return
-        
+
         config_dir = Path(self._config_path).parent
-        
+
         class ConfigFileHandler(FileSystemEventHandler):
             def __init__(self, manager: ConfigManager):
                 self.manager = manager
                 self._last_reload_time = 0
-                
+
             def on_modified(self, event):
-                if not event.is_directory and Path(event.src_path) == Path(self.manager._config_path):
+                if not event.is_directory and Path(event.src_path) == Path(
+                    self.manager._config_path
+                ):
                     # Debounce rapid file changes
                     current_time = time.time()
                     if current_time - self._last_reload_time < 1.0:
                         return
                     self._last_reload_time = current_time
-                    
-                    self.manager._logger.info("Configuration file changed, reloading...")
+
+                    self.manager._logger.info(
+                        "Configuration file changed, reloading..."
+                    )
                     try:
                         # Wait for file write completion
                         self._wait_for_file_stable()
                         self.manager.reload_config()
                     except Exception as e:
                         self.manager._logger.error(f"Hot reload failed: {e}")
-            
+
             def _wait_for_file_stable(self):
                 """Wait for file to be stable (no size changes)."""
                 config_path = Path(self.manager._config_path)
                 if not config_path.exists():
                     return
-                
+
                 last_size = -1
                 stable_checks = 0
                 max_wait = 10  # Maximum 1 second wait
-                
+
                 while stable_checks < 5 and max_wait > 0:
                     try:
                         current_size = config_path.stat().st_size
                         if current_size == last_size:
                             stable_checks += 1
                         else:
                             stable_checks = 0
                             last_size = current_size
-                        
+
                         time.sleep(0.1)
                         max_wait -= 1
                     except OSError:
                         # File might be temporarily unavailable
                         time.sleep(0.1)
                         max_wait -= 1
-        
+
         self._observer = Observer()
-        self._observer.schedule(ConfigFileHandler(self), str(config_dir), recursive=False)
+        self._observer.schedule(
+            ConfigFileHandler(self), str(config_dir), recursive=False
+        )
         self._observer.start()
-        
+
         self._logger.info(f"Hot reload monitoring started for {self._config_path}")
-    
+
     def stop_hot_reload(self) -> None:
         """Stop hot reload monitoring."""
         if self._observer:
             self._observer.stop()
             self._observer.join()
             self._observer = None
             self._logger.info("Hot reload monitoring stopped")
-    
+
     def reload_config(self) -> Config:
         """
         Reload configuration from file with validation and rollback.
-        
+
         Returns:
             Reloaded configuration object
-            
+
         Raises:
             RuntimeError: If no configuration file path is available
         """
         if not self._config_path:
             raise RuntimeError("No configuration file path available for reload")
-        
+
         old_config = self._config
         try:
             new_config = self.load_config(self._config_path)
             if old_config:
                 self._notify_config_updated(old_config, new_config)
             return new_config
         except Exception as e:
             self._logger.error(f"Configuration reload failed: {e}")
             raise
-    
+
     def add_update_callback(self, callback: Callable[[Config], None]) -> None:
         """
         Add callback to be notified of configuration updates.
-        
+
         Args:
             callback: Function to call when configuration changes
         """
         self._update_callbacks.append(callback)
-    
+
     def remove_update_callback(self, callback: Callable[[Config], None]) -> None:
         """Remove configuration update callback."""
         if callback in self._update_callbacks:
             self._update_callbacks.remove(callback)
-    
+
     def get_config(self) -> Optional[Config]:
         """Get current configuration object."""
         return self._config
-    
+
     def _find_config_file(self) -> str:
         """Find configuration file in standard locations."""
         locations = [
             "config/default.yaml",
             "/etc/camera-service/config.yaml",
-            "/opt/camera-service/config/camera-service.yaml"
+            "/opt/camera-service/config/camera-service.yaml",
         ]
-        
+
         for location in locations:
             if os.path.exists(location):
                 return location
-        
+
         raise FileNotFoundError("No configuration file found in standard locations")
-    
+
     def _load_yaml_config_safe(self, config_path: str) -> Dict[str, Any]:
         """
         Load YAML configuration file with error handling and fallback.
-        
+
         Args:
             config_path: Path to configuration file
-            
+
         Returns:
             Configuration dictionary (may be empty on errors)
         """
         if not os.path.exists(config_path):
-            self._logger.warning(f"Configuration file not found: {config_path}, using defaults")
+            self._logger.warning(
+                f"Configuration file not found: {config_path}, using defaults"
+            )
             return {}
-        
+
         try:
-            with open(config_path, 'r') as f:
+            with open(config_path, "r") as f:
                 content = f.read().strip()
                 if not content:
-                    self._logger.warning(f"Configuration file is empty: {config_path}, using defaults")
+                    self._logger.warning(
+                        f"Configuration file is empty: {config_path}, using defaults"
+                    )
                     return {}
-                
+
                 data = yaml.safe_load(content)
                 if data is None:
-                    self._logger.warning(f"Configuration file contains no data: {config_path}, using defaults")
+                    self._logger.warning(
+                        f"Configuration file contains no data: {config_path}, using defaults"
+                    )
                     return {}
-                
+
                 return data if isinstance(data, dict) else {}
         except yaml.YAMLError as e:
             self._logger.error(f"Malformed YAML in {config_path}: {e}, using defaults")
             return {}
         except Exception as e:
-            self._logger.error(f"Failed to load configuration from {config_path}: {e}, using defaults")
+            self._logger.error(
+                f"Failed to load configuration from {config_path}: {e}, using defaults"
+            )
             return {}
-    
-    def _apply_environment_overrides_safe(self, config_data: Dict[str, Any]) -> Dict[str, Any]:
+
+    def _apply_environment_overrides_safe(
+        self, config_data: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """
         Apply environment variable overrides with error tolerance.
-        
+
         Invalid environment variables are logged but do not crash the service.
-        
+
         Args:
             config_data: Base configuration data
-            
+
         Returns:
             Configuration data with valid environment overrides applied
         """
         # Map of environment variable patterns to config paths
         env_mappings = {
-            'CAMERA_SERVICE_SERVER_HOST': ('server', 'host'),
-            'CAMERA_SERVICE_SERVER_PORT': ('server', 'port'),
-            'CAMERA_SERVICE_SERVER_WEBSOCKET_PATH': ('server', 'websocket_path'),
-            'CAMERA_SERVICE_SERVER_MAX_CONNECTIONS': ('server', 'max_connections'),
-            
-            'CAMERA_SERVICE_MEDIAMTX_HOST': ('mediamtx', 'host'),
-            'CAMERA_SERVICE_MEDIAMTX_API_PORT': ('mediamtx', 'api_port'),
-            'CAMERA_SERVICE_MEDIAMTX_RTSP_PORT': ('mediamtx', 'rtsp_port'),
-            'CAMERA_SERVICE_MEDIAMTX_WEBRTC_PORT': ('mediamtx', 'webrtc_port'),
-            'CAMERA_SERVICE_MEDIAMTX_HLS_PORT': ('mediamtx', 'hls_port'),
-            'CAMERA_SERVICE_MEDIAMTX_CONFIG_PATH': ('mediamtx', 'config_path'),
-            'CAMERA_SERVICE_MEDIAMTX_RECORDINGS_PATH': ('mediamtx', 'recordings_path'),
-            'CAMERA_SERVICE_MEDIAMTX_SNAPSHOTS_PATH': ('mediamtx', 'snapshots_path'),
-            
-            'CAMERA_SERVICE_CAMERA_POLL_INTERVAL': ('camera', 'poll_interval'),
-            'CAMERA_SERVICE_CAMERA_DETECTION_TIMEOUT': ('camera', 'detection_timeout'),
-            'CAMERA_SERVICE_CAMERA_ENABLE_CAPABILITY_DETECTION': ('camera', 'enable_capability_detection'),
-            'CAMERA_SERVICE_CAMERA_AUTO_START_STREAMS': ('camera', 'auto_start_streams'),
-            
-            'CAMERA_SERVICE_LOGGING_LEVEL': ('logging', 'level'),
-            'CAMERA_SERVICE_LOGGING_FORMAT': ('logging', 'format'),
-            'CAMERA_SERVICE_LOGGING_FILE_ENABLED': ('logging', 'file_enabled'),
-            'CAMERA_SERVICE_LOGGING_FILE_PATH': ('logging', 'file_path'),
-            'CAMERA_SERVICE_LOGGING_MAX_FILE_SIZE': ('logging', 'max_file_size'),
-            'CAMERA_SERVICE_LOGGING_BACKUP_COUNT': ('logging', 'backup_count'),
-            
-            'CAMERA_SERVICE_RECORDING_AUTO_RECORD': ('recording', 'auto_record'),
-            'CAMERA_SERVICE_RECORDING_FORMAT': ('recording', 'format'),
-            'CAMERA_SERVICE_RECORDING_QUALITY': ('recording', 'quality'),
-            'CAMERA_SERVICE_RECORDING_MAX_DURATION': ('recording', 'max_duration'),
-            'CAMERA_SERVICE_RECORDING_CLEANUP_AFTER_DAYS': ('recording', 'cleanup_after_days'),
-            
-            'CAMERA_SERVICE_SNAPSHOTS_FORMAT': ('snapshots', 'format'),
-            'CAMERA_SERVICE_SNAPSHOTS_QUALITY': ('snapshots', 'quality'),
-            'CAMERA_SERVICE_SNAPSHOTS_CLEANUP_AFTER_DAYS': ('snapshots', 'cleanup_after_days'),
+            "CAMERA_SERVICE_SERVER_HOST": ("server", "host"),
+            "CAMERA_SERVICE_SERVER_PORT": ("server", "port"),
+            "CAMERA_SERVICE_SERVER_WEBSOCKET_PATH": ("server", "websocket_path"),
+            "CAMERA_SERVICE_SERVER_MAX_CONNECTIONS": ("server", "max_connections"),
+            "CAMERA_SERVICE_MEDIAMTX_HOST": ("mediamtx", "host"),
+            "CAMERA_SERVICE_MEDIAMTX_API_PORT": ("mediamtx", "api_port"),
+            "CAMERA_SERVICE_MEDIAMTX_RTSP_PORT": ("mediamtx", "rtsp_port"),
+            "CAMERA_SERVICE_MEDIAMTX_WEBRTC_PORT": ("mediamtx", "webrtc_port"),
+            "CAMERA_SERVICE_MEDIAMTX_HLS_PORT": ("mediamtx", "hls_port"),
+            "CAMERA_SERVICE_MEDIAMTX_CONFIG_PATH": ("mediamtx", "config_path"),
+            "CAMERA_SERVICE_MEDIAMTX_RECORDINGS_PATH": ("mediamtx", "recordings_path"),
+            "CAMERA_SERVICE_MEDIAMTX_SNAPSHOTS_PATH": ("mediamtx", "snapshots_path"),
+            "CAMERA_SERVICE_CAMERA_POLL_INTERVAL": ("camera", "poll_interval"),
+            "CAMERA_SERVICE_CAMERA_DETECTION_TIMEOUT": ("camera", "detection_timeout"),
+            "CAMERA_SERVICE_CAMERA_ENABLE_CAPABILITY_DETECTION": (
+                "camera",
+                "enable_capability_detection",
+            ),
+            "CAMERA_SERVICE_CAMERA_AUTO_START_STREAMS": (
+                "camera",
+                "auto_start_streams",
+            ),
+            "CAMERA_SERVICE_LOGGING_LEVEL": ("logging", "level"),
+            "CAMERA_SERVICE_LOGGING_FORMAT": ("logging", "format"),
+            "CAMERA_SERVICE_LOGGING_FILE_ENABLED": ("logging", "file_enabled"),
+            "CAMERA_SERVICE_LOGGING_FILE_PATH": ("logging", "file_path"),
+            "CAMERA_SERVICE_LOGGING_MAX_FILE_SIZE": ("logging", "max_file_size"),
+            "CAMERA_SERVICE_LOGGING_BACKUP_COUNT": ("logging", "backup_count"),
+            "CAMERA_SERVICE_RECORDING_AUTO_RECORD": ("recording", "auto_record"),
+            "CAMERA_SERVICE_RECORDING_FORMAT": ("recording", "format"),
+            "CAMERA_SERVICE_RECORDING_QUALITY": ("recording", "quality"),
+            "CAMERA_SERVICE_RECORDING_MAX_DURATION": ("recording", "max_duration"),
+            "CAMERA_SERVICE_RECORDING_CLEANUP_AFTER_DAYS": (
+                "recording",
+                "cleanup_after_days",
+            ),
+            "CAMERA_SERVICE_SNAPSHOTS_FORMAT": ("snapshots", "format"),
+            "CAMERA_SERVICE_SNAPSHOTS_QUALITY": ("snapshots", "quality"),
+            "CAMERA_SERVICE_SNAPSHOTS_CLEANUP_AFTER_DAYS": (
+                "snapshots",
+                "cleanup_after_days",
+            ),
         }
-        
+
         overridden_count = 0
         failed_overrides = []
-        
+
         for env_var, (section, setting) in env_mappings.items():
             if env_var in os.environ:
                 env_value = os.environ[env_var]
                 try:
-                    converted_value = self._convert_env_value_safe(env_value, section, setting)
-                    
+                    converted_value = self._convert_env_value_safe(
+                        env_value, section, setting
+                    )
+
                     # Ensure section exists
                     if section not in config_data:
                         config_data[section] = {}
-                    
+
                     config_data[section][setting] = converted_value
                     overridden_count += 1
-                    
-                    self._logger.debug(f"Applied environment override: {section}.{setting} = {converted_value}")
-                    
+
+                    self._logger.debug(
+                        f"Applied environment override: {section}.{setting} = {converted_value}"
+                    )
+
                 except ValueError as e:
                     failed_overrides.append(f"{env_var}: {e}")
-                    self._logger.error(f"Invalid environment variable {env_var}: {e}, using default")
-        
+                    self._logger.error(
+                        f"Invalid environment variable {env_var}: {e}, using default"
+                    )
+
         if overridden_count > 0:
-            self._logger.info(f"Applied {overridden_count} environment variable overrides")
-        
+            self._logger.info(
+                f"Applied {overridden_count} environment variable overrides"
+            )
+
         if failed_overrides:
-            self._logger.warning(f"Ignored {len(failed_overrides)} invalid environment overrides")
-        
+            self._logger.warning(
+                f"Ignored {len(failed_overrides)} invalid environment overrides"
+            )
+
         return config_data
-    
+
     def _convert_env_value_safe(self, value: str, section: str, setting: str) -> Any:
         """
         Convert environment variable string to appropriate type with error handling.
-        
+
         Args:
             value: Environment variable value
             section: Configuration section name
             setting: Configuration setting name
-            
+
         Returns:
             Converted value
-            
+
         Raises:
             ValueError: If conversion fails
         """
         # Boolean values
-        if setting in ['file_enabled', 'enable_capability_detection', 'auto_start_streams', 'auto_record']:
-            return value.lower() in ('true', '1', 'yes', 'on')
-        
+        if setting in [
+            "file_enabled",
+            "enable_capability_detection",
+            "auto_start_streams",
+            "auto_record",
+        ]:
+            return value.lower() in ("true", "1", "yes", "on")
+
         # Integer values with validation
-        if setting in ['port', 'max_connections', 'api_port', 'rtsp_port', 'webrtc_port', 'hls_port', 
-                      'max_duration', 'cleanup_after_days', 'quality', 'backup_count']:
+        if setting in [
+            "port",
+            "max_connections",
+            "api_port",
+            "rtsp_port",
+            "webrtc_port",
+            "hls_port",
+            "max_duration",
+            "cleanup_after_days",
+            "quality",
+            "backup_count",
+        ]:
             try:
                 int_value = int(value)
                 # Additional validation for specific fields
-                if setting.endswith('_port') and (int_value < 1 or int_value > 65535):
-                    raise ValueError(f"Port must be between 1 and 65535, got {int_value}")
-                if setting in ['max_connections', 'max_duration', 'cleanup_after_days', 'backup_count'] and int_value < 0:
+                if setting.endswith("_port") and (int_value < 1 or int_value > 65535):
+                    raise ValueError(
+                        f"Port must be between 1 and 65535, got {int_value}"
+                    )
+                if (
+                    setting
+                    in [
+                        "max_connections",
+                        "max_duration",
+                        "cleanup_after_days",
+                        "backup_count",
+                    ]
+                    and int_value < 0
+                ):
                     raise ValueError(f"Value must be non-negative, got {int_value}")
-                if setting == 'quality' and (int_value < 1 or int_value > 100):
-                    raise ValueError(f"Quality must be between 1 and 100, got {int_value}")
+                if setting == "quality" and (int_value < 1 or int_value > 100):
+                    raise ValueError(
+                        f"Quality must be between 1 and 100, got {int_value}"
+                    )
                 return int_value
             except (ValueError, TypeError) as e:
-                raise ValueError(f"Invalid integer value for {section}.{setting}: {value} ({e})")
-        
+                raise ValueError(
+                    f"Invalid integer value for {section}.{setting}: {value} ({e})"
+                )
+
         # Float values with validation
-        if setting in ['poll_interval', 'detection_timeout']:
+        if setting in ["poll_interval", "detection_timeout"]:
             try:
                 float_value = float(value)
-                if setting == 'poll_interval' and float_value < 0.01:
-                    raise ValueError(f"Poll interval must be at least 0.01 seconds, got {float_value}")
-                if setting == 'detection_timeout' and float_value < 0.1:
-                    raise ValueError(f"Detection timeout must be at least 0.1 seconds, got {float_value}")
+                if setting == "poll_interval" and float_value < 0.01:
+                    raise ValueError(
+                        f"Poll interval must be at least 0.01 seconds, got {float_value}"
+                    )
+                if setting == "detection_timeout" and float_value < 0.1:
+                    raise ValueError(
+                        f"Detection timeout must be at least 0.1 seconds, got {float_value}"
+                    )
                 return float_value
             except (ValueError, TypeError) as e:
-                raise ValueError(f"Invalid float value for {section}.{setting}: {value} ({e})")
-        
+                raise ValueError(
+                    f"Invalid float value for {section}.{setting}: {value} ({e})"
+                )
+
         # String values with validation
-        if setting == 'level':
-            valid_levels = ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']
+        if setting == "level":
+            valid_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
             if value not in valid_levels:
-                raise ValueError(f"Invalid logging level: {value}, must be one of {valid_levels}")
-        
+                raise ValueError(
+                    f"Invalid logging level: {value}, must be one of {valid_levels}"
+                )
+
         return value
-    
+
     def _ensure_complete_config(self, config_data: Dict[str, Any]) -> Dict[str, Any]:
         """
         Ensure all required configuration sections exist with default values.
-        
+
         Args:
             config_data: Partial configuration data
-            
+
         Returns:
             Complete configuration data with defaults filled in
         """
         default_data = asdict(self._default_config)
-        
+
         # Merge with defaults, preserving existing values
         for section_name, section_defaults in default_data.items():
             if section_name not in config_data:
                 config_data[section_name] = section_defaults.copy()
             else:
                 # Fill in missing keys within sections
                 for key, default_value in section_defaults.items():
                     if key not in config_data[section_name]:
                         config_data[section_name][key] = default_value
-        
+
         return config_data
-    
+
     def _validate_config_comprehensive(self, config_data: Dict[str, Any]) -> List[str]:
         """
         Comprehensive configuration validation with error accumulation.
-        
+
         Args:
             config_data: Configuration dictionary to validate
-            
+
         Returns:
             List of validation error messages (empty if valid)
         """
         validation_errors = []
-        
+
         if HAS_JSONSCHEMA:
             try:
                 self._validate_with_jsonschema(config_data)
             except ValueError as e:
                 validation_errors.append(str(e))
         else:
-            validation_errors.extend(self._validate_basic_schema_comprehensive(config_data))
-        
+            validation_errors.extend(
+                self._validate_basic_schema_comprehensive(config_data)
+            )
+
         return validation_errors
-    
+
     def _validate_with_jsonschema(self, config_data: Dict[str, Any]) -> None:
         """Validate configuration using JSON Schema."""
         schema = {
             "type": "object",
             "properties": {
@@ -589,187 +680,225 @@
                     "type": "object",
                     "properties": {
                         "host": {"type": "string", "minLength": 1},
                         "port": {"type": "integer", "minimum": 1, "maximum": 65535},
                         "websocket_path": {"type": "string", "minLength": 1},
-                        "max_connections": {"type": "integer", "minimum": 1}
+                        "max_connections": {"type": "integer", "minimum": 1},
                     },
-                    "required": ["host", "port"]
+                    "required": ["host", "port"],
                 },
                 "mediamtx": {
                     "type": "object",
                     "properties": {
                         "host": {"type": "string", "minLength": 1},
                         "api_port": {"type": "integer", "minimum": 1, "maximum": 65535},
-                        "rtsp_port": {"type": "integer", "minimum": 1, "maximum": 65535},
-                        "webrtc_port": {"type": "integer", "minimum": 1, "maximum": 65535},
+                        "rtsp_port": {
+                            "type": "integer",
+                            "minimum": 1,
+                            "maximum": 65535,
+                        },
+                        "webrtc_port": {
+                            "type": "integer",
+                            "minimum": 1,
+                            "maximum": 65535,
+                        },
                         "hls_port": {"type": "integer", "minimum": 1, "maximum": 65535},
                         "config_path": {"type": "string", "minLength": 1},
                         "recordings_path": {"type": "string", "minLength": 1},
-                        "snapshots_path": {"type": "string", "minLength": 1}
-                    }
+                        "snapshots_path": {"type": "string", "minLength": 1},
+                    },
                 },
                 "camera": {
                     "type": "object",
                     "properties": {
                         "poll_interval": {"type": "number", "minimum": 0.01},
                         "detection_timeout": {"type": "number", "minimum": 0.1},
                         "device_range": {
                             "type": "array",
                             "items": {"type": "integer", "minimum": 0},
-                            "maxItems": 100
+                            "maxItems": 100,
                         },
                         "enable_capability_detection": {"type": "boolean"},
-                        "auto_start_streams": {"type": "boolean"}
-                    }
+                        "auto_start_streams": {"type": "boolean"},
+                    },
                 },
                 "logging": {
                     "type": "object",
                     "properties": {
-                        "level": {"type": "string", "enum": ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]},
+                        "level": {
+                            "type": "string",
+                            "enum": ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
+                        },
                         "format": {"type": "string", "minLength": 1},
                         "file_enabled": {"type": "boolean"},
                         "file_path": {"type": "string", "minLength": 1},
                         "max_file_size": {"type": "string", "minLength": 1},
-                        "backup_count": {"type": "integer", "minimum": 0}
-                    }
+                        "backup_count": {"type": "integer", "minimum": 0},
+                    },
                 },
                 "recording": {
                     "type": "object",
                     "properties": {
                         "auto_record": {"type": "boolean"},
                         "format": {"type": "string", "enum": ["mp4", "mkv", "avi"]},
-                        "quality": {"type": "string", "enum": ["low", "medium", "high"]},
+                        "quality": {
+                            "type": "string",
+                            "enum": ["low", "medium", "high"],
+                        },
                         "max_duration": {"type": "integer", "minimum": 1},
-                        "cleanup_after_days": {"type": "integer", "minimum": 0}
-                    }
+                        "cleanup_after_days": {"type": "integer", "minimum": 0},
+                    },
                 },
                 "snapshots": {
                     "type": "object",
                     "properties": {
                         "format": {"type": "string", "enum": ["jpg", "png", "bmp"]},
                         "quality": {"type": "integer", "minimum": 1, "maximum": 100},
-                        "cleanup_after_days": {"type": "integer", "minimum": 0}
-                    }
-                }
-            }
+                        "cleanup_after_days": {"type": "integer", "minimum": 0},
+                    },
+                },
+            },
         }
-        
+
         try:
             jsonschema.validate(config_data, schema)
         except jsonschema.ValidationError as e:
             raise ValueError(f"Configuration validation failed: {e.message}")
-    
-    def _validate_basic_schema_comprehensive(self, config_data: Dict[str, Any]) -> List[str]:
+
+    def _validate_basic_schema_comprehensive(
+        self, config_data: Dict[str, Any]
+    ) -> List[str]:
         """
         Basic configuration validation without jsonschema dependency.
-        
+
         Args:
             config_data: Configuration dictionary to validate
-            
+
         Returns:
             List of validation error messages
         """
         errors = []
-        
+
         # Validate server section
-        server = config_data.get('server', {})
-        if 'port' in server:
-            port = server['port']
+        server = config_data.get("server", {})
+        if "port" in server:
+            port = server["port"]
             if not isinstance(port, int) or port < 1 or port > 65535:
                 errors.append(f"Invalid server port: {port} (must be integer 1-65535)")
-        
-        if 'max_connections' in server:
-            max_conn = server['max_connections']
+
+        if "max_connections" in server:
+            max_conn = server["max_connections"]
             if not isinstance(max_conn, int) or max_conn < 1:
-                errors.append(f"Invalid max_connections: {max_conn} (must be positive integer)")
-        
+                errors.append(
+                    f"Invalid max_connections: {max_conn} (must be positive integer)"
+                )
+
         # Validate MediaMTX ports
-        mediamtx = config_data.get('mediamtx', {})
-        port_fields = ['api_port', 'rtsp_port', 'webrtc_port', 'hls_port']
+        mediamtx = config_data.get("mediamtx", {})
+        port_fields = ["api_port", "rtsp_port", "webrtc_port", "hls_port"]
         for field in port_fields:
             if field in mediamtx:
                 port = mediamtx[field]
                 if not isinstance(port, int) or port < 1 or port > 65535:
-                    errors.append(f"Invalid MediaMTX {field}: {port} (must be integer 1-65535)")
-        
+                    errors.append(
+                        f"Invalid MediaMTX {field}: {port} (must be integer 1-65535)"
+                    )
+
         # Validate camera settings
-        camera = config_data.get('camera', {})
-        if 'poll_interval' in camera:
-            interval = camera['poll_interval']
+        camera = config_data.get("camera", {})
+        if "poll_interval" in camera:
+            interval = camera["poll_interval"]
             if not isinstance(interval, (int, float)) or interval < 0.01:
-                errors.append(f"Invalid camera poll_interval: {interval} (must be >= 0.01)")
-        
-        if 'detection_timeout' in camera:
-            timeout = camera['detection_timeout']
+                errors.append(
+                    f"Invalid camera poll_interval: {interval} (must be >= 0.01)"
+                )
+
+        if "detection_timeout" in camera:
+            timeout = camera["detection_timeout"]
             if not isinstance(timeout, (int, float)) or timeout < 0.1:
-                errors.append(f"Invalid camera detection_timeout: {timeout} (must be >= 0.1)")
-        
+                errors.append(
+                    f"Invalid camera detection_timeout: {timeout} (must be >= 0.1)"
+                )
+
         # Validate logging level
-        logging_config = config_data.get('logging', {})
-        if 'level' in logging_config:
-            level = logging_config['level']
-            valid_levels = ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']
+        logging_config = config_data.get("logging", {})
+        if "level" in logging_config:
+            level = logging_config["level"]
+            valid_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
             if level not in valid_levels:
-                errors.append(f"Invalid logging level: {level} (must be one of {valid_levels})")
-        
+                errors.append(
+                    f"Invalid logging level: {level} (must be one of {valid_levels})"
+                )
+
         # Validate recording settings
-        recording = config_data.get('recording', {})
-        if 'format' in recording:
-            format_val = recording['format']
-            valid_formats = ['mp4', 'mkv', 'avi']
+        recording = config_data.get("recording", {})
+        if "format" in recording:
+            format_val = recording["format"]
+            valid_formats = ["mp4", "mkv", "avi"]
             if format_val not in valid_formats:
-                errors.append(f"Invalid recording format: {format_val} (must be one of {valid_formats})")
-        
-        if 'quality' in recording:
-            quality = recording['quality']
-            valid_qualities = ['low', 'medium', 'high']
+                errors.append(
+                    f"Invalid recording format: {format_val} (must be one of {valid_formats})"
+                )
+
+        if "quality" in recording:
+            quality = recording["quality"]
+            valid_qualities = ["low", "medium", "high"]
             if quality not in valid_qualities:
-                errors.append(f"Invalid recording quality: {quality} (must be one of {valid_qualities})")
-        
+                errors.append(
+                    f"Invalid recording quality: {quality} (must be one of {valid_qualities})"
+                )
+
         # Validate snapshot settings
-        snapshots = config_data.get('snapshots', {})
-        if 'format' in snapshots:
-            format_val = snapshots['format']
-            valid_formats = ['jpg', 'png', 'bmp']
+        snapshots = config_data.get("snapshots", {})
+        if "format" in snapshots:
+            format_val = snapshots["format"]
+            valid_formats = ["jpg", "png", "bmp"]
             if format_val not in valid_formats:
-                errors.append(f"Invalid snapshot format: {format_val} (must be one of {valid_formats})")
-        
-        if 'quality' in snapshots:
-            quality = snapshots['quality']
+                errors.append(
+                    f"Invalid snapshot format: {format_val} (must be one of {valid_formats})"
+                )
+
+        if "quality" in snapshots:
+            quality = snapshots["quality"]
             if not isinstance(quality, int) or quality < 1 or quality > 100:
-                errors.append(f"Invalid snapshot quality: {quality} (must be integer 1-100)")
-        
+                errors.append(
+                    f"Invalid snapshot quality: {quality} (must be integer 1-100)"
+                )
+
         return errors
-    
+
     def _create_config_object(self, config_data: Dict[str, Any]) -> Config:
         """Create Config object from validated configuration data."""
         return Config(
-            server=ServerConfig(**config_data.get('server', {})),
-            mediamtx=MediaMTXConfig(**config_data.get('mediamtx', {})),
-            camera=CameraConfig(**config_data.get('camera', {})),
-            logging=LoggingConfig(**config_data.get('logging', {})),
-            recording=RecordingConfig(**config_data.get('recording', {})),
-            snapshots=SnapshotConfig(**config_data.get('snapshots', {}))
+            server=ServerConfig(**config_data.get("server", {})),
+            mediamtx=MediaMTXConfig(**config_data.get("mediamtx", {})),
+            camera=CameraConfig(**config_data.get("camera", {})),
+            logging=LoggingConfig(**config_data.get("logging", {})),
+            recording=RecordingConfig(**config_data.get("recording", {})),
+            snapshots=SnapshotConfig(**config_data.get("snapshots", {})),
         )
-    
-    def _merge_config_updates(self, current_data: Dict[str, Any], updates: Dict[str, Any]) -> Dict[str, Any]:
+
+    def _merge_config_updates(
+        self, current_data: Dict[str, Any], updates: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Merge configuration updates into current configuration data."""
         merged = current_data.copy()
-        
+
         for section, section_updates in updates.items():
             if section not in merged:
                 merged[section] = {}
-            
+
             if isinstance(section_updates, dict):
                 merged[section].update(section_updates)
             else:
                 merged[section] = section_updates
-        
+
         return merged
-    
-    def _notify_config_updated(self, old_config: Optional[Config], new_config: Config) -> None:
+
+    def _notify_config_updated(
+        self, old_config: Optional[Config], new_config: Config
+    ) -> None:
         """Notify all callbacks of configuration update."""
         for callback in self._update_callbacks:
             try:
                 callback(new_config)
             except Exception as e:
@@ -781,14 +910,14 @@
 
 
 def load_config(config_path: str = None) -> Config:
     """
     Load configuration from YAML file with environment overrides.
-    
+
     Args:
         config_path: Path to YAML configuration file
-        
+
     Returns:
         Configuration object with all settings
     """
     return _config_manager.load_config(config_path)
 
@@ -798,6 +927,6 @@
     return _config_manager
 
 
 def get_current_config() -> Optional[Config]:
     """Get the current configuration object."""
-    return _config_manager.get_config()
\ No newline at end of file
+    return _config_manager.get_config()
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/src/camera_service/service_manager.py	2025-08-04 14:29:36.404260+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/src/camera_service/service_manager.py	2025-08-04 15:35:21.452186+00:00
@@ -13,109 +13,119 @@
 import uuid
 from typing import Optional, Dict, Any
 
 from .config import Config
 from ..mediamtx_wrapper.controller import MediaMTXController, StreamConfig
-from ..camera_discovery.hybrid_monitor import CameraEventData, CameraEvent, CameraEventHandler
+from ..camera_discovery.hybrid_monitor import (
+    CameraEventData,
+    CameraEvent,
+    CameraEventHandler,
+)
 from ..websocket_server.server import WebSocketJsonRpcServer
 from .logging_config import set_correlation_id, get_correlation_id
 
 
 class HealthMonitor:
     """
     Basic health monitoring component for service health checks.
-    
+
     Provides service health verification and resource monitoring
     as specified in the architecture overview.
     """
-    
+
     def __init__(self, config: Config):
         """Initialize health monitor with configuration."""
         self._config = config
         self._logger = logging.getLogger(__name__)
         self._running = False
         self._health_check_task: Optional[asyncio.Task] = None
-    
+
     async def start(self) -> None:
         """Start health monitoring."""
         if self._running:
             return
-            
+
         correlation_id = get_correlation_id() or "health-monitor-start"
         set_correlation_id(correlation_id)
-        
-        self._logger.info("Starting health monitor", 
-                         extra={'correlation_id': correlation_id})
+
+        self._logger.info(
+            "Starting health monitor", extra={"correlation_id": correlation_id}
+        )
         self._running = True
-        
+
         # Start background health check task
         self._health_check_task = asyncio.create_task(self._health_check_loop())
-        self._logger.debug("Health monitor started successfully",
-                          extra={'correlation_id': correlation_id})
-    
+        self._logger.debug(
+            "Health monitor started successfully",
+            extra={"correlation_id": correlation_id},
+        )
+
     async def stop(self) -> None:
         """Stop health monitoring."""
         if not self._running:
             return
-            
+
         correlation_id = get_correlation_id() or "health-monitor-stop"
         set_correlation_id(correlation_id)
-        
-        self._logger.info("Stopping health monitor",
-                         extra={'correlation_id': correlation_id})
+
+        self._logger.info(
+            "Stopping health monitor", extra={"correlation_id": correlation_id}
+        )
         self._running = False
-        
+
         # Stop background health check task
         if self._health_check_task and not self._health_check_task.done():
             self._health_check_task.cancel()
             try:
                 await self._health_check_task
             except asyncio.CancelledError:
                 pass
-        
-        self._logger.debug("Health monitor stopped",
-                          extra={'correlation_id': correlation_id})
-    
+
+        self._logger.debug(
+            "Health monitor stopped", extra={"correlation_id": correlation_id}
+        )
+
     async def _health_check_loop(self) -> None:
         """Background health monitoring loop."""
         while self._running:
             try:
                 # Perform basic health checks
                 await asyncio.sleep(30)  # Health check interval
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 correlation_id = get_correlation_id() or "health-check-error"
-                self._logger.error(f"Health check error: {e}",
-                                  extra={'correlation_id': correlation_id})
+                self._logger.error(
+                    f"Health check error: {e}", extra={"correlation_id": correlation_id}
+                )
                 await asyncio.sleep(10)  # Shorter wait on error
 
 
 class ServiceManager(CameraEventHandler):
     """
     Main service orchestrator that manages the lifecycle of all camera service components.
-    
+
     The ServiceManager coordinates between the WebSocket JSON-RPC Server, Camera Discovery
     Monitor, MediaMTX Controller, and Health & Monitoring subsystems as defined in the
     architecture overview.
-    
+
     Implements CameraEventHandler to receive camera connect/disconnect events and
     coordinate with MediaMTX stream management.
     """
 
     def __init__(self, config: Config) -> None:
         """
         Initialize the service manager with configuration.
-        
+
         Args:
             config: Configuration object containing all service settings
         """
         self._config = config
         self._logger = logging.getLogger(__name__)
         self._shutdown_event: Optional[asyncio.Event] = None
         self._running = False
-        
+
         # Component references
         self._websocket_server: Optional[WebSocketJsonRpcServer] = None
         self._camera_monitor = None
         self._mediamtx_controller: Optional[MediaMTXController] = None
         self._health_monitor: Optional[HealthMonitor] = None
@@ -126,140 +136,169 @@
         return self._running
 
     async def start(self) -> None:
         """
         Start all service components in the correct order.
-        
+
         Initializes and starts:
         1. MediaMTX Controller
-        2. Camera Discovery Monitor  
+        2. Camera Discovery Monitor
         3. Health & Monitoring
         4. WebSocket JSON-RPC Server
         """
         if self._running:
             return
-            
-        correlation_id = get_correlation_id() or f"service-startup-{uuid.uuid4().hex[:8]}"
+
+        correlation_id = (
+            get_correlation_id() or f"service-startup-{uuid.uuid4().hex[:8]}"
+        )
         set_correlation_id(correlation_id)
-        
-        self._logger.info("Starting camera service components",
-                         extra={'correlation_id': correlation_id})
-        
+
+        self._logger.info(
+            "Starting camera service components",
+            extra={"correlation_id": correlation_id},
+        )
+
         try:
             self._shutdown_event = asyncio.Event()
-            
+
             # Start MediaMTX Controller
             await self._start_mediamtx_controller()
-            
+
             # Start Camera Discovery Monitor
             await self._start_camera_monitor()
-            
+
             # Start Health & Monitoring
             await self._start_health_monitor()
-            
+
             # Start WebSocket JSON-RPC Server
             await self._start_websocket_server()
-            
+
             self._running = True
-            
-            self._logger.info("All camera service components started successfully",
-                             extra={'correlation_id': correlation_id})
-            
-        except Exception as e:
-            self._logger.error(f"Failed to start service components: {e}",
-                             extra={'correlation_id': correlation_id})
+
+            self._logger.info(
+                "All camera service components started successfully",
+                extra={"correlation_id": correlation_id},
+            )
+
+        except Exception as e:
+            self._logger.error(
+                f"Failed to start service components: {e}",
+                extra={"correlation_id": correlation_id},
+            )
             # Cleanup any partially started components
             await self._cleanup_partial_startup()
             raise
 
     async def stop(self) -> None:
         """
         Stop all service components gracefully.
-        
+
         Gracefully stops components in reverse startup order:
         1. WebSocket JSON-RPC Server
         2. Health & Monitoring
         3. Camera Discovery Monitor
         4. MediaMTX Controller
         """
         if not self._running:
             return
-            
-        correlation_id = get_correlation_id() or f"service-shutdown-{uuid.uuid4().hex[:8]}"
+
+        correlation_id = (
+            get_correlation_id() or f"service-shutdown-{uuid.uuid4().hex[:8]}"
+        )
         set_correlation_id(correlation_id)
-        
-        self._logger.info("Stopping camera service components",
-                         extra={'correlation_id': correlation_id})
-        
+
+        self._logger.info(
+            "Stopping camera service components",
+            extra={"correlation_id": correlation_id},
+        )
+
         try:
             # Stop components in reverse order
             await self._stop_websocket_server()
             await self._stop_health_monitor()
             await self._stop_camera_monitor()
             await self._stop_mediamtx_controller()
-            
+
             self._running = False
-            
+
             if self._shutdown_event:
                 self._shutdown_event.set()
-                
-            self._logger.info("All camera service components stopped",
-                             extra={'correlation_id': correlation_id})
-            
-        except Exception as e:
-            self._logger.error(f"Error during service shutdown: {e}",
-                             extra={'correlation_id': correlation_id})
+
+            self._logger.info(
+                "All camera service components stopped",
+                extra={"correlation_id": correlation_id},
+            )
+
+        except Exception as e:
+            self._logger.error(
+                f"Error during service shutdown: {e}",
+                extra={"correlation_id": correlation_id},
+            )
             raise
 
     async def wait_for_shutdown(self) -> None:
         """
         Wait for shutdown signal.
-        
+
         Blocks until the service receives a shutdown signal or stop() is called.
         """
         if not self._shutdown_event:
             raise RuntimeError("Service not started")
-            
+
         await self._shutdown_event.wait()
 
     async def handle_camera_event(self, event_data: CameraEventData) -> None:
         """
         Handle camera connect/disconnect events from the camera monitor.
-        
+
         Coordinates MediaMTX stream configuration updates based on camera events
         with robust error handling and defensive sequencing.
-        
+
         Args:
             event_data: Camera event information including device path and type
         """
-        correlation_id = get_correlation_id() or f"camera-event-{event_data.device_path.split('/')[-1]}-{uuid.uuid4().hex[:8]}"
+        correlation_id = (
+            get_correlation_id()
+            or f"camera-event-{event_data.device_path.split('/')[-1]}-{uuid.uuid4().hex[:8]}"
+        )
         set_correlation_id(correlation_id)
-        
+
         self._logger.info(
             f"Handling camera event: {event_data.event_type.value} - {event_data.device_path}",
             extra={
-                'correlation_id': correlation_id, 
-                'device_path': event_data.device_path,
-                'event_type': event_data.event_type.value
-            }
-        )
-        
+                "correlation_id": correlation_id,
+                "device_path": event_data.device_path,
+                "event_type": event_data.event_type.value,
+            },
+        )
+
         try:
             if event_data.event_type == CameraEvent.CONNECTED:
                 await self._handle_camera_connected(event_data)
             elif event_data.event_type == CameraEvent.DISCONNECTED:
                 await self._handle_camera_disconnected(event_data)
             elif event_data.event_type == CameraEvent.STATUS_CHANGED:
                 await self._handle_camera_status_changed(event_data)
             else:
-                self._logger.warning(f"Unknown camera event type: {event_data.event_type}",
-                                   extra={'correlation_id': correlation_id, 'device_path': event_data.device_path})
-                
-        except Exception as e:
-            self._logger.error(f"Error handling camera event: {e}", 
-                             extra={'correlation_id': correlation_id, 'device_path': event_data.device_path},
-                             exc_info=True)
+                self._logger.warning(
+                    f"Unknown camera event type: {event_data.event_type}",
+                    extra={
+                        "correlation_id": correlation_id,
+                        "device_path": event_data.device_path,
+                    },
+                )
+
+        except Exception as e:
+            self._logger.error(
+                f"Error handling camera event: {e}",
+                extra={
+                    "correlation_id": correlation_id,
+                    "device_path": event_data.device_path,
+                },
+                exc_info=True,
+            )
 
     async def _handle_camera_connected(self, event_data: CameraEventData) -> None:
         """
         Handle camera connection event with robust MediaMTX stream creation and defensive error handling.
 
@@ -269,57 +308,71 @@
         Args:
             event_data: Camera connection event data
         """
         correlation_id = get_correlation_id()
         device_path = event_data.device_path
-        
-        self._logger.debug(f"Creating stream for connected camera: {device_path}",
-                          extra={'correlation_id': correlation_id, 'device_path': device_path})
-        
+
+        self._logger.debug(
+            f"Creating stream for connected camera: {device_path}",
+            extra={"correlation_id": correlation_id, "device_path": device_path},
+        )
+
         # Extract stream name from device path
         stream_name = self._get_stream_name_from_device_path(device_path)
-        
+
         # Get enhanced camera metadata with capability validation
         camera_metadata = await self._get_enhanced_camera_metadata(event_data)
-        
+
         # Defensive guard: Check MediaMTX controller availability
         stream_created = False
         streams_dict = {}
         mediamtx_error = None
-        
+
         if not self._mediamtx_controller:
-            self._logger.warning("MediaMTX controller not available for stream creation",
-                               extra={'correlation_id': correlation_id, 'device_path': device_path})
+            self._logger.warning(
+                "MediaMTX controller not available for stream creation",
+                extra={"correlation_id": correlation_id, "device_path": device_path},
+            )
         else:
             try:
                 # Create stream configuration
                 stream_config = StreamConfig(
                     name=stream_name,
                     source=device_path,
-                    settings={"videoCodec": "h264"}
-                )
-                
+                    settings={"videoCodec": "h264"},
+                )
+
                 # Create stream in MediaMTX
                 await self._mediamtx_controller.create_stream(stream_config)
                 stream_created = True
-                
+
                 # Generate stream URLs for notification
                 streams_dict = {
                     "rtsp": f"rtsp://{self._config.mediamtx.host}:{self._config.mediamtx.rtsp_port}/{stream_name}",
                     "webrtc": f"http://{self._config.mediamtx.host}:{self._config.mediamtx.webrtc_port}/{stream_name}",
-                    "hls": f"http://{self._config.mediamtx.host}:{self._config.mediamtx.hls_port}/{stream_name}"
+                    "hls": f"http://{self._config.mediamtx.host}:{self._config.mediamtx.hls_port}/{stream_name}",
                 }
-                
-                self._logger.debug(f"Successfully created stream: {stream_name}",
-                                 extra={'correlation_id': correlation_id, 'device_path': device_path})
-                
+
+                self._logger.debug(
+                    f"Successfully created stream: {stream_name}",
+                    extra={
+                        "correlation_id": correlation_id,
+                        "device_path": device_path,
+                    },
+                )
+
             except Exception as e:
                 mediamtx_error = str(e)
-                self._logger.error(f"Failed to create MediaMTX stream for {device_path}: {e}",
-                                 extra={'correlation_id': correlation_id, 'device_path': device_path})
+                self._logger.error(
+                    f"Failed to create MediaMTX stream for {device_path}: {e}",
+                    extra={
+                        "correlation_id": correlation_id,
+                        "device_path": device_path,
+                    },
+                )
                 # Continue with notification despite MediaMTX error
-        
+
         # Prepare notification parameters with capability validation context
         notification_params = {
             "device": device_path,
             "status": "CONNECTED",
             "name": camera_metadata["name"],
@@ -327,45 +380,52 @@
             "fps": camera_metadata["fps"],
             "streams": streams_dict,
             # Enhanced metadata with provisional/confirmed state annotations
             "metadata_validation": camera_metadata["validation_status"],
             "metadata_source": camera_metadata["capability_source"],
-            "metadata_provisional": camera_metadata["validation_status"] in ["provisional", "none"],
-            "metadata_confirmed": camera_metadata["validation_status"] == "confirmed"
+            "metadata_provisional": camera_metadata["validation_status"]
+            in ["provisional", "none"],
+            "metadata_confirmed": camera_metadata["validation_status"] == "confirmed",
         }
-        
+
         # Add capability validation context to logging
         validation_status = camera_metadata.get("validation_status", "none")
         capability_source = camera_metadata.get("capability_source", "default")
         consecutive_successes = camera_metadata.get("consecutive_successes", 0)
-        
+
         try:
             self._logger.info(
                 f"Camera connected with {capability_source} metadata: {device_path} "
                 f"({camera_metadata['resolution']}@{camera_metadata['fps']}fps, "
                 f"validation: {validation_status}, confirmations: {consecutive_successes})",
                 extra={
-                    'correlation_id': correlation_id,
-                    'device_path': device_path,
-                    'capability_validation': validation_status,
-                    'capability_source': capability_source,
-                    'capability_confirmations': consecutive_successes,
-                    'stream_created': stream_created,
-                    'mediamtx_error': mediamtx_error
-                }
-            )
-            
+                    "correlation_id": correlation_id,
+                    "device_path": device_path,
+                    "capability_validation": validation_status,
+                    "capability_source": capability_source,
+                    "capability_confirmations": consecutive_successes,
+                    "stream_created": stream_created,
+                    "mediamtx_error": mediamtx_error,
+                },
+            )
+
             # Send notification to all connected clients
             if self._websocket_server:
-                await self._websocket_server.notify_camera_status_update(notification_params)
-            
-            self._logger.info(f"Stream orchestration completed for camera: {device_path}",
-                            extra={'correlation_id': correlation_id, 'device_path': device_path})
-            
-        except Exception as e:
-            self._logger.error(f"Failed to complete camera connection orchestration: {e}",
-                             extra={'correlation_id': correlation_id, 'device_path': device_path})
+                await self._websocket_server.notify_camera_status_update(
+                    notification_params
+                )
+
+            self._logger.info(
+                f"Stream orchestration completed for camera: {device_path}",
+                extra={"correlation_id": correlation_id, "device_path": device_path},
+            )
+
+        except Exception as e:
+            self._logger.error(
+                f"Failed to complete camera connection orchestration: {e}",
+                extra={"correlation_id": correlation_id, "device_path": device_path},
+            )
 
     async def _handle_camera_disconnected(self, event_data: CameraEventData) -> None:
         """
         Handle camera disconnection event with robust MediaMTX stream cleanup.
 
@@ -375,73 +435,93 @@
         Args:
             event_data: Camera disconnection event data
         """
         correlation_id = get_correlation_id()
         device_path = event_data.device_path
-        
-        self._logger.debug(f"Removing stream for disconnected camera: {device_path}",
-                          extra={'correlation_id': correlation_id, 'device_path': device_path})
-        
+
+        self._logger.debug(
+            f"Removing stream for disconnected camera: {device_path}",
+            extra={"correlation_id": correlation_id, "device_path": device_path},
+        )
+
         # Defensive guard: Check MediaMTX controller availability
         stream_removed = False
         mediamtx_error = None
-        
+
         if not self._mediamtx_controller:
-            self._logger.warning("MediaMTX controller not available for stream removal",
-                               extra={'correlation_id': correlation_id, 'device_path': device_path})
+            self._logger.warning(
+                "MediaMTX controller not available for stream removal",
+                extra={"correlation_id": correlation_id, "device_path": device_path},
+            )
         else:
             try:
                 # Extract stream name from device path
                 stream_name = self._get_stream_name_from_device_path(device_path)
-                
+
                 # Delete stream from MediaMTX with error handling
                 await self._mediamtx_controller.delete_stream(stream_name)
                 stream_removed = True
-                
-                self._logger.debug(f"Successfully removed stream: {stream_name}",
-                                 extra={'correlation_id': correlation_id, 'device_path': device_path})
-                
+
+                self._logger.debug(
+                    f"Successfully removed stream: {stream_name}",
+                    extra={
+                        "correlation_id": correlation_id,
+                        "device_path": device_path,
+                    },
+                )
+
             except Exception as e:
                 mediamtx_error = str(e)
-                self._logger.error(f"Failed to delete MediaMTX stream for {device_path}: {e}",
-                                 extra={'correlation_id': correlation_id, 'device_path': device_path})
+                self._logger.error(
+                    f"Failed to delete MediaMTX stream for {device_path}: {e}",
+                    extra={
+                        "correlation_id": correlation_id,
+                        "device_path": device_path,
+                    },
+                )
                 # Continue with notification despite MediaMTX error
-        
+
         # Get camera metadata for notification (uses cached/default for disconnected)
         camera_metadata = await self._get_enhanced_camera_metadata(event_data)
-        
+
         # Prepare camera status notification for disconnection
         notification_params = {
             "device": device_path,
             "status": "DISCONNECTED",
             "name": camera_metadata["name"],
             "resolution": "",  # Empty for disconnected cameras
-            "fps": 0,          # Zero for disconnected cameras
-            "streams": {},     # Empty streams for disconnected cameras
+            "fps": 0,  # Zero for disconnected cameras
+            "streams": {},  # Empty streams for disconnected cameras
             # Metadata context for disconnected state
             "metadata_validation": "none",
             "metadata_source": "cached",
             "metadata_provisional": False,
-            "metadata_confirmed": False
+            "metadata_confirmed": False,
         }
-        
+
         try:
             # Send notification to all connected clients
             if self._websocket_server:
-                await self._websocket_server.notify_camera_status_update(notification_params)
-            
-            self._logger.info(f"Stream removal completed for camera: {device_path}",
-                            extra={
-                                'correlation_id': correlation_id, 
-                                'device_path': device_path,
-                                'stream_removed': stream_removed,
-                                'mediamtx_error': mediamtx_error
-                            })
-            
-        except Exception as e:
-            self._logger.error(f"Failed to complete camera disconnection orchestration: {e}",
-                             extra={'correlation_id': correlation_id, 'device_path': device_path})
+                await self._websocket_server.notify_camera_status_update(
+                    notification_params
+                )
+
+            self._logger.info(
+                f"Stream removal completed for camera: {device_path}",
+                extra={
+                    "correlation_id": correlation_id,
+                    "device_path": device_path,
+                    "stream_removed": stream_removed,
+                    "mediamtx_error": mediamtx_error,
+                },
+            )
+
+        except Exception as e:
+            self._logger.error(
+                f"Failed to complete camera disconnection orchestration: {e}",
+                extra={"correlation_id": correlation_id, "device_path": device_path},
+            )
 
     async def _handle_camera_status_changed(self, event_data: CameraEventData) -> None:
         """
         Handle camera status change event with enhanced state transition logging.
 
@@ -451,24 +531,28 @@
         Args:
             event_data: Camera status change event data
         """
         correlation_id = get_correlation_id()
         device_path = event_data.device_path
-        
+
         old_status = "unknown"
-        new_status = event_data.device_info.status if event_data.device_info else "unknown"
-        
-        self._logger.debug(f"Handling status change for camera: {device_path} ({old_status} -> {new_status})",
-                          extra={'correlation_id': correlation_id, 'device_path': device_path})
-        
+        new_status = (
+            event_data.device_info.status if event_data.device_info else "unknown"
+        )
+
+        self._logger.debug(
+            f"Handling status change for camera: {device_path} ({old_status} -> {new_status})",
+            extra={"correlation_id": correlation_id, "device_path": device_path},
+        )
+
         try:
             # Extract stream name from device path
             stream_name = self._get_stream_name_from_device_path(device_path)
-            
+
             # Get enhanced camera metadata for notification
             camera_metadata = await self._get_enhanced_camera_metadata(event_data)
-            
+
             # Determine notification parameters based on new status
             if event_data.device_info and event_data.device_info.status == "CONNECTED":
                 # Camera is now available - generate stream URLs
                 notification_params = {
                     "device": device_path,
@@ -477,392 +561,466 @@
                     "resolution": camera_metadata["resolution"],
                     "fps": camera_metadata["fps"],
                     "streams": {
                         "rtsp": f"rtsp://{self._config.mediamtx.host}:{self._config.mediamtx.rtsp_port}/{stream_name}",
                         "webrtc": f"http://{self._config.mediamtx.host}:{self._config.mediamtx.webrtc_port}/{stream_name}",
-                        "hls": f"http://{self._config.mediamtx.host}:{self._config.mediamtx.hls_port}/{stream_name}"
+                        "hls": f"http://{self._config.mediamtx.host}:{self._config.mediamtx.hls_port}/{stream_name}",
                     },
                     "metadata_validation": camera_metadata["validation_status"],
                     "metadata_source": camera_metadata["capability_source"],
-                    "metadata_provisional": camera_metadata["validation_status"] in ["provisional", "none"],
-                    "metadata_confirmed": camera_metadata["validation_status"] == "confirmed"
+                    "metadata_provisional": camera_metadata["validation_status"]
+                    in ["provisional", "none"],
+                    "metadata_confirmed": camera_metadata["validation_status"]
+                    == "confirmed",
                 }
-                
+
                 # Log capability validation context for status change
                 validation_status = camera_metadata.get("validation_status", "none")
                 self._logger.info(
                     f"Camera status changed to CONNECTED: {device_path} (validation: {validation_status})",
                     extra={
-                        'correlation_id': correlation_id,
-                        'device_path': device_path,
-                        'capability_validation': validation_status,
-                        'status_transition': f"{old_status}_to_CONNECTED"
-                    }
+                        "correlation_id": correlation_id,
+                        "device_path": device_path,
+                        "capability_validation": validation_status,
+                        "status_transition": f"{old_status}_to_CONNECTED",
+                    },
                 )
             else:
                 # Camera has error or other status
-                error_status = "ERROR" if event_data.device_info and event_data.device_info.status == "ERROR" else "DISCONNECTED"
+                error_status = (
+                    "ERROR"
+                    if event_data.device_info
+                    and event_data.device_info.status == "ERROR"
+                    else "DISCONNECTED"
+                )
                 notification_params = {
                     "device": device_path,
                     "status": error_status,
                     "name": camera_metadata["name"],
                     "resolution": "",
                     "fps": 0,
                     "streams": {},
                     "metadata_validation": "none",
                     "metadata_source": "cached",
                     "metadata_provisional": False,
-                    "metadata_confirmed": False
+                    "metadata_confirmed": False,
                 }
-                
+
                 self._logger.info(
                     f"Camera status changed to {error_status}: {device_path}",
                     extra={
-                        'correlation_id': correlation_id,
-                        'device_path': device_path,
-                        'status_transition': f"{old_status}_to_{error_status}"
-                    }
-                )
-            
+                        "correlation_id": correlation_id,
+                        "device_path": device_path,
+                        "status_transition": f"{old_status}_to_{error_status}",
+                    },
+                )
+
             # Send notification to all connected clients
             if self._websocket_server:
-                await self._websocket_server.notify_camera_status_update(notification_params)
-            
-            self._logger.info(f"Status change notification sent for camera: {device_path}",
-                            extra={'correlation_id': correlation_id, 'device_path': device_path})
-            
-        except Exception as e:
-            self._logger.error(f"Error handling status change for {device_path}: {e}",
-                             extra={'correlation_id': correlation_id, 'device_path': device_path})
+                await self._websocket_server.notify_camera_status_update(
+                    notification_params
+                )
+
+            self._logger.info(
+                f"Status change notification sent for camera: {device_path}",
+                extra={"correlation_id": correlation_id, "device_path": device_path},
+            )
+
+        except Exception as e:
+            self._logger.error(
+                f"Error handling status change for {device_path}: {e}",
+                extra={"correlation_id": correlation_id, "device_path": device_path},
+            )
 
     def _get_stream_name_from_device_path(self, device_path: str) -> str:
         """
         Generate a deterministic stream name from camera device path.
-        
+
         Extracts device number from standard paths (e.g., /dev/video0 -> camera0)
         or generates a hash-based name for non-standard paths.
-        
+
         Args:
             device_path: Camera device path (e.g., /dev/video0)
-            
+
         Returns:
             Stream name (e.g., camera0)
         """
         if not device_path:
             return "camera_unknown"
-            
+
         # Try to extract video device number from standard paths
-        match = re.search(r'/(?:dev/)?video(\d+)', device_path)
+        match = re.search(r"/(?:dev/)?video(\d+)", device_path)
         if match:
             return f"camera{match.group(1)}"
-        
+
         # For non-standard paths, generate deterministic hash-based name
         import hashlib
+
         path_hash = hashlib.md5(device_path.encode()).hexdigest()[:8]
         return f"camera_{path_hash}"
 
-    async def _get_enhanced_camera_metadata(self, event_data: CameraEventData) -> Dict[str, Any]:
+    async def _get_enhanced_camera_metadata(
+        self, event_data: CameraEventData
+    ) -> Dict[str, Any]:
         """
         Get enhanced camera metadata with capability validation status.
 
         Integrates with camera monitor capability detection using the robust
         get_effective_capability_metadata method that handles provisional/confirmed logic.
         Provides clear annotation of capability validation status and data source.
-        
+
         Args:
             event_data: Camera event data containing device info
-            
+
         Returns:
             Dictionary with camera metadata and validation context:
                 - name, resolution, fps (data fields)
                 - validation_status: "confirmed", "provisional", "none", "error"
                 - capability_source: "confirmed_capability", "provisional_capability", "device_info", "default"
                 - consecutive_successes: number of consecutive capability confirmations (if available)
         """
         correlation_id = get_correlation_id()
         device_path = event_data.device_path
-        
+
         # Extract device number for default naming
         device_num = "unknown"
         try:
-            match = re.search(r'/dev/video(\d+)', device_path)
+            match = re.search(r"/dev/video(\d+)", device_path)
             if match:
                 device_num = match.group(1)
         except Exception:
             pass
-        
+
         # Initialize with default metadata
         camera_metadata = {
             "name": f"Camera {device_num}",
             "resolution": "1920x1080",  # Architecture default
-            "fps": 30,                   # Architecture default
+            "fps": 30,  # Architecture default
             "validation_status": "none",
             "capability_source": "default",
-            "consecutive_successes": 0
+            "consecutive_successes": 0,
         }
-        
+
         # Override with device info if available
         if event_data.device_info:
-            camera_metadata["name"] = event_data.device_info.name or camera_metadata["name"]
+            camera_metadata["name"] = (
+                event_data.device_info.name or camera_metadata["name"]
+            )
             camera_metadata["capability_source"] = "device_info"
-        
+
         # Attempt to get enhanced capability data from camera monitor using robust method
         try:
-            if (self._camera_monitor and 
-                hasattr(self._camera_monitor, 'get_effective_capability_metadata')):
-                
-                capability_data = self._camera_monitor.get_effective_capability_metadata(device_path)
-                
+            if self._camera_monitor and hasattr(
+                self._camera_monitor, "get_effective_capability_metadata"
+            ):
+
+                capability_data = (
+                    self._camera_monitor.get_effective_capability_metadata(device_path)
+                )
+
                 if capability_data:
                     # Extract validation status and consecutive successes
                     validation_status = capability_data.get("validation_status", "none")
-                    consecutive_successes = capability_data.get("consecutive_successes", 0)
-                    
+                    consecutive_successes = capability_data.get(
+                        "consecutive_successes", 0
+                    )
+
                     # Update metadata with capability data
-                    camera_metadata.update({
-                        "resolution": capability_data.get("resolution", camera_metadata["resolution"]),
-                        "fps": capability_data.get("fps", camera_metadata["fps"]),
-                        "validation_status": validation_status,
-                        "consecutive_successes": consecutive_successes
-                    })
-                    
+                    camera_metadata.update(
+                        {
+                            "resolution": capability_data.get(
+                                "resolution", camera_metadata["resolution"]
+                            ),
+                            "fps": capability_data.get("fps", camera_metadata["fps"]),
+                            "validation_status": validation_status,
+                            "consecutive_successes": consecutive_successes,
+                        }
+                    )
+
                     # Determine capability source based on validation status
                     if validation_status == "confirmed":
                         camera_metadata["capability_source"] = "confirmed_capability"
                         self._logger.debug(
                             f"Using confirmed capability data for {device_path}: "
                             f"{camera_metadata['resolution']}@{camera_metadata['fps']}fps "
                             f"(confirmations: {consecutive_successes})",
                             extra={
-                                'correlation_id': correlation_id, 
-                                'device_path': device_path,
-                                'capability_validation': 'confirmed'
-                            }
+                                "correlation_id": correlation_id,
+                                "device_path": device_path,
+                                "capability_validation": "confirmed",
+                            },
                         )
                     elif validation_status == "provisional":
                         camera_metadata["capability_source"] = "provisional_capability"
                         self._logger.debug(
                             f"Using provisional capability data for {device_path}: "
                             f"{camera_metadata['resolution']}@{camera_metadata['fps']}fps "
                             f"(pending confirmation)",
                             extra={
-                                'correlation_id': correlation_id,
-                                'device_path': device_path,
-                                'capability_validation': 'provisional'
-                            }
+                                "correlation_id": correlation_id,
+                                "device_path": device_path,
+                                "capability_validation": "provisional",
+                            },
                         )
                     elif validation_status == "failed":
                         camera_metadata["validation_status"] = "error"
                         camera_metadata["capability_source"] = "default"
                         self._logger.debug(
                             f"Capability detection failed for {device_path}, using defaults",
                             extra={
-                                'correlation_id': correlation_id,
-                                'device_path': device_path,
-                                'capability_validation': 'error'
-                            }
+                                "correlation_id": correlation_id,
+                                "device_path": device_path,
+                                "capability_validation": "error",
+                            },
                         )
                     else:
                         # validation_status == "none" or unknown
                         camera_metadata["capability_source"] = "default"
                         self._logger.debug(
                             f"No capability data available for {device_path}, using defaults",
                             extra={
-                                'correlation_id': correlation_id,
-                                'device_path': device_path,
-                                'capability_validation': 'none'
-                            }
+                                "correlation_id": correlation_id,
+                                "device_path": device_path,
+                                "capability_validation": "none",
+                            },
                         )
-                        
+
         except Exception as e:
             # Capability detection error - use defaults with error annotation
             camera_metadata["validation_status"] = "error"
             camera_metadata["capability_source"] = "default"
             self._logger.warning(
                 f"Error retrieving capability metadata for {device_path}: {e}, using defaults",
                 extra={
-                    'correlation_id': correlation_id,
-                    'device_path': device_path,
-                    'capability_validation': 'error'
-                }
-            )
-        
+                    "correlation_id": correlation_id,
+                    "device_path": device_path,
+                    "capability_validation": "error",
+                },
+            )
+
         return camera_metadata
 
     async def _start_mediamtx_controller(self) -> None:
         """Start the MediaMTX controller component."""
         correlation_id = get_correlation_id()
-        self._logger.debug("Starting MediaMTX controller",
-                          extra={'correlation_id': correlation_id})
-        
+        self._logger.debug(
+            "Starting MediaMTX controller", extra={"correlation_id": correlation_id}
+        )
+
         try:
             from ..mediamtx_wrapper.controller import MediaMTXController
+
             self._mediamtx_controller = MediaMTXController(self._config.mediamtx)
             await self._mediamtx_controller.start()
-            
+
             # Verify MediaMTX health after startup
             health_status = await self._mediamtx_controller.health_check()
-            self._logger.info(f"MediaMTX controller started: {health_status.get('status', 'unknown')}",
-                            extra={'correlation_id': correlation_id})
-                            
-        except Exception as e:
-            self._logger.error(f"Failed to start MediaMTX controller: {e}",
-                             extra={'correlation_id': correlation_id})
+            self._logger.info(
+                f"MediaMTX controller started: {health_status.get('status', 'unknown')}",
+                extra={"correlation_id": correlation_id},
+            )
+
+        except Exception as e:
+            self._logger.error(
+                f"Failed to start MediaMTX controller: {e}",
+                extra={"correlation_id": correlation_id},
+            )
             raise
 
     async def _start_camera_monitor(self) -> None:
         """Start the camera discovery and monitoring component."""
         correlation_id = get_correlation_id()
-        self._logger.debug("Starting camera discovery monitor",
-                          extra={'correlation_id': correlation_id})
-        
+        self._logger.debug(
+            "Starting camera discovery monitor",
+            extra={"correlation_id": correlation_id},
+        )
+
         try:
             from ..camera_discovery.hybrid_monitor import HybridCameraMonitor
+
             self._camera_monitor = HybridCameraMonitor(self._config.camera)
-            
+
             # Register ourselves as an event handler
             self._camera_monitor.add_event_handler(self)
-            
+
             # Start camera monitoring
             await self._camera_monitor.start()
-            self._logger.info("Camera discovery monitor started",
-                            extra={'correlation_id': correlation_id})
-                            
-        except Exception as e:
-            self._logger.error(f"Failed to start camera monitor: {e}",
-                             extra={'correlation_id': correlation_id})
+            self._logger.info(
+                "Camera discovery monitor started",
+                extra={"correlation_id": correlation_id},
+            )
+
+        except Exception as e:
+            self._logger.error(
+                f"Failed to start camera monitor: {e}",
+                extra={"correlation_id": correlation_id},
+            )
             raise
 
     async def _start_health_monitor(self) -> None:
         """Start the health monitoring component."""
         correlation_id = get_correlation_id()
-        self._logger.debug("Starting health monitor",
-                          extra={'correlation_id': correlation_id})
-        
+        self._logger.debug(
+            "Starting health monitor", extra={"correlation_id": correlation_id}
+        )
+
         try:
             self._health_monitor = HealthMonitor(self._config)
             await self._health_monitor.start()
-            self._logger.info("Health monitor started",
-                            extra={'correlation_id': correlation_id})
-                            
-        except Exception as e:
-            self._logger.error(f"Failed to start health monitor: {e}",
-                             extra={'correlation_id': correlation_id})
+            self._logger.info(
+                "Health monitor started", extra={"correlation_id": correlation_id}
+            )
+
+        except Exception as e:
+            self._logger.error(
+                f"Failed to start health monitor: {e}",
+                extra={"correlation_id": correlation_id},
+            )
             raise
 
     async def _start_websocket_server(self) -> None:
         """Start the WebSocket JSON-RPC server component."""
         correlation_id = get_correlation_id()
-        self._logger.debug("Starting WebSocket JSON-RPC server",
-                          extra={'correlation_id': correlation_id})
-        
+        self._logger.debug(
+            "Starting WebSocket JSON-RPC server",
+            extra={"correlation_id": correlation_id},
+        )
+
         try:
             from ..websocket_server.server import WebSocketJsonRpcServer
+
             self._websocket_server = WebSocketJsonRpcServer(
-                self._config, 
-                self._camera_monitor, 
-                self._mediamtx_controller
+                self._config, self._camera_monitor, self._mediamtx_controller
             )
             await self._websocket_server.start()
-            self._logger.info("WebSocket JSON-RPC server started",
-                            extra={'correlation_id': correlation_id})
-                            
-        except Exception as e:
-            self._logger.error(f"Failed to start WebSocket server: {e}",
-                             extra={'correlation_id': correlation_id})
+            self._logger.info(
+                "WebSocket JSON-RPC server started",
+                extra={"correlation_id": correlation_id},
+            )
+
+        except Exception as e:
+            self._logger.error(
+                f"Failed to start WebSocket server: {e}",
+                extra={"correlation_id": correlation_id},
+            )
             raise
 
     async def _stop_websocket_server(self) -> None:
         """Stop the WebSocket JSON-RPC server component."""
         if self._websocket_server:
             correlation_id = get_correlation_id()
-            self._logger.debug("Stopping WebSocket JSON-RPC server",
-                             extra={'correlation_id': correlation_id})
+            self._logger.debug(
+                "Stopping WebSocket JSON-RPC server",
+                extra={"correlation_id": correlation_id},
+            )
             try:
                 await self._websocket_server.stop()
-                self._logger.info("WebSocket JSON-RPC server stopped",
-                                extra={'correlation_id': correlation_id})
+                self._logger.info(
+                    "WebSocket JSON-RPC server stopped",
+                    extra={"correlation_id": correlation_id},
+                )
             except Exception as e:
-                self._logger.error(f"Error stopping WebSocket server: {e}",
-                                 extra={'correlation_id': correlation_id})
+                self._logger.error(
+                    f"Error stopping WebSocket server: {e}",
+                    extra={"correlation_id": correlation_id},
+                )
             finally:
                 self._websocket_server = None
 
     async def _stop_health_monitor(self) -> None:
         """Stop the health monitoring component."""
         if self._health_monitor:
             correlation_id = get_correlation_id()
-            self._logger.debug("Stopping health monitor",
-                             extra={'correlation_id': correlation_id})
+            self._logger.debug(
+                "Stopping health monitor", extra={"correlation_id": correlation_id}
+            )
             try:
                 await self._health_monitor.stop()
-                self._logger.info("Health monitor stopped",
-                                extra={'correlation_id': correlation_id})
+                self._logger.info(
+                    "Health monitor stopped", extra={"correlation_id": correlation_id}
+                )
             except Exception as e:
-                self._logger.error(f"Error stopping health monitor: {e}",
-                                 extra={'correlation_id': correlation_id})
+                self._logger.error(
+                    f"Error stopping health monitor: {e}",
+                    extra={"correlation_id": correlation_id},
+                )
             finally:
                 self._health_monitor = None
 
     async def _stop_camera_monitor(self) -> None:
         """Stop the camera discovery and monitoring component."""
         if self._camera_monitor:
             correlation_id = get_correlation_id()
-            self._logger.debug("Stopping camera discovery monitor",
-                             extra={'correlation_id': correlation_id})
+            self._logger.debug(
+                "Stopping camera discovery monitor",
+                extra={"correlation_id": correlation_id},
+            )
             try:
                 # Unregister event handler
                 self._camera_monitor.remove_event_handler(self)
                 # Stop camera monitoring
                 await self._camera_monitor.stop()
-                self._logger.info("Camera discovery monitor stopped",
-                                extra={'correlation_id': correlation_id})
+                self._logger.info(
+                    "Camera discovery monitor stopped",
+                    extra={"correlation_id": correlation_id},
+                )
             except Exception as e:
-                self._logger.error(f"Error stopping camera monitor: {e}",
-                                 extra={'correlation_id': correlation_id})
+                self._logger.error(
+                    f"Error stopping camera monitor: {e}",
+                    extra={"correlation_id": correlation_id},
+                )
             finally:
                 self._camera_monitor = None
 
     async def _stop_mediamtx_controller(self) -> None:
         """Stop the MediaMTX controller component."""
         if self._mediamtx_controller:
             correlation_id = get_correlation_id()
-            self._logger.debug("Stopping MediaMTX controller",
-                             extra={'correlation_id': correlation_id})
+            self._logger.debug(
+                "Stopping MediaMTX controller", extra={"correlation_id": correlation_id}
+            )
             try:
                 await self._mediamtx_controller.stop()
-                self._logger.info("MediaMTX controller stopped",
-                                extra={'correlation_id': correlation_id})
+                self._logger.info(
+                    "MediaMTX controller stopped",
+                    extra={"correlation_id": correlation_id},
+                )
             except Exception as e:
-                self._logger.error(f"Error stopping MediaMTX controller: {e}",
-                                 extra={'correlation_id': correlation_id})
+                self._logger.error(
+                    f"Error stopping MediaMTX controller: {e}",
+                    extra={"correlation_id": correlation_id},
+                )
             finally:
                 self._mediamtx_controller = None
 
     async def _cleanup_partial_startup(self) -> None:
         """Clean up any partially started components after startup failure."""
         correlation_id = get_correlation_id()
-        self._logger.warning("Cleaning up partially started components",
-                           extra={'correlation_id': correlation_id})
-        
+        self._logger.warning(
+            "Cleaning up partially started components",
+            extra={"correlation_id": correlation_id},
+        )
+
         try:
             if self._websocket_server:
                 await self._websocket_server.stop()
                 self._websocket_server = None
-                
+
             if self._health_monitor:
                 await self._health_monitor.stop()
                 self._health_monitor = None
-                
+
             if self._camera_monitor:
-                if hasattr(self._camera_monitor, 'remove_event_handler'):
+                if hasattr(self._camera_monitor, "remove_event_handler"):
                     self._camera_monitor.remove_event_handler(self)
                 await self._camera_monitor.stop()
                 self._camera_monitor = None
-                
+
             if self._mediamtx_controller:
                 await self._mediamtx_controller.stop()
                 self._mediamtx_controller = None
-                
-        except Exception as e:
-            self._logger.error(f"Error during partial state cleanup: {e}",
-                             extra={'correlation_id': correlation_id})
\ No newline at end of file
+
+        except Exception as e:
+            self._logger.error(
+                f"Error during partial state cleanup: {e}",
+                extra={"correlation_id": correlation_id},
+            )
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/src/mediamtx_wrapper/controller.py	2025-08-03 19:27:38.808800+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/src/mediamtx_wrapper/controller.py	2025-08-04 15:35:22.490522+00:00
@@ -21,20 +21,21 @@
 
 
 @dataclass
 class StreamConfig:
     """Configuration for a MediaMTX stream path."""
+
     name: str
     source: str
     record: bool = False
     record_path: Optional[str] = None
 
 
 class MediaMTXController:
     """
     Async controller for managing MediaMTX via REST API.
-    
+
     Handles stream creation/deletion, recording control, health monitoring,
     and configuration management for the MediaMTX media server with enhanced
     error handling and operational robustness.
     """
 
@@ -54,20 +55,20 @@
         health_max_backoff_interval: int = 120,
         health_recovery_confirmation_threshold: int = 3,
         backoff_base_multiplier: float = 2.0,
         backoff_jitter_range: tuple = (0.8, 1.2),
         process_termination_timeout: float = 3.0,
-        process_kill_timeout: float = 2.0
+        process_kill_timeout: float = 2.0,
     ):
         """
         Initialize MediaMTX controller.
-        
+
         Args:
             host: MediaMTX server hostname or IP
             api_port: MediaMTX REST API port
             rtsp_port: RTSP streaming port
-            webrtc_port: WebRTC streaming port  
+            webrtc_port: WebRTC streaming port
             hls_port: HLS streaming port
             config_path: Path to MediaMTX configuration file
             recordings_path: Directory for recording files
             snapshots_path: Directory for snapshot files
             health_check_interval: Normal health check interval in seconds (default: 30)
@@ -86,145 +87,161 @@
         self._webrtc_port = webrtc_port
         self._hls_port = hls_port
         self._config_path = config_path
         self._recordings_path = recordings_path
         self._snapshots_path = snapshots_path
-        
+
         # Configurable health monitoring parameters
         self._health_check_interval = health_check_interval
         self._health_failure_threshold = health_failure_threshold
         self._health_circuit_breaker_timeout = health_circuit_breaker_timeout
         self._health_max_backoff_interval = health_max_backoff_interval
-        self._health_recovery_confirmation_threshold = health_recovery_confirmation_threshold
+        self._health_recovery_confirmation_threshold = (
+            health_recovery_confirmation_threshold
+        )
         self._backoff_base_multiplier = backoff_base_multiplier
         self._backoff_jitter_range = backoff_jitter_range
-        
+
         # Configurable process management parameters
         self._process_termination_timeout = process_termination_timeout
         self._process_kill_timeout = process_kill_timeout
-        
+
         self._logger = logging.getLogger(__name__)
         self._base_url = f"http://{self._host}:{self._api_port}"
-        
+
         # HTTP client session for REST API calls
         self._session: Optional[aiohttp.ClientSession] = None
         self._health_check_task: Optional[asyncio.Task] = None
         self._running = False
         self._recording_sessions: Dict[str, Dict[str, Any]] = {}
         self._last_health_status = None
-        
+
         # Enhanced tracking for health monitoring
         self._health_state = {
-            'consecutive_failures': 0,
-            'consecutive_successes_during_recovery': 0,
-            'last_success_time': 0.0,
-            'last_failure_time': 0.0,
-            'total_checks': 0,
-            'recovery_count': 0,
-            'circuit_breaker_activations': 0
+            "consecutive_failures": 0,
+            "consecutive_successes_during_recovery": 0,
+            "last_success_time": 0.0,
+            "last_failure_time": 0.0,
+            "total_checks": 0,
+            "recovery_count": 0,
+            "circuit_breaker_activations": 0,
         }
 
     async def start(self) -> None:
         """
         Start the MediaMTX controller.
-        
+
         Initializes HTTP client session and begins health monitoring.
         """
         if self._running:
             self._logger.warning("MediaMTX controller is already running")
             return
-            
+
         correlation_id = get_correlation_id() or str(uuid.uuid4())[:8]
         set_correlation_id(correlation_id)
-        
-        self._logger.info("Starting MediaMTX controller", extra={'correlation_id': correlation_id})
-        
+
+        self._logger.info(
+            "Starting MediaMTX controller", extra={"correlation_id": correlation_id}
+        )
+
         try:
             # Validate directory permissions before starting
             await self._validate_directory_permissions()
-            
+
             # Create aiohttp ClientSession with timeout configuration
             timeout = aiohttp.ClientTimeout(total=15, connect=5)
             connector = aiohttp.TCPConnector(limit=10, limit_per_host=5)
             self._session = aiohttp.ClientSession(
                 timeout=timeout,
                 connector=connector,
-                headers={"Content-Type": "application/json"}
-            )
-            
+                headers={"Content-Type": "application/json"},
+            )
+
             # Start health monitoring task
             self._health_check_task = asyncio.create_task(self._health_monitor_loop())
             self._running = True
-            
-            self._logger.info("MediaMTX controller started successfully", 
-                             extra={'correlation_id': correlation_id})
+
+            self._logger.info(
+                "MediaMTX controller started successfully",
+                extra={"correlation_id": correlation_id},
+            )
         except Exception as e:
-            self._logger.error(f"Failed to start MediaMTX controller: {e}", 
-                             extra={'correlation_id': correlation_id})
+            self._logger.error(
+                f"Failed to start MediaMTX controller: {e}",
+                extra={"correlation_id": correlation_id},
+            )
             await self._cleanup_on_start_failure()
             raise
 
     async def stop(self) -> None:
         """
         Stop the MediaMTX controller.
-        
+
         Closes HTTP client session and stops monitoring tasks.
         """
         if not self._running:
             return
-            
+
         correlation_id = get_correlation_id() or str(uuid.uuid4())[:8]
         set_correlation_id(correlation_id)
-        
-        self._logger.info("Stopping MediaMTX controller", extra={'correlation_id': correlation_id})
+
+        self._logger.info(
+            "Stopping MediaMTX controller", extra={"correlation_id": correlation_id}
+        )
         self._running = False
-        
+
         # Stop health monitoring task
         if self._health_check_task and not self._health_check_task.done():
             self._health_check_task.cancel()
             try:
                 await self._health_check_task
             except asyncio.CancelledError:
                 pass
-        
+
         # Close aiohttp ClientSession
         if self._session:
             await self._session.close()
             self._session = None
-            
-        self._logger.info("MediaMTX controller stopped", extra={'correlation_id': correlation_id})
+
+        self._logger.info(
+            "MediaMTX controller stopped", extra={"correlation_id": correlation_id}
+        )
 
     async def _validate_directory_permissions(self) -> None:
         """Validate that required directories exist and are writable."""
         correlation_id = get_correlation_id() or str(uuid.uuid4())[:8]
-        
+
         directories = [
             ("recordings", self._recordings_path),
-            ("snapshots", self._snapshots_path)
+            ("snapshots", self._snapshots_path),
         ]
-        
+
         for dir_type, dir_path in directories:
             try:
                 # Create directory if it doesn't exist
                 os.makedirs(dir_path, exist_ok=True)
-                
+
                 # Test write permissions
-                test_file = os.path.join(dir_path, f".write_test_{uuid.uuid4().hex[:8]}")
-                with open(test_file, 'w') as f:
+                test_file = os.path.join(
+                    dir_path, f".write_test_{uuid.uuid4().hex[:8]}"
+                )
+                with open(test_file, "w") as f:
                     f.write("test")
                 os.remove(test_file)
-                
-                self._logger.debug(f"Validated {dir_type} directory: {dir_path}", 
-                                 extra={'correlation_id': correlation_id})
-                
+
+                self._logger.debug(
+                    f"Validated {dir_type} directory: {dir_path}",
+                    extra={"correlation_id": correlation_id},
+                )
+
             except PermissionError as e:
                 error_msg = f"Permission denied for {dir_type} directory: {dir_path}"
-                self._logger.error(error_msg, extra={'correlation_id': correlation_id})
+                self._logger.error(error_msg, extra={"correlation_id": correlation_id})
                 raise RuntimeError(error_msg) from e
             except OSError as e:
                 error_msg = f"Cannot access {dir_type} directory: {dir_path} - {e}"
-                self._logger.error(error_msg, extra={'correlation_id': correlation_id})
+                self._logger.error(error_msg, extra={"correlation_id": correlation_id})
                 raise RuntimeError(error_msg) from e
 
     async def _cleanup_on_start_failure(self) -> None:
         """Clean up resources when startup fails."""
         self._running = False
@@ -235,1032 +252,1318 @@
             self._session = None
 
     async def health_check(self) -> Dict[str, Any]:
         """
         Perform health check on MediaMTX server with enhanced error context.
-        
+
         Returns:
             Dict containing health status and metrics
-            
+
         Raises:
             ConnectionError: If MediaMTX is unreachable
         """
         if not self._session:
             raise ConnectionError("MediaMTX controller not started")
-            
+
         correlation_id = get_correlation_id() or str(uuid.uuid4())[:8]
         start_time = time.time()
-        
+
         try:
             # Call MediaMTX API config endpoint to verify connectivity
-            async with self._session.get(f"{self._base_url}/v3/config/global/get") as response:
+            async with self._session.get(
+                f"{self._base_url}/v3/config/global/get"
+            ) as response:
                 response_time = int((time.time() - start_time) * 1000)
-                
+
                 if response.status == 200:
                     config_data = await response.json()
-                    self._health_state['last_success_time'] = time.time()
-                    self._health_state['total_checks'] += 1
-                    
+                    self._health_state["last_success_time"] = time.time()
+                    self._health_state["total_checks"] += 1
+
                     return {
                         "status": "healthy",
                         "version": config_data.get("serverVersion", "unknown"),
                         "uptime": config_data.get("serverUptime", 0),
                         "api_port": self._api_port,
                         "response_time_ms": response_time,
                         "correlation_id": correlation_id,
-                        "consecutive_failures": self._health_state['consecutive_failures'],
-                        "circuit_breaker_activations": self._health_state['circuit_breaker_activations']
+                        "consecutive_failures": self._health_state[
+                            "consecutive_failures"
+                        ],
+                        "circuit_breaker_activations": self._health_state[
+                            "circuit_breaker_activations"
+                        ],
                     }
                 else:
                     error_text = await response.text()
-                    self._health_state['last_failure_time'] = time.time()
-                    self._health_state['total_checks'] += 1
-                    
+                    self._health_state["last_failure_time"] = time.time()
+                    self._health_state["total_checks"] += 1
+
                     return {
                         "status": "unhealthy",
                         "error": f"HTTP {response.status}: {error_text}",
                         "api_port": self._api_port,
                         "response_time_ms": response_time,
                         "correlation_id": correlation_id,
-                        "consecutive_failures": self._health_state['consecutive_failures']
+                        "consecutive_failures": self._health_state[
+                            "consecutive_failures"
+                        ],
                     }
-                    
+
         except aiohttp.ClientError as e:
-            self._health_state['last_failure_time'] = time.time()
-            self._health_state['total_checks'] += 1
+            self._health_state["last_failure_time"] = time.time()
+            self._health_state["total_checks"] += 1
             raise ConnectionError(f"MediaMTX unreachable at {self._base_url}: {e}")
         except Exception as e:
-            self._logger.error(f"Health check failed with unexpected error: {e}", 
-                             extra={'correlation_id': correlation_id})
-            self._health_state['last_failure_time'] = time.time()
-            self._health_state['total_checks'] += 1
-            
+            self._logger.error(
+                f"Health check failed with unexpected error: {e}",
+                extra={"correlation_id": correlation_id},
+            )
+            self._health_state["last_failure_time"] = time.time()
+            self._health_state["total_checks"] += 1
+
             return {
                 "status": "error",
                 "error": f"Unexpected error: {e}",
                 "api_port": self._api_port,
                 "correlation_id": correlation_id,
-                "consecutive_failures": self._health_state['consecutive_failures']
+                "consecutive_failures": self._health_state["consecutive_failures"],
             }
 
     async def create_stream(self, stream_config: StreamConfig) -> Dict[str, str]:
         """
         Create a new stream path in MediaMTX with enhanced idempotent behavior.
-        
+
         Args:
             stream_config: Stream configuration parameters
-            
+
         Returns:
             Dict containing stream URLs for different protocols
-            
+
         Raises:
             ValueError: If stream configuration is invalid
             ConnectionError: If MediaMTX is unreachable
         """
         if not self._session:
             raise ConnectionError("MediaMTX controller not started")
-            
+
         if not stream_config.name or not stream_config.source:
             raise ValueError("Stream name and source are required")
-            
+
         correlation_id = get_correlation_id() or str(uuid.uuid4())[:8]
         set_correlation_id(correlation_id)
-        
-        self._logger.info(f"Creating stream path: {stream_config.name} from {stream_config.source}",
-                         extra={'correlation_id': correlation_id, 'stream_name': stream_config.name})
-        
+
+        self._logger.info(
+            f"Creating stream path: {stream_config.name} from {stream_config.source}",
+            extra={"correlation_id": correlation_id, "stream_name": stream_config.name},
+        )
+
         try:
             # Enhanced idempotent behavior - check if stream already exists
             try:
                 existing_stream = await self.get_stream_status(stream_config.name)
                 if existing_stream.get("name") == stream_config.name:
-                    self._logger.info(f"Stream path already exists, returning existing URLs: {stream_config.name}",
-                                    extra={'correlation_id': correlation_id, 'stream_name': stream_config.name})
+                    self._logger.info(
+                        f"Stream path already exists, returning existing URLs: {stream_config.name}",
+                        extra={
+                            "correlation_id": correlation_id,
+                            "stream_name": stream_config.name,
+                        },
+                    )
                     return self._generate_stream_urls(stream_config.name)
             except ValueError:
                 # Stream doesn't exist, proceed with creation
                 pass
-            
+
             # Create MediaMTX path configuration
             path_config = {
                 "source": stream_config.source,
                 "sourceProtocol": "automatic",
-                "record": stream_config.record
+                "record": stream_config.record,
             }
-            
+
             if stream_config.record and stream_config.record_path:
                 path_config["recordPath"] = stream_config.record_path
-            
+
             # Add stream path via MediaMTX API
             async with self._session.post(
                 f"{self._base_url}/v3/config/paths/add/{stream_config.name}",
-                json=path_config
+                json=path_config,
             ) as response:
                 if response.status in [200, 201]:
-                    self._logger.info(f"Successfully created stream path: {stream_config.name}",
-                                    extra={'correlation_id': correlation_id, 'stream_name': stream_config.name})
+                    self._logger.info(
+                        f"Successfully created stream path: {stream_config.name}",
+                        extra={
+                            "correlation_id": correlation_id,
+                            "stream_name": stream_config.name,
+                        },
+                    )
                     return self._generate_stream_urls(stream_config.name)
                 elif response.status == 409:
                     # Stream already exists, return URLs (additional idempotency)
-                    self._logger.info(f"Stream path already exists (409 conflict): {stream_config.name}",
-                                    extra={'correlation_id': correlation_id, 'stream_name': stream_config.name})
+                    self._logger.info(
+                        f"Stream path already exists (409 conflict): {stream_config.name}",
+                        extra={
+                            "correlation_id": correlation_id,
+                            "stream_name": stream_config.name,
+                        },
+                    )
                     return self._generate_stream_urls(stream_config.name)
                 else:
                     error_text = await response.text()
                     error_context = f"stream_name={stream_config.name}, source={stream_config.source}, record={stream_config.record}"
                     error_msg = f"Failed to create stream {stream_config.name}: HTTP {response.status} - {error_text} (context: {error_context})"
-                    self._logger.error(error_msg, extra={'correlation_id': correlation_id, 'stream_name': stream_config.name})
+                    self._logger.error(
+                        error_msg,
+                        extra={
+                            "correlation_id": correlation_id,
+                            "stream_name": stream_config.name,
+                        },
+                    )
                     raise ConnectionError(error_msg)
-                    
+
         except aiohttp.ClientError as e:
             error_msg = f"MediaMTX unreachable during stream creation for {stream_config.name}: {e}"
-            self._logger.error(error_msg, extra={'correlation_id': correlation_id, 'stream_name': stream_config.name})
+            self._logger.error(
+                error_msg,
+                extra={
+                    "correlation_id": correlation_id,
+                    "stream_name": stream_config.name,
+                },
+            )
             raise ConnectionError(error_msg)
 
     def _generate_stream_urls(self, stream_name: str) -> Dict[str, str]:
         """Generate stream URLs for different protocols."""
         return {
             "rtsp": f"rtsp://{self._host}:{self._rtsp_port}/{stream_name}",
             "webrtc": f"http://{self._host}:{self._webrtc_port}/{stream_name}",
-            "hls": f"http://{self._host}:{self._hls_port}/{stream_name}"
+            "hls": f"http://{self._host}:{self._hls_port}/{stream_name}",
         }
 
     async def delete_stream(self, stream_name: str) -> bool:
         """
         Delete a stream path from MediaMTX with enhanced error handling.
-        
+
         Args:
             stream_name: Name of the stream to delete
-            
+
         Returns:
             True if stream was deleted successfully or didn't exist
-            
+
         Raises:
             ConnectionError: If MediaMTX is unreachable
         """
         if not self._session:
             raise ConnectionError("MediaMTX controller not started")
-            
+
         if not stream_name:
             raise ValueError("Stream name is required")
-            
+
         correlation_id = get_correlation_id() or str(uuid.uuid4())[:8]
         set_correlation_id(correlation_id)
-        
-        self._logger.info(f"Deleting stream path: {stream_name}",
-                         extra={'correlation_id': correlation_id, 'stream_name': stream_name})
-        
+
+        self._logger.info(
+            f"Deleting stream path: {stream_name}",
+            extra={"correlation_id": correlation_id, "stream_name": stream_name},
+        )
+
         try:
             # Delete stream path via MediaMTX API
-            async with self._session.post(f"{self._base_url}/v3/config/paths/delete/{stream_name}") as response:
+            async with self._session.post(
+                f"{self._base_url}/v3/config/paths/delete/{stream_name}"
+            ) as response:
                 if response.status in [200, 204]:
-                    self._logger.info(f"Successfully deleted stream path: {stream_name}",
-                                    extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+                    self._logger.info(
+                        f"Successfully deleted stream path: {stream_name}",
+                        extra={
+                            "correlation_id": correlation_id,
+                            "stream_name": stream_name,
+                        },
+                    )
                     return True
                 elif response.status == 404:
                     # Stream already doesn't exist (idempotent)
-                    self._logger.info(f"Stream path already deleted or never existed: {stream_name}",
-                                    extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+                    self._logger.info(
+                        f"Stream path already deleted or never existed: {stream_name}",
+                        extra={
+                            "correlation_id": correlation_id,
+                            "stream_name": stream_name,
+                        },
+                    )
                     return True
                 else:
                     error_text = await response.text()
                     error_msg = f"Failed to delete stream {stream_name}: HTTP {response.status} - {error_text}"
-                    self._logger.error(error_msg, extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+                    self._logger.error(
+                        error_msg,
+                        extra={
+                            "correlation_id": correlation_id,
+                            "stream_name": stream_name,
+                        },
+                    )
                     return False
-                    
+
         except aiohttp.ClientError as e:
-            error_msg = f"MediaMTX unreachable during stream deletion for {stream_name}: {e}"
-            self._logger.error(error_msg, extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+            error_msg = (
+                f"MediaMTX unreachable during stream deletion for {stream_name}: {e}"
+            )
+            self._logger.error(
+                error_msg,
+                extra={"correlation_id": correlation_id, "stream_name": stream_name},
+            )
             raise ConnectionError(error_msg)
 
     async def start_recording(
-        self, 
-        stream_name: str, 
-        duration: Optional[int] = None,
-        format: str = "mp4"
+        self, stream_name: str, duration: Optional[int] = None, format: str = "mp4"
     ) -> Dict[str, Any]:
         """
         Start recording for the specified stream with enhanced session management.
-        
+
         Args:
             stream_name: Name of the stream to record
             duration: Recording duration in seconds (None for unlimited)
             format: Recording format (mp4, mkv)
-            
+
         Returns:
             Dict containing recording session information
-            
+
         Raises:
             ValueError: If stream does not exist or is already recording
             ConnectionError: If MediaMTX is unreachable
         """
         if not self._session:
             raise ConnectionError("MediaMTX controller not started")
-            
+
         if not stream_name:
             raise ValueError("Stream name is required")
-            
+
         if stream_name in self._recording_sessions:
             raise ValueError(f"Recording already active for stream: {stream_name}")
-            
+
         correlation_id = get_correlation_id() or str(uuid.uuid4())[:8]
         set_correlation_id(correlation_id)
-        
+
         # Validate format
         valid_formats = ["mp4", "mkv", "avi"]
         if format not in valid_formats:
-            raise ValueError(f"Invalid format: {format}. Must be one of: {valid_formats}")
-        
+            raise ValueError(
+                f"Invalid format: {format}. Must be one of: {valid_formats}"
+            )
+
         try:
             # Ensure recordings directory exists and is writable
             try:
                 os.makedirs(self._recordings_path, exist_ok=True)
                 # Test write permissions
-                test_file = os.path.join(self._recordings_path, f".write_test_{uuid.uuid4().hex[:8]}")
-                with open(test_file, 'w') as f:
+                test_file = os.path.join(
+                    self._recordings_path, f".write_test_{uuid.uuid4().hex[:8]}"
+                )
+                with open(test_file, "w") as f:
                     f.write("test")
                 os.remove(test_file)
             except (PermissionError, OSError) as e:
-                error_msg = f"Cannot write to recordings directory {self._recordings_path}: {e}"
-                self._logger.error(error_msg, extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+                error_msg = (
+                    f"Cannot write to recordings directory {self._recordings_path}: {e}"
+                )
+                self._logger.error(
+                    error_msg,
+                    extra={
+                        "correlation_id": correlation_id,
+                        "stream_name": stream_name,
+                    },
+                )
                 raise ValueError(error_msg) from e
-            
+
             # Generate recording filename with timestamp
             timestamp = time.strftime("%Y-%m-%d_%H-%M-%S")
             filename = f"{stream_name}_{timestamp}.{format}"
             record_path = os.path.join(self._recordings_path, filename)
-            
+
             # Record start time for duration calculation
             start_time = time.time()
             start_time_iso = time.strftime("%Y-%m-%dT%H:%M:%SZ")
-            
+
             # Update stream configuration to enable recording
-            path_config = {
-                "record": True,
-                "recordPath": record_path
-            }
-            
+            path_config = {"record": True, "recordPath": record_path}
+
             if duration:
                 path_config["recordDuration"] = duration
-                
+
             async with self._session.post(
-                f"{self._base_url}/v3/config/paths/edit/{stream_name}",
-                json=path_config
+                f"{self._base_url}/v3/config/paths/edit/{stream_name}", json=path_config
             ) as response:
                 if response.status == 200:
                     # Store recording session for duration tracking
                     self._recording_sessions[stream_name] = {
                         "filename": filename,
                         "start_time": start_time,
                         "start_time_iso": start_time_iso,
                         "record_path": record_path,
                         "format": format,
                         "duration": duration,
-                        "correlation_id": correlation_id
+                        "correlation_id": correlation_id,
                     }
-                    
-                    self._logger.info(f"Started recording for stream {stream_name}: {filename}",
-                                    extra={'correlation_id': correlation_id, 'stream_name': stream_name, 'filename': filename})
+
+                    self._logger.info(
+                        f"Started recording for stream {stream_name}: {filename}",
+                        extra={
+                            "correlation_id": correlation_id,
+                            "stream_name": stream_name,
+                            "filename": filename,
+                        },
+                    )
                     return {
                         "stream_name": stream_name,
                         "filename": filename,
                         "status": "started",
                         "start_time": start_time_iso,
                         "record_path": record_path,
                         "format": format,
-                        "duration": duration
+                        "duration": duration,
                     }
                 else:
                     error_text = await response.text()
                     error_msg = f"Failed to start recording for {stream_name}: HTTP {response.status} - {error_text}"
-                    self._logger.error(error_msg, extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+                    self._logger.error(
+                        error_msg,
+                        extra={
+                            "correlation_id": correlation_id,
+                            "stream_name": stream_name,
+                        },
+                    )
                     raise ValueError(error_msg)
-                    
+
         except aiohttp.ClientError as e:
-            error_msg = f"MediaMTX unreachable during recording start for {stream_name}: {e}"
-            self._logger.error(error_msg, extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+            error_msg = (
+                f"MediaMTX unreachable during recording start for {stream_name}: {e}"
+            )
+            self._logger.error(
+                error_msg,
+                extra={"correlation_id": correlation_id, "stream_name": stream_name},
+            )
             raise ConnectionError(error_msg)
-        
+
     async def stop_recording(self, stream_name: str) -> Dict[str, Any]:
         """
         Stop recording for the specified stream with enhanced error handling and accurate duration calculation.
-        
+
         Args:
             stream_name: Name of the stream to stop recording
-            
+
         Returns:
             Dict containing recording completion information
-            
+
         Raises:
             ValueError: If stream is not currently recording
             ConnectionError: If MediaMTX is unreachable
         """
         if not self._session:
             raise ConnectionError("MediaMTX controller not started")
-            
+
         if not stream_name:
             raise ValueError("Stream name is required")
-            
+
         correlation_id = get_correlation_id() or str(uuid.uuid4())[:8]
         set_correlation_id(correlation_id)
-        
+
         try:
             # Get recording session for duration calculation
             session = self._recording_sessions.get(stream_name)
             if not session:
-                raise ValueError(f"No active recording session found for stream: {stream_name}")
-            
+                raise ValueError(
+                    f"No active recording session found for stream: {stream_name}"
+                )
+
             # Calculate recording duration with high precision
             end_time = time.time()
             end_time_iso = time.strftime("%Y-%m-%dT%H:%M:%SZ")
             actual_duration = int(end_time - session["start_time"])
-            
+
             # Update stream configuration to disable recording
-            path_config = {
-                "record": False
-            }
-            
+            path_config = {"record": False}
+
             async with self._session.post(
-                f"{self._base_url}/v3/config/paths/edit/{stream_name}",
-                json=path_config
+                f"{self._base_url}/v3/config/paths/edit/{stream_name}", json=path_config
             ) as response:
                 if response.status == 200:
                     # Enhanced file validation and error handling
                     file_path = session["record_path"]
                     file_size = 0
                     file_exists = False
                     file_error = None
-                    
+
                     try:
                         if os.path.exists(file_path):
                             file_size = os.path.getsize(file_path)
                             file_exists = True
-                            self._logger.debug(f"Recording file validated: {file_path} ({file_size} bytes)",
-                                             extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+                            self._logger.debug(
+                                f"Recording file validated: {file_path} ({file_size} bytes)",
+                                extra={
+                                    "correlation_id": correlation_id,
+                                    "stream_name": stream_name,
+                                },
+                            )
                         else:
                             file_error = f"Recording file not found: {file_path}"
-                            self._logger.warning(file_error, extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+                            self._logger.warning(
+                                file_error,
+                                extra={
+                                    "correlation_id": correlation_id,
+                                    "stream_name": stream_name,
+                                },
+                            )
                     except PermissionError as e:
-                        file_error = f"Permission denied accessing file: {file_path} - {e}"
-                        self._logger.warning(file_error, extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+                        file_error = (
+                            f"Permission denied accessing file: {file_path} - {e}"
+                        )
+                        self._logger.warning(
+                            file_error,
+                            extra={
+                                "correlation_id": correlation_id,
+                                "stream_name": stream_name,
+                            },
+                        )
                     except OSError as e:
                         file_error = f"Error accessing file: {file_path} - {e}"
-                        self._logger.warning(file_error, extra={'correlation_id': correlation_id, 'stream_name': stream_name})
-                    
+                        self._logger.warning(
+                            file_error,
+                            extra={
+                                "correlation_id": correlation_id,
+                                "stream_name": stream_name,
+                            },
+                        )
+
                     # Clean up recording session
                     del self._recording_sessions[stream_name]
-                    
+
                     result = {
                         "stream_name": stream_name,
                         "filename": session["filename"],
                         "status": "completed",
                         "start_time": session["start_time_iso"],
                         "end_time": end_time_iso,
                         "duration": actual_duration,
                         "file_size": file_size,
-                        "file_exists": file_exists
+                        "file_exists": file_exists,
                     }
-                    
+
                     if file_error:
                         result["file_warning"] = file_error
-                    
+
                     self._logger.info(
                         f"Stopped recording for stream {stream_name}: duration={actual_duration}s, "
                         f"file_size={file_size}, file_exists={file_exists}",
-                        extra={'correlation_id': correlation_id, 'stream_name': stream_name, 'filename': session["filename"]}
-                    )
-                    
+                        extra={
+                            "correlation_id": correlation_id,
+                            "stream_name": stream_name,
+                            "filename": session["filename"],
+                        },
+                    )
+
                     return result
                 else:
                     error_text = await response.text()
                     # Keep session for retry - don't delete on API failure
                     error_msg = f"Failed to stop recording for {stream_name}: HTTP {response.status} - {error_text}"
-                    self._logger.error(error_msg, extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+                    self._logger.error(
+                        error_msg,
+                        extra={
+                            "correlation_id": correlation_id,
+                            "stream_name": stream_name,
+                        },
+                    )
                     raise ValueError(error_msg)
-                    
+
         except aiohttp.ClientError as e:
-            error_msg = f"MediaMTX unreachable during recording stop for {stream_name}: {e}"
-            self._logger.error(error_msg, extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+            error_msg = (
+                f"MediaMTX unreachable during recording stop for {stream_name}: {e}"
+            )
+            self._logger.error(
+                error_msg,
+                extra={"correlation_id": correlation_id, "stream_name": stream_name},
+            )
             raise ConnectionError(error_msg)
 
     async def take_snapshot(
-        self, 
-        stream_name: str, 
-        filename: Optional[str] = None
+        self, stream_name: str, filename: Optional[str] = None
     ) -> Dict[str, Any]:
         """
         Capture a snapshot from the specified stream using FFmpeg with enhanced process management.
-        
+
         Args:
             stream_name: Name of the stream to capture
             filename: Custom filename (None for auto-generated)
-            
+
         Returns:
             Dict containing snapshot information
-            
+
         Raises:
             ValueError: If stream does not exist
             ConnectionError: If MediaMTX is unreachable
         """
         if not self._session:
             raise ConnectionError("MediaMTX controller not started")
-            
+
         if not stream_name:
             raise ValueError("Stream name is required")
-            
+
         correlation_id = get_correlation_id() or str(uuid.uuid4())[:8]
         set_correlation_id(correlation_id)
-        
+
         ffmpeg_process = None
         process_termination_attempted = False
         process_killed = False
-        
+
         try:
             # Generate filename if not provided
             if not filename:
                 timestamp = time.strftime("%Y-%m-%d_%H-%M-%S")
                 filename = f"{stream_name}_snapshot_{timestamp}.jpg"
-            
+
             # Validate snapshots directory with enhanced error handling
             try:
                 os.makedirs(self._snapshots_path, exist_ok=True)
                 # Test write permissions
-                test_file = os.path.join(self._snapshots_path, f".write_test_{uuid.uuid4().hex[:8]}")
-                with open(test_file, 'w') as f:
+                test_file = os.path.join(
+                    self._snapshots_path, f".write_test_{uuid.uuid4().hex[:8]}"
+                )
+                with open(test_file, "w") as f:
                     f.write("test")
                 os.remove(test_file)
             except (PermissionError, OSError) as e:
-                error_msg = f"Cannot write to snapshots directory {self._snapshots_path}: {e}"
-                self._logger.error(error_msg, extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+                error_msg = (
+                    f"Cannot write to snapshots directory {self._snapshots_path}: {e}"
+                )
+                self._logger.error(
+                    error_msg,
+                    extra={
+                        "correlation_id": correlation_id,
+                        "stream_name": stream_name,
+                    },
+                )
                 return {
                     "stream_name": stream_name,
                     "filename": filename,
                     "status": "failed",
                     "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                     "file_size": 0,
-                    "error": error_msg
+                    "error": error_msg,
                 }
-                
+
             snapshot_path = os.path.join(self._snapshots_path, filename)
-            
+
             # Use FFmpeg to capture snapshot from RTSP stream
             rtsp_url = f"rtsp://{self._host}:{self._rtsp_port}/{stream_name}"
-            
-            self._logger.info(f"Capturing snapshot from {rtsp_url} to {snapshot_path}",
-                            extra={'correlation_id': correlation_id, 'stream_name': stream_name, 'filename': filename})
-            
+
+            self._logger.info(
+                f"Capturing snapshot from {rtsp_url} to {snapshot_path}",
+                extra={
+                    "correlation_id": correlation_id,
+                    "stream_name": stream_name,
+                    "filename": filename,
+                },
+            )
+
             # FFmpeg command to capture single frame with enhanced options
             ffmpeg_cmd = [
-                'ffmpeg',
-                '-y',  # Overwrite output file
-                '-i', rtsp_url,  # Input RTSP stream
-                '-vframes', '1',  # Capture only 1 frame
-                '-q:v', '2',  # High quality
-                '-timeout', '5000000',  # 5 second timeout in microseconds
-                '-rtsp_transport', 'tcp',  # Use TCP for reliability
-                '-loglevel', 'warning',  # Reduce FFmpeg output
-                snapshot_path
+                "ffmpeg",
+                "-y",  # Overwrite output file
+                "-i",
+                rtsp_url,  # Input RTSP stream
+                "-vframes",
+                "1",  # Capture only 1 frame
+                "-q:v",
+                "2",  # High quality
+                "-timeout",
+                "5000000",  # 5 second timeout in microseconds
+                "-rtsp_transport",
+                "tcp",  # Use TCP for reliability
+                "-loglevel",
+                "warning",  # Reduce FFmpeg output
+                snapshot_path,
             ]
-            
+
             # Execute FFmpeg with enhanced timeout and process management
             ffmpeg_process = await asyncio.wait_for(
                 asyncio.create_subprocess_exec(
                     *ffmpeg_cmd,
                     stdout=asyncio.subprocess.PIPE,
-                    stderr=asyncio.subprocess.PIPE
+                    stderr=asyncio.subprocess.PIPE,
                 ),
-                timeout=10.0  # 10 second timeout for process creation
-            )
-            
+                timeout=10.0,  # 10 second timeout for process creation
+            )
+
             stdout, stderr = await asyncio.wait_for(
                 ffmpeg_process.communicate(),
-                timeout=15.0  # 15 second timeout for execution
-            )
-            
+                timeout=15.0,  # 15 second timeout for execution
+            )
+
             if ffmpeg_process.returncode == 0 and os.path.exists(snapshot_path):
                 try:
                     file_size = os.path.getsize(snapshot_path)
                     self._logger.info(
                         f"Successfully captured snapshot for stream {stream_name}: {filename} ({file_size} bytes)",
-                        extra={'correlation_id': correlation_id, 'stream_name': stream_name, 'filename': filename}
+                        extra={
+                            "correlation_id": correlation_id,
+                            "stream_name": stream_name,
+                            "filename": filename,
+                        },
                     )
                     return {
                         "stream_name": stream_name,
                         "filename": filename,
                         "status": "completed",
                         "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                         "file_size": file_size,
-                        "file_path": snapshot_path
+                        "file_path": snapshot_path,
                     }
                 except OSError as e:
-                    self._logger.warning(f"Could not get file size for snapshot {snapshot_path}: {e}",
-                                       extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+                    self._logger.warning(
+                        f"Could not get file size for snapshot {snapshot_path}: {e}",
+                        extra={
+                            "correlation_id": correlation_id,
+                            "stream_name": stream_name,
+                        },
+                    )
                     return {
                         "stream_name": stream_name,
                         "filename": filename,
                         "status": "completed",
                         "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                         "file_size": 0,
                         "file_path": snapshot_path,
-                        "warning": f"Could not determine file size: {e}"
+                        "warning": f"Could not determine file size: {e}",
                     }
             else:
                 # FFmpeg failed - log error and return failure
-                error_msg = stderr.decode() if stderr else f"FFmpeg exit code: {ffmpeg_process.returncode}"
-                self._logger.error(f"FFmpeg snapshot failed for {stream_name}: {error_msg}",
-                                 extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+                error_msg = (
+                    stderr.decode()
+                    if stderr
+                    else f"FFmpeg exit code: {ffmpeg_process.returncode}"
+                )
+                self._logger.error(
+                    f"FFmpeg snapshot failed for {stream_name}: {error_msg}",
+                    extra={
+                        "correlation_id": correlation_id,
+                        "stream_name": stream_name,
+                    },
+                )
                 return {
                     "stream_name": stream_name,
                     "filename": filename,
                     "status": "failed",
                     "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                     "file_size": 0,
-                    "error": f"FFmpeg capture failed: {error_msg}"
+                    "error": f"FFmpeg capture failed: {error_msg}",
                 }
-                
+
         except asyncio.TimeoutError:
             # Enhanced timeout handling with robust process cleanup
             cleanup_context = await self._cleanup_ffmpeg_process(
                 ffmpeg_process, stream_name, correlation_id
             )
-            
-            timeout_error = f"Snapshot capture timeout for {stream_name} ({cleanup_context})"
-            self._logger.error(timeout_error, extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+
+            timeout_error = (
+                f"Snapshot capture timeout for {stream_name} ({cleanup_context})"
+            )
+            self._logger.error(
+                timeout_error,
+                extra={"correlation_id": correlation_id, "stream_name": stream_name},
+            )
             return {
                 "stream_name": stream_name,
                 "filename": filename or f"{stream_name}_snapshot_timeout.jpg",
                 "status": "failed",
                 "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                 "file_size": 0,
-                "error": timeout_error
+                "error": timeout_error,
             }
         except Exception as e:
             # Enhanced error handling with process cleanup
             cleanup_context = await self._cleanup_ffmpeg_process(
                 ffmpeg_process, stream_name, correlation_id
             )
-                
-            error_msg = f"Failed to capture snapshot for {stream_name} ({cleanup_context}): {e}"
-            self._logger.error(error_msg, extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+
+            error_msg = (
+                f"Failed to capture snapshot for {stream_name} ({cleanup_context}): {e}"
+            )
+            self._logger.error(
+                error_msg,
+                extra={"correlation_id": correlation_id, "stream_name": stream_name},
+            )
             return {
                 "stream_name": stream_name,
                 "filename": filename or f"{stream_name}_snapshot_failed.jpg",
                 "status": "failed",
                 "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                 "file_size": 0,
-                "error": error_msg
+                "error": error_msg,
             }
 
     async def _cleanup_ffmpeg_process(
-        self, 
-        process: Optional[asyncio.subprocess.Process], 
-        stream_name: str, 
-        correlation_id: str
+        self,
+        process: Optional[asyncio.subprocess.Process],
+        stream_name: str,
+        correlation_id: str,
     ) -> str:
         """
         Robust FFmpeg process cleanup with escalating termination.
-        
+
         Returns:
             String describing cleanup action taken
         """
         if not process or process.returncode is not None:
             return "no_cleanup_needed"
-        
+
         cleanup_actions = []
-        
+
         try:
             # Step 1: Graceful termination (SIGTERM)
             process.terminate()
             cleanup_actions.append("terminated")
-            
+
             try:
-                await asyncio.wait_for(process.wait(), timeout=self._process_termination_timeout)
+                await asyncio.wait_for(
+                    process.wait(), timeout=self._process_termination_timeout
+                )
                 cleanup_actions.append("graceful_exit")
                 return "_".join(cleanup_actions)
             except asyncio.TimeoutError:
                 cleanup_actions.append("term_timeout")
-        
+
         except Exception as e:
             cleanup_actions.append(f"term_error_{type(e).__name__}")
-        
+
         try:
             # Step 2: Force kill (SIGKILL)
             process.kill()
             cleanup_actions.append("killed")
-            
+
             try:
-                await asyncio.wait_for(process.wait(), timeout=self._process_kill_timeout)
+                await asyncio.wait_for(
+                    process.wait(), timeout=self._process_kill_timeout
+                )
                 cleanup_actions.append("force_exit")
             except asyncio.TimeoutError:
                 cleanup_actions.append("kill_timeout")
                 self._logger.error(
                     f"FFmpeg process for {stream_name} did not respond to SIGKILL within {self._process_kill_timeout}s",
-                    extra={'correlation_id': correlation_id, 'stream_name': stream_name}
+                    extra={
+                        "correlation_id": correlation_id,
+                        "stream_name": stream_name,
+                    },
                 )
         except Exception as e:
             cleanup_actions.append(f"kill_error_{type(e).__name__}")
             self._logger.error(
                 f"Error killing FFmpeg process for {stream_name}: {e}",
-                extra={'correlation_id': correlation_id, 'stream_name': stream_name}
-            )
-        
+                extra={"correlation_id": correlation_id, "stream_name": stream_name},
+            )
+
         return "_".join(cleanup_actions)
-        
+
     async def get_stream_list(self) -> List[Dict[str, Any]]:
         """
         Get list of all configured streams with enhanced error handling.
-        
+
         Returns:
             List of stream configuration dictionaries
-            
+
         Raises:
             ConnectionError: If MediaMTX is unreachable
         """
         if not self._session:
             raise ConnectionError("MediaMTX controller not started")
-            
+
         correlation_id = get_correlation_id() or str(uuid.uuid4())[:8]
-        
+
         try:
             async with self._session.get(f"{self._base_url}/v3/paths/list") as response:
                 if response.status == 200:
                     data = await response.json()
                     streams = []
-                    
+
                     # Parse MediaMTX paths list response
                     if "items" in data:
                         for path_name, path_info in data["items"].items():
-                            streams.append({
-                                "name": path_name,
-                                "source": path_info.get("source", ""),
-                                "ready": path_info.get("ready", False),
-                                "readers": path_info.get("readers", 0),
-                                "bytes_sent": path_info.get("bytesSent", 0)
-                            })
-                    
-                    self._logger.debug(f"Retrieved {len(streams)} streams from MediaMTX",
-                                     extra={'correlation_id': correlation_id})
+                            streams.append(
+                                {
+                                    "name": path_name,
+                                    "source": path_info.get("source", ""),
+                                    "ready": path_info.get("ready", False),
+                                    "readers": path_info.get("readers", 0),
+                                    "bytes_sent": path_info.get("bytesSent", 0),
+                                }
+                            )
+
+                    self._logger.debug(
+                        f"Retrieved {len(streams)} streams from MediaMTX",
+                        extra={"correlation_id": correlation_id},
+                    )
                     return streams
                 else:
                     error_text = await response.text()
                     error_msg = f"Failed to get stream list: HTTP {response.status} - {error_text}"
-                    self._logger.error(error_msg, extra={'correlation_id': correlation_id})
+                    self._logger.error(
+                        error_msg, extra={"correlation_id": correlation_id}
+                    )
                     raise ConnectionError(error_msg)
-                    
+
         except aiohttp.ClientError as e:
             error_msg = f"MediaMTX unreachable during stream list: {e}"
-            self._logger.error(error_msg, extra={'correlation_id': correlation_id})
+            self._logger.error(error_msg, extra={"correlation_id": correlation_id})
             raise ConnectionError(error_msg)
 
     async def get_stream_status(self, stream_name: str) -> Dict[str, Any]:
         """
         Get detailed status for a specific stream with enhanced error context.
-        
+
         Args:
             stream_name: Name of the stream
-            
+
         Returns:
             Dict containing detailed stream status
-            
+
         Raises:
             ValueError: If stream does not exist
             ConnectionError: If MediaMTX is unreachable
         """
         if not self._session:
             raise ConnectionError("MediaMTX controller not started")
-            
+
         if not stream_name:
             raise ValueError("Stream name is required")
-            
+
         correlation_id = get_correlation_id() or str(uuid.uuid4())[:8]
-        
+
         try:
-            async with self._session.get(f"{self._base_url}/v3/paths/get/{stream_name}") as response:
+            async with self._session.get(
+                f"{self._base_url}/v3/paths/get/{stream_name}"
+            ) as response:
                 if response.status == 200:
                     data = await response.json()
                     return {
                         "name": stream_name,
                         "status": "active" if data.get("ready", False) else "inactive",
                         "source": data.get("source", ""),
                         "readers": data.get("readers", 0),
                         "bytes_sent": data.get("bytesSent", 0),
                         "recording": data.get("record", False),
-                        "correlation_id": correlation_id
+                        "correlation_id": correlation_id,
                     }
                 elif response.status == 404:
                     raise ValueError(f"Stream not found: {stream_name}")
                 else:
                     error_text = await response.text()
                     error_msg = f"Failed to get stream status for {stream_name}: HTTP {response.status} - {error_text}"
-                    self._logger.error(error_msg, extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+                    self._logger.error(
+                        error_msg,
+                        extra={
+                            "correlation_id": correlation_id,
+                            "stream_name": stream_name,
+                        },
+                    )
                     raise ConnectionError(error_msg)
-                    
+
         except aiohttp.ClientError as e:
-            error_msg = f"MediaMTX unreachable during stream status for {stream_name}: {e}"
-            self._logger.error(error_msg, extra={'correlation_id': correlation_id, 'stream_name': stream_name})
+            error_msg = (
+                f"MediaMTX unreachable during stream status for {stream_name}: {e}"
+            )
+            self._logger.error(
+                error_msg,
+                extra={"correlation_id": correlation_id, "stream_name": stream_name},
+            )
             raise ConnectionError(error_msg)
 
     async def update_configuration(self, config_updates: Dict[str, Any]) -> bool:
         """
         Update MediaMTX configuration dynamically with enhanced validation and error handling.
-        
+
         Args:
             config_updates: Configuration parameters to update
-            
+
         Returns:
             True if configuration was updated successfully
-            
+
         Raises:
             ValueError: If configuration is invalid
             ConnectionError: If MediaMTX is unreachable
         """
         if not self._session:
             raise ConnectionError("MediaMTX controller not started")
-            
+
         if not config_updates:
             raise ValueError("Configuration updates are required")
-        
+
         correlation_id = get_correlation_id() or str(uuid.uuid4())[:8]
         set_correlation_id(correlation_id)
-        
+
         # Enhanced configuration validation schema
         valid_config_schema = {
-            'logLevel': {'type': str, 'values': ['error', 'warn', 'info', 'debug']},
-            'logDestinations': {'type': list},
-            'readTimeout': {'type': (str, int), 'min': 1, 'max': 300},
-            'writeTimeout': {'type': (str, int), 'min': 1, 'max': 300},
-            'readBufferCount': {'type': int, 'min': 1, 'max': 4096},
-            'udpMaxPayloadSize': {'type': int, 'min': 1024, 'max': 65507},
-            'runOnConnect': {'type': str},
-            'runOnConnectRestart': {'type': bool},
-            'api': {'type': bool},
-            'apiAddress': {'type': str, 'pattern': r'^[0-9.:]+$'},
-            'metrics': {'type': bool},
-            'metricsAddress': {'type': str, 'pattern': r'^[0-9.:]+$'},
-            'pprof': {'type': bool},
-            'pprofAddress': {'type': str, 'pattern': r'^[0-9.:]+$'},
-            'rtsp': {'type': bool},
-            'rtspAddress': {'type': str, 'pattern': r'^[0-9.:]+$'},
-            'rtspsAddress': {'type': str, 'pattern': r'^[0-9.:]+$'},
-            'rtmp': {'type': bool},
-            'rtmpAddress': {'type': str, 'pattern': r'^[0-9.:]+$'},
-            'rtmps': {'type': bool},
-            'rtmpsAddress': {'type': str, 'pattern': r'^[0-9.:]+$'},
-            'hls': {'type': bool},
-            'hlsAddress': {'type': str, 'pattern': r'^[0-9.:]+$'},
-            'hlsAllowOrigin': {'type': str},
-            'webrtc': {'type': bool},
-            'webrtcAddress': {'type': str, 'pattern': r'^[0-9.:]+$'}
+            "logLevel": {"type": str, "values": ["error", "warn", "info", "debug"]},
+            "logDestinations": {"type": list},
+            "readTimeout": {"type": (str, int), "min": 1, "max": 300},
+            "writeTimeout": {"type": (str, int), "min": 1, "max": 300},
+            "readBufferCount": {"type": int, "min": 1, "max": 4096},
+            "udpMaxPayloadSize": {"type": int, "min": 1024, "max": 65507},
+            "runOnConnect": {"type": str},
+            "runOnConnectRestart": {"type": bool},
+            "api": {"type": bool},
+            "apiAddress": {"type": str, "pattern": r"^[0-9.:]+$"},
+            "metrics": {"type": bool},
+            "metricsAddress": {"type": str, "pattern": r"^[0-9.:]+$"},
+            "pprof": {"type": bool},
+            "pprofAddress": {"type": str, "pattern": r"^[0-9.:]+$"},
+            "rtsp": {"type": bool},
+            "rtspAddress": {"type": str, "pattern": r"^[0-9.:]+$"},
+            "rtspsAddress": {"type": str, "pattern": r"^[0-9.:]+$"},
+            "rtmp": {"type": bool},
+            "rtmpAddress": {"type": str, "pattern": r"^[0-9.:]+$"},
+            "rtmps": {"type": bool},
+            "rtmpsAddress": {"type": str, "pattern": r"^[0-9.:]+$"},
+            "hls": {"type": bool},
+            "hlsAddress": {"type": str, "pattern": r"^[0-9.:]+$"},
+            "hlsAllowOrigin": {"type": str},
+            "webrtc": {"type": bool},
+            "webrtcAddress": {"type": str, "pattern": r"^[0-9.:]+$"},
         }
-        
+
         # Validate configuration keys
         unknown_keys = set(config_updates.keys()) - set(valid_config_schema.keys())
         if unknown_keys:
             error_msg = f"Unknown configuration keys: {list(unknown_keys)}"
-            self._logger.error(error_msg, extra={'correlation_id': correlation_id})
+            self._logger.error(error_msg, extra={"correlation_id": correlation_id})
             raise ValueError(error_msg)
-            
+
         # Enhanced configuration value validation with error accumulation
         validation_errors = []
         for key, value in config_updates.items():
             schema = valid_config_schema[key]
-            
+
             # Type validation
-            if not isinstance(value, schema['type']):
-                validation_errors.append(f"Invalid type for {key}: expected {schema['type']}, got {type(value)}")
+            if not isinstance(value, schema["type"]):
+                validation_errors.append(
+                    f"Invalid type for {key}: expected {schema['type']}, got {type(value)}"
+                )
                 continue
-            
+
             # Value constraints validation
-            if 'values' in schema and value not in schema['values']:
-                validation_errors.append(f"Invalid value for {key}: {value}, allowed values: {schema['values']}")
-                
-            if 'min' in schema and isinstance(value, (int, float)) and value < schema['min']:
-                validation_errors.append(f"Value for {key} too small: {value}, minimum: {schema['min']}")
-                
-            if 'max' in schema and isinstance(value, (int, float)) and value > schema['max']:
-                validation_errors.append(f"Value for {key} too large: {value}, maximum: {schema['max']}")
-            
+            if "values" in schema and value not in schema["values"]:
+                validation_errors.append(
+                    f"Invalid value for {key}: {value}, allowed values: {schema['values']}"
+                )
+
+            if (
+                "min" in schema
+                and isinstance(value, (int, float))
+                and value < schema["min"]
+            ):
+                validation_errors.append(
+                    f"Value for {key} too small: {value}, minimum: {schema['min']}"
+                )
+
+            if (
+                "max" in schema
+                and isinstance(value, (int, float))
+                and value > schema["max"]
+            ):
+                validation_errors.append(
+                    f"Value for {key} too large: {value}, maximum: {schema['max']}"
+                )
+
             # Pattern validation for string types
-            if 'pattern' in schema and isinstance(value, str):
+            if "pattern" in schema and isinstance(value, str):
                 import re
-                if not re.match(schema['pattern'], value):
+
+                if not re.match(schema["pattern"], value):
                     validation_errors.append(f"Invalid format for {key}: {value}")
-        
+
         if validation_errors:
-            error_msg = f"Configuration validation failed: {'; '.join(validation_errors)}"
-            self._logger.error(error_msg, extra={'correlation_id': correlation_id})
+            error_msg = (
+                f"Configuration validation failed: {'; '.join(validation_errors)}"
+            )
+            self._logger.error(error_msg, extra={"correlation_id": correlation_id})
             raise ValueError(error_msg)
-        
+
         try:
-            self._logger.info(f"Updating MediaMTX configuration: {list(config_updates.keys())}",
-                            extra={'correlation_id': correlation_id})
-            
+            self._logger.info(
+                f"Updating MediaMTX configuration: {list(config_updates.keys())}",
+                extra={"correlation_id": correlation_id},
+            )
+
             async with self._session.post(
-                f"{self._base_url}/v3/config/global/patch",
-                json=config_updates
+                f"{self._base_url}/v3/config/global/patch", json=config_updates
             ) as response:
                 if response.status == 200:
-                    self._logger.info(f"MediaMTX configuration updated successfully: {list(config_updates.keys())}",
-                                    extra={'correlation_id': correlation_id})
+                    self._logger.info(
+                        f"MediaMTX configuration updated successfully: {list(config_updates.keys())}",
+                        extra={"correlation_id": correlation_id},
+                    )
                     return True
                 else:
                     error_text = await response.text()
                     error_msg = f"Failed to update configuration: HTTP {response.status} - {error_text}"
-                    self._logger.error(error_msg, extra={'correlation_id': correlation_id})
+                    self._logger.error(
+                        error_msg, extra={"correlation_id": correlation_id}
+                    )
                     raise ValueError(error_msg)
-                    
+
         except aiohttp.ClientError as e:
             error_msg = f"MediaMTX unreachable during configuration update: {e}"
-            self._logger.error(error_msg, extra={'correlation_id': correlation_id})
+            self._logger.error(error_msg, extra={"correlation_id": correlation_id})
             raise ConnectionError(error_msg)
 
     async def _health_monitor_loop(self) -> None:
         """
         Background task for continuous health monitoring with configurable circuit breaker and adaptive backoff.
-        
+
         Monitors MediaMTX health and logs status changes with automatic recovery.
         Uses configurable parameters for all thresholds and backoff calculations.
-        
+
         Anti-Flapping Recovery Design:
         - Circuit breaker requires N consecutive successful health checks before fully resetting
         - This prevents "flapping" where transient successes briefly clear the breaker
         - Configurable confirmation threshold (default: 3) balances stability vs recovery speed
         - Any failure during recovery resets the confirmation counter
         - Partial recovery progress is logged for observability
         """
         correlation_id = str(uuid.uuid4())[:8]
         set_correlation_id(correlation_id)
-        self._logger.info("Starting MediaMTX health monitoring loop",
-                         extra={'correlation_id': correlation_id})
-        
+        self._logger.info(
+            "Starting MediaMTX health monitoring loop",
+            extra={"correlation_id": correlation_id},
+        )
+
         consecutive_failures = 0
         consecutive_successes_during_recovery = 0
         circuit_breaker_active = False
         circuit_breaker_start_time = 0
-        
+
         try:
             while self._running:
                 try:
                     # Set unique correlation ID for each health check
                     check_correlation_id = str(uuid.uuid4())[:8]
                     set_correlation_id(check_correlation_id)
-                    
+
                     # Check circuit breaker state with recovery confirmation
                     if circuit_breaker_active:
-                        if time.time() - circuit_breaker_start_time > self._health_circuit_breaker_timeout:
+                        if (
+                            time.time() - circuit_breaker_start_time
+                            > self._health_circuit_breaker_timeout
+                        ):
                             # Circuit breaker timeout expired - attempt recovery probe
                             consecutive_successes_during_recovery = 0
-                            self._health_state['consecutive_successes_during_recovery'] = 0
-                            self._logger.info("Circuit breaker timeout expired - attempting recovery probe",
-                                            extra={'correlation_id': check_correlation_id, 'circuit_breaker_transition': True})
+                            self._health_state[
+                                "consecutive_successes_during_recovery"
+                            ] = 0
+                            self._logger.info(
+                                "Circuit breaker timeout expired - attempting recovery probe",
+                                extra={
+                                    "correlation_id": check_correlation_id,
+                                    "circuit_breaker_transition": True,
+                                },
+                            )
                             # Continue to health check - don't reset circuit_breaker_active yet
                         else:
                             # Skip health check during circuit breaker with configurable wait
                             cb_wait_interval = min(10, self._health_check_interval)
                             await asyncio.sleep(cb_wait_interval)
                             continue
-                    
+
                     health_status = await self.health_check()
                     current_status = health_status.get("status")
-                    
+
                     # Enhanced status change logging with recovery confirmation logic
                     # Anti-flapping design: Circuit breaker requires N consecutive successes before full reset
                     if current_status != self._last_health_status:
                         if current_status == "healthy":
                             if self._last_health_status in ["unhealthy", "error"]:
                                 if circuit_breaker_active:
                                     # During circuit breaker recovery - count consecutive successes
                                     consecutive_successes_during_recovery += 1
-                                    self._health_state['consecutive_successes_during_recovery'] = consecutive_successes_during_recovery
-                                    
-                                    if consecutive_successes_during_recovery >= self._health_recovery_confirmation_threshold:
+                                    self._health_state[
+                                        "consecutive_successes_during_recovery"
+                                    ] = consecutive_successes_during_recovery
+
+                                    if (
+                                        consecutive_successes_during_recovery
+                                        >= self._health_recovery_confirmation_threshold
+                                    ):
                                         # Confirmed recovery - fully reset circuit breaker
                                         circuit_breaker_active = False
                                         consecutive_failures = 0
                                         consecutive_successes_during_recovery = 0
-                                        self._health_state['recovery_count'] += 1
-                                        self._health_state['consecutive_failures'] = 0
-                                        self._health_state['consecutive_successes_during_recovery'] = 0
-                                        
+                                        self._health_state["recovery_count"] += 1
+                                        self._health_state["consecutive_failures"] = 0
+                                        self._health_state[
+                                            "consecutive_successes_during_recovery"
+                                        ] = 0
+
                                         self._logger.info(
                                             f"MediaMTX health FULLY RECOVERED: {self._last_health_status} -> {current_status} "
                                             f"after {self._health_recovery_confirmation_threshold} consecutive successes "
                                             f"(recovery #{self._health_state['recovery_count']})",
-                                            extra={'correlation_id': check_correlation_id, 'health_transition': True, 'circuit_breaker_reset': True}
+                                            extra={
+                                                "correlation_id": check_correlation_id,
+                                                "health_transition": True,
+                                                "circuit_breaker_reset": True,
+                                            },
                                         )
                                     else:
                                         # Partial recovery - still in confirmation phase
                                         self._logger.info(
                                             f"MediaMTX health IMPROVING: {self._last_health_status} -> {current_status} "
                                             f"(confirmation: {consecutive_successes_during_recovery}/{self._health_recovery_confirmation_threshold})",
-                                            extra={'correlation_id': check_correlation_id, 'health_transition': True, 'recovery_partial': True}
+                                            extra={
+                                                "correlation_id": check_correlation_id,
+                                                "health_transition": True,
+                                                "recovery_partial": True,
+                                            },
                                         )
                                 else:
                                     # Not in circuit breaker state - normal recovery
                                     consecutive_failures = 0
-                                    self._health_state['consecutive_failures'] = 0
+                                    self._health_state["consecutive_failures"] = 0
                                     self._logger.info(
                                         f"MediaMTX health RECOVERED: {self._last_health_status} -> {current_status}",
-                                        extra={'correlation_id': check_correlation_id, 'health_transition': True}
+                                        extra={
+                                            "correlation_id": check_correlation_id,
+                                            "health_transition": True,
+                                        },
                                     )
                             else:
                                 # First time healthy or continuing healthy state
                                 consecutive_failures = 0
                                 consecutive_successes_during_recovery = 0
-                                self._health_state['consecutive_failures'] = 0
-                                self._health_state['consecutive_successes_during_recovery'] = 0
+                                self._health_state["consecutive_failures"] = 0
+                                self._health_state[
+                                    "consecutive_successes_during_recovery"
+                                ] = 0
                         else:
                             # Health degraded - reset recovery progress
                             consecutive_failures += 1
                             consecutive_successes_during_recovery = 0
-                            self._health_state['consecutive_failures'] = consecutive_failures
-                            self._health_state['consecutive_successes_during_recovery'] = 0
-                            
+                            self._health_state["consecutive_failures"] = (
+                                consecutive_failures
+                            )
+                            self._health_state[
+                                "consecutive_successes_during_recovery"
+                            ] = 0
+
                             self._logger.warning(
                                 f"MediaMTX health DEGRADED: {self._last_health_status or 'unknown'} -> {current_status} "
                                 f"(consecutive_failures: {consecutive_failures}/{self._health_failure_threshold})",
-                                extra={'correlation_id': check_correlation_id, 'health_transition': True}
+                                extra={
+                                    "correlation_id": check_correlation_id,
+                                    "health_transition": True,
+                                },
                             )
-                            
+
                             # Activate circuit breaker if too many failures
-                            if consecutive_failures >= self._health_failure_threshold and not circuit_breaker_active:
+                            if (
+                                consecutive_failures >= self._health_failure_threshold
+                                and not circuit_breaker_active
+                            ):
                                 circuit_breaker_active = True
                                 circuit_breaker_start_time = time.time()
                                 consecutive_successes_during_recovery = 0
-                                self._health_state['circuit_breaker_activations'] += 1
-                                self._health_state['consecutive_successes_during_recovery'] = 0
+                                self._health_state["circuit_breaker_activations"] += 1
+                                self._health_state[
+                                    "consecutive_successes_during_recovery"
+                                ] = 0
                                 self._logger.error(
                                     f"MediaMTX health circuit breaker ACTIVATED after {consecutive_failures} consecutive failures "
                                     f"(activation #{self._health_state['circuit_breaker_activations']})",
-                                    extra={'correlation_id': check_correlation_id, 'circuit_breaker': True}
+                                    extra={
+                                        "correlation_id": check_correlation_id,
+                                        "circuit_breaker": True,
+                                    },
                                 )
-                        
+
                         self._last_health_status = current_status
-                    
+
                     # Determine sleep interval based on health status with configurable backoff
                     if current_status == "healthy":
                         sleep_interval = self._health_check_interval
                         consecutive_failures = 0
                     else:
                         # Configurable exponential backoff with jitter
                         base_interval = min(
-                            self._health_check_interval * (self._backoff_base_multiplier ** consecutive_failures), 
-                            self._health_max_backoff_interval
+                            self._health_check_interval
+                            * (self._backoff_base_multiplier**consecutive_failures),
+                            self._health_max_backoff_interval,
                         )
                         jitter = random.uniform(*self._backoff_jitter_range)
                         sleep_interval = base_interval * jitter
                         consecutive_failures += 1
-                        
-                        self._logger.debug(f"Health monitoring backoff: {sleep_interval:.1f}s (failure #{consecutive_failures})",
-                                         extra={'correlation_id': check_correlation_id})
-                    
+
+                        self._logger.debug(
+                            f"Health monitoring backoff: {sleep_interval:.1f}s (failure #{consecutive_failures})",
+                            extra={"correlation_id": check_correlation_id},
+                        )
+
                     await asyncio.sleep(sleep_interval)
-                    
+
                 except asyncio.CancelledError:
-                    self._logger.info("Health monitoring loop cancelled",
-                                    extra={'correlation_id': correlation_id})
+                    self._logger.info(
+                        "Health monitoring loop cancelled",
+                        extra={"correlation_id": correlation_id},
+                    )
                     break
                 except Exception as e:
                     consecutive_failures += 1
-                    consecutive_successes_during_recovery = 0  # Reset recovery progress on error
-                    self._health_state['consecutive_failures'] = consecutive_failures
-                    self._health_state['consecutive_successes_during_recovery'] = 0
+                    consecutive_successes_during_recovery = (
+                        0  # Reset recovery progress on error
+                    )
+                    self._health_state["consecutive_failures"] = consecutive_failures
+                    self._health_state["consecutive_successes_during_recovery"] = 0
                     self._logger.error(
                         f"Health monitoring error (failure #{consecutive_failures}): {e}",
-                        extra={'correlation_id': check_correlation_id},
-                        exc_info=True
-                    )
-                    
+                        extra={"correlation_id": check_correlation_id},
+                        exc_info=True,
+                    )
+
                     # Configurable exponential backoff on errors with circuit breaker
-                    if consecutive_failures >= self._health_failure_threshold and not circuit_breaker_active:
+                    if (
+                        consecutive_failures >= self._health_failure_threshold
+                        and not circuit_breaker_active
+                    ):
                         circuit_breaker_active = True
                         circuit_breaker_start_time = time.time()
                         consecutive_successes_during_recovery = 0
-                        self._health_state['circuit_breaker_activations'] += 1
-                        self._health_state['consecutive_successes_during_recovery'] = 0
+                        self._health_state["circuit_breaker_activations"] += 1
+                        self._health_state["consecutive_successes_during_recovery"] = 0
                         self._logger.error(
                             f"Health monitoring circuit breaker ACTIVATED due to repeated errors "
                             f"(activation #{self._health_state['circuit_breaker_activations']})",
-                            extra={'correlation_id': check_correlation_id, 'circuit_breaker': True}
+                            extra={
+                                "correlation_id": check_correlation_id,
+                                "circuit_breaker": True,
+                            },
                         )
-                    
+
                     # Configurable error backoff with jitter
                     base_error_sleep = min(
-                        self._health_check_interval * (self._backoff_base_multiplier ** consecutive_failures), 
-                        self._health_max_backoff_interval
+                        self._health_check_interval
+                        * (self._backoff_base_multiplier**consecutive_failures),
+                        self._health_max_backoff_interval,
                     )
                     jitter = random.uniform(*self._backoff_jitter_range)
                     error_sleep = base_error_sleep * jitter
                     await asyncio.sleep(error_sleep)
-                    
+
         except Exception as e:
-            self._logger.error(f"Critical error in health monitoring loop: {e}",
-                             extra={'correlation_id': correlation_id},
-                             exc_info=True)
+            self._logger.error(
+                f"Critical error in health monitoring loop: {e}",
+                extra={"correlation_id": correlation_id},
+                exc_info=True,
+            )
         finally:
             self._logger.info(
                 f"Health monitoring loop ended - Final stats: checks={self._health_state['total_checks']}, "
                 f"recoveries={self._health_state['recovery_count']}, failures={self._health_state['consecutive_failures']}, "
                 f"circuit_breaker_activations={self._health_state['circuit_breaker_activations']}, "
                 f"recovery_confirmation_threshold={self._health_recovery_confirmation_threshold}",
-                extra={'correlation_id': correlation_id}
-            )
\ No newline at end of file
+                extra={"correlation_id": correlation_id},
+            )
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/mocks/common_types.py	2025-08-03 19:27:38.811192+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/mocks/common_types.py	2025-08-04 15:35:22.703268+00:00
@@ -8,14 +8,17 @@
 
 
 @dataclass
 class CameraDevice:
     """Mock CameraDevice for testing."""
+
     device: str
     name: str
     status: str
-    
+
     def __post_init__(self):
         # Validate status values
         valid_statuses = ["CONNECTED", "DISCONNECTED", "ERROR", "BUSY"]
         if self.status not in valid_statuses:
-            raise ValueError(f"Invalid status: {self.status}, must be one of {valid_statuses}")
\ No newline at end of file
+            raise ValueError(
+                f"Invalid status: {self.status}, must be one of {valid_statuses}"
+            )
﻿--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/__init__.py	2025-08-03 19:27:38.811192+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/__init__.py	2025-08-04 15:35:22.734050+00:00
@@ -1,11 +1,10 @@
-
 # tests/unit/test_websocket_server/__init__.py
 """
 WebSocket JSON-RPC server test package.
 
 Contains comprehensive test coverage for:
 - Status aggregation with real data integration
-- Notification filtering and API compliance  
+- Notification filtering and API compliance
 - Method handler functionality and error handling
 - Graceful degradation and dependency management
-"""
\ No newline at end of file
+"""
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/conftest.py	2025-08-03 19:27:38.811554+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/conftest.py	2025-08-04 15:35:22.777282+00:00
@@ -1,6 +1,7 @@
 import pytest
 from camera_discovery.hybrid_monitor import HybridCameraMonitor
+
 
 @pytest.fixture
 def monitor():
     return HybridCameraMonitor(device_range=[0, 1, 2], enable_capability_detection=True)
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/ivv/test_integration_smoke.py	2025-08-04 15:25:00.513305+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/ivv/test_integration_smoke.py	2025-08-04 15:35:22.890018+00:00
@@ -1,10 +1,10 @@
 """
 S5 Core Integration Smoke Test
 
 Tests the complete end-to-end flow:
-- Camera discovery → MediaMTX stream creation → WebSocket notification → 
+- Camera discovery → MediaMTX stream creation → WebSocket notification →
   recording/snapshot operations → shutdown/error recovery
 
 This test validates the critical integration points defined in the S5 acceptance plan.
 
 Prerequisites:
@@ -38,76 +38,76 @@
 from src.common.types import CameraDevice, CameraEvent, CameraEventData
 
 
 class WebSocketTestClient:
     """Test client for WebSocket JSON-RPC communication."""
-    
+
     def __init__(self, uri: str):
         self.uri = uri
         self.websocket = None
         self.received_messages = []
         self.request_id = 1
-        
+
     async def connect(self):
         """Connect to WebSocket server."""
         self.websocket = await websockets.connect(self.uri)
-        
+
     async def disconnect(self):
         """Disconnect from WebSocket server."""
         if self.websocket:
             await self.websocket.close()
-            
+
     async def send_request(self, method: str, params: Optional[Dict] = None) -> Dict:
         """Send JSON-RPC request and wait for response."""
         request = {
             "jsonrpc": "2.0",
             "method": method,
             "id": self.request_id,
-            "params": params or {}
+            "params": params or {},
         }
         self.request_id += 1
-        
+
         await self.websocket.send(json.dumps(request))
-        
+
         # Wait for response with matching ID
         timeout = 5.0
         start_time = time.time()
-        
+
         while time.time() - start_time < timeout:
             try:
                 message = await asyncio.wait_for(self.websocket.recv(), timeout=1.0)
                 response = json.loads(message)
-                
+
                 # Store all messages for inspection
                 self.received_messages.append(response)
-                
+
                 # Return response if ID matches
                 if response.get("id") == request["id"]:
                     return response
-                    
+
             except asyncio.TimeoutError:
                 continue
-                
+
         raise TimeoutError(f"No response received for {method} within {timeout}s")
-        
+
     async def wait_for_notification(self, method: str, timeout: float = 5.0) -> Dict:
         """Wait for specific notification method."""
         start_time = time.time()
-        
+
         while time.time() - start_time < timeout:
             try:
                 message = await asyncio.wait_for(self.websocket.recv(), timeout=1.0)
                 response = json.loads(message)
-                
+
                 self.received_messages.append(response)
-                
+
                 if response.get("method") == method:
                     return response
-                    
+
             except asyncio.TimeoutError:
                 continue
-                
+
         raise TimeoutError(f"No notification {method} received within {timeout}s")
 
 
 @pytest.fixture
 async def test_config():
@@ -116,39 +116,39 @@
         config_data = {
             "server": {
                 "host": "localhost",
                 "port": 8002,
                 "websocket_path": "/ws",
-                "max_connections": 100
+                "max_connections": 100,
             },
             "mediamtx": {
                 "host": "localhost",
                 "api_port": 9997,
                 "rtsp_port": 8554,
                 "webrtc_port": 8889,
                 "hls_port": 8888,
-                "timeout": 10.0
+                "timeout": 10.0,
             },
             "camera_discovery": {
                 "device_range": [0, 1, 2],
                 "poll_interval": 2.0,
                 "enable_capability_detection": True,
-                "detection_timeout": 5.0
+                "detection_timeout": 5.0,
             },
             "logging": {
                 "level": "INFO",
                 "format": "human",
-                "correlation_enabled": True
+                "correlation_enabled": True,
             },
             "recording": {
                 "output_dir": os.path.join(temp_dir, "recordings"),
                 "snapshot_dir": os.path.join(temp_dir, "snapshots"),
                 "max_duration": 3600,
-                "cleanup_interval": 300
-            }
+                "cleanup_interval": 300,
+            },
         }
-        
+
         config = CameraServiceConfig()
         config.update_from_dict(config_data)
         yield config
 
 
@@ -168,282 +168,290 @@
         name="Test Camera",
         status="CONNECTED",
         capabilities={
             "formats": ["YUYV", "MJPEG"],
             "resolutions": ["1920x1080", "1280x720"],
-            "framerates": ["30", "15"]
-        }
+            "framerates": ["30", "15"],
+        },
     )
     return device
 
 
 class TestEndToEndIntegration:
     """Core integration smoke tests for S5 validation."""
-    
+
     @pytest.mark.asyncio
     @pytest.mark.integration
     async def test_ping_basic_connectivity(self, test_config, websocket_client):
         """Test basic WebSocket connectivity and ping method."""
         # Start minimal service for ping test
         server = WebSocketJsonRpcServer(
             host=test_config.server.host,
             port=test_config.server.port,
             websocket_path=test_config.server.websocket_path,
-            max_connections=test_config.server.max_connections
-        )
-        
+            max_connections=test_config.server.max_connections,
+        )
+
         # Start server in background
         server_task = asyncio.create_task(server.start())
         await asyncio.sleep(0.5)  # Allow server startup
-        
+
         try:
             # Test connection and ping
             await websocket_client.connect()
             response = await websocket_client.send_request("ping")
-            
+
             # Validate response format
             assert response["jsonrpc"] == "2.0"
             assert response["result"] == "pong"
             assert "id" in response
-            
+
         finally:
             # Cleanup
             server_task.cancel()
             try:
                 await server_task
             except asyncio.CancelledError:
                 pass
-                
+
     @pytest.mark.asyncio
     @pytest.mark.integration
-    async def test_end_to_end_camera_flow(self, test_config, websocket_client, mock_camera_device):
+    async def test_end_to_end_camera_flow(
+        self, test_config, websocket_client, mock_camera_device
+    ):
         """
         Test complete camera discovery → MediaMTX → notification → operations flow.
-        
+
         This is the core smoke test validating S5 acceptance criteria:
         - Camera discovery and notification
         - MediaMTX stream creation
         - WebSocket notification delivery
         - Recording and snapshot operations
         """
-        
+
         # Create mocked dependencies for controlled testing
         mock_mediamtx = Mock()
         mock_mediamtx.create_stream = AsyncMock(return_value={"status": "created"})
-        mock_mediamtx.start_recording = AsyncMock(return_value={"filename": "test_recording.mp4"})
+        mock_mediamtx.start_recording = AsyncMock(
+            return_value={"filename": "test_recording.mp4"}
+        )
         mock_mediamtx.stop_recording = AsyncMock(return_value={"duration": 10})
-        mock_mediamtx.take_snapshot = AsyncMock(return_value={"filename": "test_snapshot.jpg"})
-        mock_mediamtx.get_stream_metrics = AsyncMock(return_value={
-            "bytes_sent": 12345,
-            "readers": 1,
-            "uptime": 30
-        })
-        
+        mock_mediamtx.take_snapshot = AsyncMock(
+            return_value={"filename": "test_snapshot.jpg"}
+        )
+        mock_mediamtx.get_stream_metrics = AsyncMock(
+            return_value={"bytes_sent": 12345, "readers": 1, "uptime": 30}
+        )
+
         mock_camera_monitor = Mock()
         mock_camera_monitor.get_camera_list = Mock(return_value=[mock_camera_device])
         mock_camera_monitor.get_camera_by_device = Mock(return_value=mock_camera_device)
-        mock_camera_monitor.get_effective_capability_metadata = Mock(return_value={
-            "resolution": "1920x1080",
-            "fps": 30,
-            "validation_status": "confirmed",
-            "formats": ["YUYV", "MJPEG"]
-        })
-        
+        mock_camera_monitor.get_effective_capability_metadata = Mock(
+            return_value={
+                "resolution": "1920x1080",
+                "fps": 30,
+                "validation_status": "confirmed",
+                "formats": ["YUYV", "MJPEG"],
+            }
+        )
+
         # Initialize service manager with mocked dependencies
         service_manager = CameraServiceManager(test_config)
         service_manager._mediamtx_controller = mock_mediamtx
         service_manager._camera_monitor = mock_camera_monitor
-        
+
         # Initialize WebSocket server
         server = WebSocketJsonRpcServer(
             host=test_config.server.host,
             port=test_config.server.port,
             websocket_path=test_config.server.websocket_path,
             max_connections=test_config.server.max_connections,
             mediamtx_controller=mock_mediamtx,
-            camera_monitor=mock_camera_monitor
-        )
-        
+            camera_monitor=mock_camera_monitor,
+        )
+
         # Connect service manager to server for notifications
         service_manager._websocket_server = server
-        
+
         # Start server
         server_task = asyncio.create_task(server.start())
         await asyncio.sleep(0.5)  # Allow startup
-        
+
         try:
             # Step 1: Connect WebSocket client
             await websocket_client.connect()
-            
+
             # Step 2: Test get_camera_list (should return mock camera)
-            camera_list_response = await websocket_client.send_request("get_camera_list")
+            camera_list_response = await websocket_client.send_request(
+                "get_camera_list"
+            )
             assert camera_list_response["jsonrpc"] == "2.0"
             assert "result" in camera_list_response
-            
+
             cameras = camera_list_response["result"]["cameras"]
             assert len(cameras) >= 1
-            
+
             # Validate camera data structure
             camera = cameras[0]
             assert camera["device"] == "/dev/video0"
             assert camera["status"] == "CONNECTED"
             assert camera["name"] == "Test Camera"
             assert "streams" in camera
-            
+
             # Step 3: Simulate camera connection event and test notification
             camera_event = CameraEvent(
                 event_type="connected",
                 device_path="/dev/video0",
                 event_data=CameraEventData(
                     device_info=mock_camera_device,
                     capabilities=mock_camera_device.capabilities,
-                    timestamp=time.time()
-                )
-            )
-            
+                    timestamp=time.time(),
+                ),
+            )
+
             # Trigger camera event handler
             notification_task = asyncio.create_task(
-                websocket_client.wait_for_notification("camera_status_update", timeout=3.0)
-            )
-            
+                websocket_client.wait_for_notification(
+                    "camera_status_update", timeout=3.0
+                )
+            )
+
             # Simulate camera connection
             await service_manager.handle_camera_event(camera_event)
-            
+
             # Wait for notification
             try:
                 notification = await notification_task
-                
+
                 # Validate notification structure per API spec
                 assert notification["jsonrpc"] == "2.0"
                 assert notification["method"] == "camera_status_update"
                 assert "params" in notification
-                
+
                 params = notification["params"]
                 assert params["device"] == "/dev/video0"
                 assert params["status"] == "CONNECTED"
                 assert "name" in params
                 assert "resolution" in params
                 assert "fps" in params
                 assert "streams" in params
-                
+
             except TimeoutError:
                 pytest.fail("Camera status notification not received within timeout")
-            
+
             # Step 4: Test camera status API
             status_response = await websocket_client.send_request(
-                "get_camera_status", 
-                {"device": "/dev/video0"}
-            )
-            
+                "get_camera_status", {"device": "/dev/video0"}
+            )
+
             assert status_response["jsonrpc"] == "2.0"
             assert "result" in status_response
-            
+
             status = status_response["result"]
             assert status["device"] == "/dev/video0"
             assert status["status"] == "CONNECTED"
             assert "capabilities" in status
             assert "metrics" in status
-            
+
             # Step 5: Test recording operations
             # Start recording
             start_recording_response = await websocket_client.send_request(
-                "start_recording",
-                {"device": "/dev/video0"}
-            )
-            
+                "start_recording", {"device": "/dev/video0"}
+            )
+
             assert start_recording_response["jsonrpc"] == "2.0"
             assert "result" in start_recording_response
             assert start_recording_response["result"]["status"] == "started"
-            
+
             # Verify MediaMTX start_recording was called
             mock_mediamtx.start_recording.assert_called_once()
-            
+
             # Stop recording
             stop_recording_response = await websocket_client.send_request(
-                "stop_recording",
-                {"device": "/dev/video0"}
-            )
-            
+                "stop_recording", {"device": "/dev/video0"}
+            )
+
             assert stop_recording_response["jsonrpc"] == "2.0"
             assert "result" in stop_recording_response
             assert "duration" in stop_recording_response["result"]
-            
+
             # Verify MediaMTX stop_recording was called
             mock_mediamtx.stop_recording.assert_called_once()
-            
+
             # Step 6: Test snapshot operation
             snapshot_response = await websocket_client.send_request(
-                "take_snapshot",
-                {"device": "/dev/video0"}
-            )
-            
+                "take_snapshot", {"device": "/dev/video0"}
+            )
+
             assert snapshot_response["jsonrpc"] == "2.0"
             assert "result" in snapshot_response
             assert "filename" in snapshot_response["result"]
             assert "timestamp" in snapshot_response["result"]
-            
+
             # Verify MediaMTX take_snapshot was called
             mock_mediamtx.take_snapshot.assert_called_once()
-            
+
             # Step 7: Test error handling with invalid device
             error_response = await websocket_client.send_request(
-                "get_camera_status",
-                {"device": "/dev/video99"}
-            )
-            
+                "get_camera_status", {"device": "/dev/video99"}
+            )
+
             assert error_response["jsonrpc"] == "2.0"
             assert "error" in error_response
             assert error_response["error"]["code"] == -1000  # Camera not found
-            
+
         finally:
             # Cleanup
             server_task.cancel()
             try:
                 await server_task
             except asyncio.CancelledError:
                 pass
-                
+
     @pytest.mark.asyncio
     @pytest.mark.integration
     async def test_mediamtx_error_recovery(self, test_config, websocket_client):
         """Test service behavior when MediaMTX is unavailable."""
-        
+
         # Create MediaMTX controller that simulates connection errors
         mock_mediamtx = Mock()
-        mock_mediamtx.create_stream = AsyncMock(side_effect=ConnectionError("MediaMTX unavailable"))
-        mock_mediamtx.start_recording = AsyncMock(side_effect=ConnectionError("MediaMTX unavailable"))
-        
+        mock_mediamtx.create_stream = AsyncMock(
+            side_effect=ConnectionError("MediaMTX unavailable")
+        )
+        mock_mediamtx.start_recording = AsyncMock(
+            side_effect=ConnectionError("MediaMTX unavailable")
+        )
+
         mock_camera_monitor = Mock()
         mock_camera_monitor.get_camera_list = Mock(return_value=[])
-        
+
         # Initialize server with failing MediaMTX
         server = WebSocketJsonRpcServer(
             host=test_config.server.host,
             port=test_config.server.port,
             websocket_path=test_config.server.websocket_path,
             max_connections=test_config.server.max_connections,
             mediamtx_controller=mock_mediamtx,
-            camera_monitor=mock_camera_monitor
-        )
-        
+            camera_monitor=mock_camera_monitor,
+        )
+
         server_task = asyncio.create_task(server.start())
         await asyncio.sleep(0.5)
-        
+
         try:
             await websocket_client.connect()
-            
+
             # Test that MediaMTX errors are properly handled
             error_response = await websocket_client.send_request(
-                "start_recording",
-                {"device": "/dev/video0"}
-            )
-            
+                "start_recording", {"device": "/dev/video0"}
+            )
+
             assert error_response["jsonrpc"] == "2.0"
             assert "error" in error_response
             assert error_response["error"]["code"] == -1003  # MediaMTX error
             assert "MediaMTX" in error_response["error"]["message"]
-            
+
         finally:
             server_task.cancel()
             try:
                 await server_task
             except asyncio.CancelledError:
@@ -451,62 +459,66 @@
 
     @pytest.mark.asyncio
     @pytest.mark.integration
     async def test_multiple_websocket_clients(self, test_config):
         """Test that multiple WebSocket clients can connect and receive notifications."""
-        
+
         mock_mediamtx = Mock()
         mock_camera_monitor = Mock()
         mock_camera_monitor.get_camera_list = Mock(return_value=[])
-        
+
         server = WebSocketJsonRpcServer(
             host=test_config.server.host,
             port=test_config.server.port,
             websocket_path=test_config.server.websocket_path,
             max_connections=test_config.server.max_connections,
             mediamtx_controller=mock_mediamtx,
-            camera_monitor=mock_camera_monitor
-        )
-        
+            camera_monitor=mock_camera_monitor,
+        )
+
         server_task = asyncio.create_task(server.start())
         await asyncio.sleep(0.5)
-        
+
         clients = []
         try:
             # Connect multiple clients
             for i in range(3):
                 client = WebSocketTestClient("ws://localhost:8002/ws")
                 await client.connect()
                 clients.append(client)
-            
+
             # Send ping to each client
             for i, client in enumerate(clients):
                 response = await client.send_request("ping")
                 assert response["result"] == "pong"
-            
+
             # Test broadcast notification (simulate camera event)
             notification_params = {
                 "device": "/dev/video0",
                 "status": "CONNECTED",
                 "name": "Test Camera",
                 "resolution": "1920x1080",
                 "fps": 30,
-                "streams": {"rtsp": "rtsp://localhost:8554/camera0"}
+                "streams": {"rtsp": "rtsp://localhost:8554/camera0"},
             }
-            
+
             # Trigger broadcast
-            await server.broadcast_notification("camera_status_update", notification_params)
-            
+            await server.broadcast_notification(
+                "camera_status_update", notification_params
+            )
+
             # Verify all clients receive notification
             for client in clients:
                 try:
-                    notification = await client.wait_for_notification("camera_status_update", timeout=2.0)
+                    notification = await client.wait_for_notification(
+                        "camera_status_update", timeout=2.0
+                    )
                     assert notification["method"] == "camera_status_update"
                     assert notification["params"]["device"] == "/dev/video0"
                 except TimeoutError:
                     pytest.fail(f"Client did not receive broadcast notification")
-                    
+
         finally:
             # Cleanup
             for client in clients:
                 await client.disconnect()
             server_task.cancel()
@@ -517,61 +529,61 @@
 
 
 # Performance and resource validation tests
 class TestPerformanceAndResources:
     """Performance and resource usage validation tests."""
-    
+
     @pytest.mark.asyncio
     @pytest.mark.integration
     @pytest.mark.slow
     async def test_resource_usage_limits(self, test_config):
         """Validate service operates within acceptable resource limits."""
         # This test would need psutil or similar for real resource monitoring
         # For now, we'll simulate the checks
-        
+
         mock_mediamtx = Mock()
         mock_camera_monitor = Mock()
         mock_camera_monitor.get_camera_list = Mock(return_value=[])
-        
+
         server = WebSocketJsonRpcServer(
             host=test_config.server.host,
             port=test_config.server.port,
             websocket_path=test_config.server.websocket_path,
             max_connections=test_config.server.max_connections,
             mediamtx_controller=mock_mediamtx,
-            camera_monitor=mock_camera_monitor
-        )
-        
+            camera_monitor=mock_camera_monitor,
+        )
+
         server_task = asyncio.create_task(server.start())
         await asyncio.sleep(0.5)
-        
+
         try:
             # Simulate resource usage checks
             # In real implementation, would use psutil to monitor:
             # - CPU usage < 20%
             # - Memory usage < 100MB
             # - No memory leaks over time
-            
+
             client = WebSocketTestClient("ws://localhost:8002/ws")
             await client.connect()
-            
+
             # Send multiple requests to stress test
             for i in range(50):
                 response = await client.send_request("ping")
                 assert response["result"] == "pong"
-            
+
             await client.disconnect()
-            
+
             # In real test, verify resource cleanup
             assert True  # Placeholder for actual resource validation
-            
+
         finally:
             server_task.cancel()
             try:
                 await server_task
             except asyncio.CancelledError:
                 pass
 
 
 if __name__ == "__main__":
     # Allow running individual tests
-    pytest.main([__file__, "-v"])
\ No newline at end of file
+    pytest.main([__file__, "-v"])
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/src/websocket_server/server.py	2025-08-03 19:27:38.809768+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/src/websocket_server/server.py	2025-08-04 15:35:23.050103+00:00
@@ -19,50 +19,53 @@
 
 
 @dataclass
 class JsonRpcRequest:
     """JSON-RPC 2.0 request structure."""
+
     jsonrpc: str
     method: str
     id: Optional[Any] = None
     params: Optional[Dict[str, Any]] = None
 
 
 @dataclass
 class JsonRpcResponse:
     """JSON-RPC 2.0 response structure."""
+
     jsonrpc: str
     id: Optional[Any]
     result: Optional[Any] = None
     error: Optional[Dict[str, Any]] = None
 
 
 @dataclass
 class JsonRpcNotification:
     """JSON-RPC 2.0 notification structure."""
+
     jsonrpc: str
     method: str
     params: Optional[Dict[str, Any]] = None
 
 
 class ClientConnection:
     """Represents a connected WebSocket client."""
-    
+
     def __init__(self, websocket: WebSocketServerProtocol, client_id: str):
         """
         Initialize client connection.
-        
+
         Args:
             websocket: WebSocket connection object
             client_id: Unique identifier for this client
         """
         self.websocket = websocket
         self.client_id = client_id
         self.authenticated = False
         self.subscriptions: Set[str] = set()
         self.connected_at = asyncio.get_event_loop().time()
-        
+
         # TODO: HIGH: Implement authentication state management [IV&V:S6]
         # TODO: MEDIUM: Implement permission tracking system [IV&V:S6]
         # TODO: MEDIUM: Implement rate limiting state tracking [IV&V:S6]
 
 
@@ -85,15 +88,15 @@
         host: str,
         port: int,
         websocket_path: str,
         max_connections: int,
         mediamtx_controller=None,
-        camera_monitor=None
+        camera_monitor=None,
     ):
         """
         Initialize WebSocket JSON-RPC server.
-        
+
         Args:
             host: Server bind address
             port: Server port
             websocket_path: WebSocket endpoint path
             max_connections: Maximum concurrent client connections
@@ -104,60 +107,62 @@
         self._port = port
         self._websocket_path = websocket_path
         self._max_connections = max_connections
         self._mediamtx_controller = mediamtx_controller
         self._camera_monitor = camera_monitor
-        
+
         self._logger = logging.getLogger(__name__)
         self._server = None
         self._running = False
-        
+
         # Client connection management
         self._clients: Dict[str, ClientConnection] = {}
         self._connection_lock = asyncio.Lock()
-        
+
         # JSON-RPC method handlers
         self._method_handlers: Dict[str, Callable] = {}
-        
+
         # TODO: HIGH: Initialize JWT/API key authentication system [IV&V:S6]
         # TODO: MEDIUM: Initialize rate limiting with configurable thresholds [IV&V:S6]
         # TODO: MEDIUM: Initialize Prometheus metrics collection [IV&V:S6]
 
     def set_mediamtx_controller(self, controller) -> None:
         """
         Set the MediaMTX controller for stream operations.
-        
+
         Args:
             controller: MediaMTX controller instance
         """
         self._mediamtx_controller = controller
 
     def set_camera_monitor(self, monitor) -> None:
         """
         Set the camera monitor for device information.
-        
+
         Args:
             monitor: Camera monitor instance
         """
         self._camera_monitor = monitor
 
     async def start(self) -> None:
         """
         Start the WebSocket JSON-RPC server.
-        
+
         Initializes the WebSocket server and begins accepting client connections.
         """
         if self._running:
             self._logger.warning("WebSocket server is already running")
             return
-        
-        self._logger.info(f"Starting WebSocket JSON-RPC server on {self._host}:{self._port}{self._websocket_path}")
-        
+
+        self._logger.info(
+            f"Starting WebSocket JSON-RPC server on {self._host}:{self._port}{self._websocket_path}"
+        )
+
         try:
             # Register built-in methods
             self._register_builtin_methods()
-            
+
             # Start WebSocket server with proper error handling
             self._server = await websockets.serve(
                 self._handle_client_connection,
                 self._host,
                 self._port,
@@ -167,132 +172,145 @@
                 compression=None,  # Disable compression for simplicity
                 ping_interval=30,  # Ping every 30 seconds
                 ping_timeout=10,  # Ping timeout
                 close_timeout=5,  # Close timeout
                 # Path handling - only accept connections to our WebSocket path
-                process_request=self._process_request
-            )
-            
+                process_request=self._process_request,
+            )
+
             self._running = True
-            self._logger.info(f"WebSocket JSON-RPC server started successfully on {self._host}:{self._port}{self._websocket_path}")
-            
+            self._logger.info(
+                f"WebSocket JSON-RPC server started successfully on {self._host}:{self._port}{self._websocket_path}"
+            )
+
         except Exception as e:
             self._logger.error(f"Failed to start WebSocket server: {e}")
             await self._cleanup_server()
             raise
 
     async def stop(self) -> None:
         """
         Stop the WebSocket JSON-RPC server.
-        
+
         Gracefully closes all client connections and stops the server.
         """
         if not self._running:
             return
-        
+
         self._logger.info("Stopping WebSocket JSON-RPC server")
-        
+
         try:
             self._running = False
-            
+
             # Close all client connections
             await self._close_all_connections()
-            
+
             # Stop WebSocket server
             if self._server:
                 self._server.close()
                 await self._server.wait_closed()
                 self._server = None
-            
+
             # Cleanup resources and tasks
             await self._cleanup_server()
-            
+
             self._logger.info("WebSocket JSON-RPC server stopped")
-            
+
         except Exception as e:
             self._logger.error(f"Error during WebSocket server shutdown: {e}")
             raise
 
     async def _process_request(self, path: str, request_headers) -> Optional[tuple]:
         """
         Process incoming WebSocket request to validate path and enforce limits.
-        
+
         Args:
             path: Request path
             request_headers: HTTP request headers
-            
+
         Returns:
             None to accept connection, or (status, headers, body) to reject
         """
         # Validate WebSocket path
         if path != self._websocket_path:
             self._logger.warning(f"Invalid WebSocket path requested: {path}")
             return (404, {}, b"Not Found")
-        
+
         # Check connection limits
         async with self._connection_lock:
             if len(self._clients) >= self._max_connections:
-                self._logger.warning(f"Connection limit reached: {len(self._clients)}/{self._max_connections}")
+                self._logger.warning(
+                    f"Connection limit reached: {len(self._clients)}/{self._max_connections}"
+                )
                 return (503, {}, b"Service Unavailable - Connection limit reached")
-        
+
         # Accept connection
         return None
 
-    async def _handle_client_connection(self, websocket: WebSocketServerProtocol, path: str) -> None:
+    async def _handle_client_connection(
+        self, websocket: WebSocketServerProtocol, path: str
+    ) -> None:
         """
         Handle new client WebSocket connection.
-        
+
         Args:
             websocket: WebSocket connection object
             path: Request path
         """
         client_id = str(uuid.uuid4())
-        client_ip = websocket.remote_address[0] if websocket.remote_address else "unknown"
-        
+        client_ip = (
+            websocket.remote_address[0] if websocket.remote_address else "unknown"
+        )
+
         self._logger.info(f"New client connection: {client_id} from {client_ip}")
-        
+
         # Create client connection object
         client = ClientConnection(websocket, client_id)
-        
+
         try:
             # Add client to tracking
             async with self._connection_lock:
                 self._clients[client_id] = client
-            
-            self._logger.debug(f"Client {client_id} added to connection pool ({len(self._clients)} total)")
-            
+
+            self._logger.debug(
+                f"Client {client_id} added to connection pool ({len(self._clients)} total)"
+            )
+
             # Handle authentication (basic implementation for MVP)
             # TODO: MEDIUM: Expand authentication per security architecture [IV&V:S6]
             client.authenticated = True
-            
+
             # Process incoming messages from client
             async for message in websocket:
                 try:
                     if isinstance(message, str):
                         response = await self._handle_json_rpc_message(client, message)
                         if response:
                             await websocket.send(response)
                     else:
-                        self._logger.warning(f"Received non-text message from client {client_id}")
-                        
+                        self._logger.warning(
+                            f"Received non-text message from client {client_id}"
+                        )
+
                 except Exception as e:
-                    self._logger.error(f"Error processing message from client {client_id}: {e}")
+                    self._logger.error(
+                        f"Error processing message from client {client_id}: {e}"
+                    )
                     # Send JSON-RPC error response if possible
                     try:
-                        error_response = json.dumps({
-                            "jsonrpc": "2.0",
-                            "error": {
-                                "code": -32603,
-                                "message": "Internal error"
-                            },
-                            "id": None
-                        })
+                        error_response = json.dumps(
+                            {
+                                "jsonrpc": "2.0",
+                                "error": {"code": -32603, "message": "Internal error"},
+                                "id": None,
+                            }
+                        )
                         await websocket.send(error_response)
                     except Exception:
                         # Connection might be broken, will be cleaned up below
                         break
-                        
+
         except ConnectionClosed:
             self._logger.info(f"Client {client_id} disconnected normally")
         except WebSocketException as e:
             self._logger.warning(f"WebSocket error for client {client_id}: {e}")
         except Exception as e:
@@ -300,16 +318,16 @@
         finally:
             # Cleanup on disconnect
             async with self._connection_lock:
                 if client_id in self._clients:
                     del self._clients[client_id]
-                    self._logger.info(f"Removed client {client_id} from connection pool ({len(self._clients)} remaining)")
+                    self._logger.info(
+                        f"Removed client {client_id} from connection pool ({len(self._clients)} remaining)"
+                    )
 
     async def _handle_json_rpc_message(
-        self, 
-        client: ClientConnection, 
-        message: str
+        self, client: ClientConnection, message: str
     ) -> Optional[str]:
         """
         Process incoming JSON-RPC message from client.
 
         Args:
@@ -319,160 +337,166 @@
         Returns:
             JSON-RPC response string or None for notifications
         """
         correlation_id = None
         request_id = None
-        
+
         try:
             # Parse JSON-RPC request
             try:
                 request_data = json.loads(message)
             except json.JSONDecodeError as e:
-                return json.dumps({
-                    "jsonrpc": "2.0",
-                    "error": {
-                        "code": -32700,
-                        "message": "Parse error"
-                    },
-                    "id": None
-                })
-            
+                return json.dumps(
+                    {
+                        "jsonrpc": "2.0",
+                        "error": {"code": -32700, "message": "Parse error"},
+                        "id": None,
+                    }
+                )
+
             # Extract correlation ID from request ID field
             request_id = request_data.get("id")
-            correlation_id = str(request_id) if request_id is not None else str(uuid.uuid4())[:8]
-            
+            correlation_id = (
+                str(request_id) if request_id is not None else str(uuid.uuid4())[:8]
+            )
+
             # Set correlation ID for structured logging
             set_correlation_id(correlation_id)
-            
+
             # Validate JSON-RPC structure
-            if not isinstance(request_data, dict) or request_data.get("jsonrpc") != "2.0":
-                return json.dumps({
-                    "jsonrpc": "2.0",
-                    "error": {
-                        "code": -32600,
-                        "message": "Invalid Request"
-                    },
-                    "id": request_id
-                })
-            
+            if (
+                not isinstance(request_data, dict)
+                or request_data.get("jsonrpc") != "2.0"
+            ):
+                return json.dumps(
+                    {
+                        "jsonrpc": "2.0",
+                        "error": {"code": -32600, "message": "Invalid Request"},
+                        "id": request_id,
+                    }
+                )
+
             method_name = request_data.get("method")
             if not method_name or not isinstance(method_name, str):
-                return json.dumps({
-                    "jsonrpc": "2.0",
-                    "error": {
-                        "code": -32600,
-                        "message": "Invalid Request - missing method"
-                    },
-                    "id": request_id
-                })
-            
+                return json.dumps(
+                    {
+                        "jsonrpc": "2.0",
+                        "error": {
+                            "code": -32600,
+                            "message": "Invalid Request - missing method",
+                        },
+                        "id": request_id,
+                    }
+                )
+
             params = request_data.get("params")
-            
-            self._logger.debug(f"Processing JSON-RPC method '{method_name}' from client {client.client_id}")
-            
+
+            self._logger.debug(
+                f"Processing JSON-RPC method '{method_name}' from client {client.client_id}"
+            )
+
             # Check if method exists
             if method_name not in self._method_handlers:
-                return json.dumps({
-                    "jsonrpc": "2.0",
-                    "error": {
-                        "code": -32601,
-                        "message": "Method not found"
-                    },
-                    "id": request_id
-                })
-            
+                return json.dumps(
+                    {
+                        "jsonrpc": "2.0",
+                        "error": {"code": -32601, "message": "Method not found"},
+                        "id": request_id,
+                    }
+                )
+
             # Call method handler
             try:
                 handler = self._method_handlers[method_name]
                 if params is not None:
                     result = await handler(params)
                 else:
                     result = await handler()
-                
+
                 # Return response for requests with ID (notifications have no ID)
                 if request_id is not None:
-                    return json.dumps({
-                        "jsonrpc": "2.0",
-                        "result": result,
-                        "id": request_id
-                    })
+                    return json.dumps(
+                        {"jsonrpc": "2.0", "result": result, "id": request_id}
+                    )
                 else:
                     # Notification - no response
                     return None
-                    
+
             except Exception as e:
                 self._logger.error(f"Error in method handler '{method_name}': {e}")
                 if request_id is not None:
-                    return json.dumps({
-                        "jsonrpc": "2.0",
-                        "error": {
-                            "code": -32603,
-                            "message": "Internal error"
-                        },
-                        "id": request_id
-                    })
+                    return json.dumps(
+                        {
+                            "jsonrpc": "2.0",
+                            "error": {"code": -32603, "message": "Internal error"},
+                            "id": request_id,
+                        }
+                    )
                 else:
                     return None
 
         except Exception as e:
-            self._logger.error(f"Error processing JSON-RPC message from client {client.client_id}: {e}")
-            return json.dumps({
-                "jsonrpc": "2.0",
-                "error": {
-                    "code": -32603,
-                    "message": "Internal error"
-                },
-                "id": request_id
-            })
+            self._logger.error(
+                f"Error processing JSON-RPC message from client {client.client_id}: {e}"
+            )
+            return json.dumps(
+                {
+                    "jsonrpc": "2.0",
+                    "error": {"code": -32603, "message": "Internal error"},
+                    "id": request_id,
+                }
+            )
 
     async def _close_all_connections(self) -> None:
         """Close all active client connections gracefully."""
         async with self._connection_lock:
             if not self._clients:
                 return
-            
+
             # Send shutdown notification to clients
-            shutdown_notification = json.dumps({
-                "jsonrpc": "2.0",
-                "method": "server_shutdown",
-                "params": {
-                    "message": "Server is shutting down"
+            shutdown_notification = json.dumps(
+                {
+                    "jsonrpc": "2.0",
+                    "method": "server_shutdown",
+                    "params": {"message": "Server is shutting down"},
                 }
-            })
-            
+            )
+
             # Close all WebSocket connections
             close_tasks = []
             for client in self._clients.values():
                 if client.websocket.open:
                     try:
                         # Send shutdown notification
                         close_tasks.append(client.websocket.send(shutdown_notification))
                     except Exception:
                         pass  # Ignore errors when sending shutdown notification
-                    
+
                     # Close connection
                     close_tasks.append(client.websocket.close())
-            
+
             # Wait for all connections to close
             if close_tasks:
                 await asyncio.gather(*close_tasks, return_exceptions=True)
-            
+
             # Clear client tracking
             client_count = len(self._clients)
             self._clients.clear()
             self._logger.info(f"Closed {client_count} client connections")
 
     async def _cleanup_server(self) -> None:
         """Clean up server resources and reset state."""
         self._server = None
         self._running = False
-        
+
         # Clear any remaining client references
         async with self._connection_lock:
             self._clients.clear()
 
-    def register_method(self, method_name: str, handler: Callable, version: str = "1.0") -> None:
+    def register_method(
+        self, method_name: str, handler: Callable, version: str = "1.0"
+    ) -> None:
         """
         Register a JSON-RPC method handler with version information.
 
         Args:
             method_name: Name of the JSON-RPC method
@@ -487,11 +511,11 @@
         self._logger.debug(f"Registered JSON-RPC method: {method_name} (v{version})")
 
     def unregister_method(self, method_name: str) -> None:
         """
         Unregister a JSON-RPC method handler.
-        
+
         Args:
             method_name: Name of the JSON-RPC method to remove
         """
         if method_name in self._method_handlers:
             del self._method_handlers[method_name]
@@ -508,14 +532,14 @@
             Version string if registered, else None
         """
         return self._method_versions.get(method_name)
 
     async def broadcast_notification(
-        self, 
-        method: str, 
+        self,
+        method: str,
         params: Optional[Dict[str, Any]] = None,
-        target_clients: Optional[List[str]] = None
+        target_clients: Optional[List[str]] = None,
     ) -> None:
         """
         Broadcast a JSON-RPC notification to connected clients.
 
         Args:
@@ -524,169 +548,182 @@
             target_clients: List of client IDs to notify (None for all clients)
         """
         if not self._clients:
             self._logger.debug(f"No clients connected, skipping notification: {method}")
             return
-        
+
         # Extract or generate correlation ID for notification tracing
         correlation_id = params.get("correlation_id") if params else None
         if not correlation_id:
             correlation_id = str(uuid.uuid4())[:8]
-        
+
         # Set correlation ID for structured logging
         set_correlation_id(correlation_id)
-        
+
         # Create JSON-RPC 2.0 notification structure
-        notification = JsonRpcNotification(
-            jsonrpc="2.0",
-            method=method,
-            params=params
-        )
-        
+        notification = JsonRpcNotification(jsonrpc="2.0", method=method, params=params)
+
         # Serialize notification to JSON
         try:
-            notification_json = json.dumps({
-                "jsonrpc": notification.jsonrpc,
-                "method": notification.method,
-                "params": notification.params
-            })
+            notification_json = json.dumps(
+                {
+                    "jsonrpc": notification.jsonrpc,
+                    "method": notification.method,
+                    "params": notification.params,
+                }
+            )
         except Exception as e:
             self._logger.error(f"Failed to serialize notification {method}: {e}")
             return
-            
+
         self._logger.debug(f"Broadcasting notification: {method}")
-        
+
         # Determine target clients
         if target_clients:
-            clients_to_notify = [self._clients[cid] for cid in target_clients if cid in self._clients]
+            clients_to_notify = [
+                self._clients[cid] for cid in target_clients if cid in self._clients
+            ]
         else:
             clients_to_notify = list(self._clients.values())
-        
+
         # Send notification to each target client
         failed_clients = []
         for client in clients_to_notify:
             try:
                 if client.websocket.open:
                     await client.websocket.send(notification_json)
                 else:
                     failed_clients.append(client.client_id)
             except Exception as e:
-                self._logger.warning(f"Failed to send notification to client {client.client_id}: {e}")
+                self._logger.warning(
+                    f"Failed to send notification to client {client.client_id}: {e}"
+                )
                 failed_clients.append(client.client_id)
-        
+
         # Clean up failed connections
         if failed_clients:
             async with self._connection_lock:
                 for client_id in failed_clients:
                     if client_id in self._clients:
                         del self._clients[client_id]
                         self._logger.info(f"Removed disconnected client: {client_id}")
-        
+
         success_count = len(clients_to_notify) - len(failed_clients)
-        self._logger.debug(f"Notification {method} sent to {success_count}/{len(clients_to_notify)} clients")
+        self._logger.debug(
+            f"Notification {method} sent to {success_count}/{len(clients_to_notify)} clients"
+        )
 
     async def send_notification_to_client(
-        self, 
-        client_id: str, 
-        method: str, 
-        params: Optional[Dict[str, Any]] = None
+        self, client_id: str, method: str, params: Optional[Dict[str, Any]] = None
     ) -> bool:
         """
         Send a JSON-RPC notification to a specific client.
-        
+
         Args:
             client_id: Target client identifier
             method: Notification method name
             params: Notification parameters
-            
+
         Returns:
             True if notification was sent successfully
         """
         # Validate client exists and is connected
         async with self._connection_lock:
             if client_id not in self._clients:
                 self._logger.warning(f"Client {client_id} not found for notification")
                 return False
-            
+
             client = self._clients[client_id]
-            
+
             if not client.websocket.open:
                 # Remove disconnected client
                 del self._clients[client_id]
-                self._logger.info(f"Removed disconnected client during notification: {client_id}")
+                self._logger.info(
+                    f"Removed disconnected client during notification: {client_id}"
+                )
                 return False
-        
+
         # Send notification to specific client
         try:
-            notification_json = json.dumps({
-                "jsonrpc": "2.0",
-                "method": method,
-                "params": params
-            })
-            
+            notification_json = json.dumps(
+                {"jsonrpc": "2.0", "method": method, "params": params}
+            )
+
             await client.websocket.send(notification_json)
             self._logger.debug(f"Sent notification '{method}' to client {client_id}")
             return True
-            
+
         except Exception as e:
-            self._logger.warning(f"Failed to send notification to client {client_id}: {e}")
+            self._logger.warning(
+                f"Failed to send notification to client {client_id}: {e}"
+            )
             # Handle send failure and connection cleanup
             async with self._connection_lock:
                 if client_id in self._clients:
                     del self._clients[client_id]
                     self._logger.info(f"Removed client after send failure: {client_id}")
             return False
 
     def _register_builtin_methods(self) -> None:
         """Register built-in JSON-RPC methods."""
         self.register_method("ping", self._method_ping, version="1.0")
-        self.register_method("get_camera_list", self._method_get_camera_list, version="1.0")
-        self.register_method("get_camera_status", self._method_get_camera_status, version="1.0")
+        self.register_method(
+            "get_camera_list", self._method_get_camera_list, version="1.0"
+        )
+        self.register_method(
+            "get_camera_status", self._method_get_camera_status, version="1.0"
+        )
         self.register_method("take_snapshot", self._method_take_snapshot, version="1.0")
-        self.register_method("start_recording", self._method_start_recording, version="1.0")
-        self.register_method("stop_recording", self._method_stop_recording, version="1.0")
+        self.register_method(
+            "start_recording", self._method_start_recording, version="1.0"
+        )
+        self.register_method(
+            "stop_recording", self._method_stop_recording, version="1.0"
+        )
         self._logger.debug("Registered built-in JSON-RPC methods")
 
     def _get_stream_name_from_device_path(self, device_path: str) -> str:
         """
         Extract stream name from camera device path.
-        
+
         Args:
             device_path: Camera device path (e.g., /dev/video0)
-            
+
         Returns:
             Stream name for MediaMTX (e.g., camera0)
         """
         try:
             # Extract device number from path like /dev/video0
-            if device_path.startswith('/dev/video'):
-                device_num = device_path.replace('/dev/video', '')
+            if device_path.startswith("/dev/video"):
+                device_num = device_path.replace("/dev/video", "")
                 return f"camera{device_num}"
             else:
                 # Fallback for non-standard device paths
                 return f"camera_{abs(hash(device_path)) % 1000}"
         except Exception:
             return "camera_unknown"
 
-    def _generate_filename(self, device_path: str, extension: str, custom_filename: Optional[str] = None) -> str:
+    def _generate_filename(
+        self, device_path: str, extension: str, custom_filename: Optional[str] = None
+    ) -> str:
         """
         Generate filename for snapshots and recordings.
-        
+
         Args:
             device_path: Camera device path
             extension: File extension (jpg, mp4, etc.)
             custom_filename: Custom filename if provided
-            
+
         Returns:
             Generated filename with timestamp
         """
         if custom_filename:
             # Ensure custom filename has correct extension
-            if not custom_filename.endswith(f'.{extension}'):
+            if not custom_filename.endswith(f".{extension}"):
                 return f"{custom_filename}.{extension}"
             return custom_filename
-        
+
         # Generate timestamp-based filename
         timestamp = time.strftime("%Y-%m-%d_%H-%M-%S")
         stream_name = self._get_stream_name_from_device_path(device_path)
         return f"{stream_name}_{timestamp}.{extension}"
 
@@ -700,11 +737,13 @@
         Returns:
             "pong" response string
         """
         return "pong"
 
-    async def _method_get_camera_list(self, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
+    async def _method_get_camera_list(
+        self, params: Optional[Dict[str, Any]] = None
+    ) -> Dict[str, Any]:
         """
         Get list of all discovered cameras with their current status and aggregated metadata.
 
         Integrates real data from camera discovery monitor (with provisional/confirmed capability logic)
         and MediaMTX controller. Returns architecture-compliant response structure.
@@ -715,92 +754,100 @@
         Returns:
             Object with camera list and metadata per API specification
         """
         if not self._camera_monitor:
             self._logger.warning("Camera monitor not available for get_camera_list")
-            return {
-                "cameras": [],
-                "total": 0,
-                "connected": 0
-            }
-        
+            return {"cameras": [], "total": 0, "connected": 0}
+
         try:
             # Get connected cameras from camera monitor
             connected_cameras = await self._camera_monitor.get_connected_cameras()
-            
+
             cameras = []
             connected_count = 0
-            
+
             for device_path, camera_device in connected_cameras.items():
                 # Get real capability metadata with provisional/confirmed logic
                 resolution = "1920x1080"  # Architecture default
-                fps = 30                  # Architecture default
-                
+                fps = 30  # Architecture default
+
                 # Use effective capability metadata (provisional or confirmed)
-                if hasattr(self._camera_monitor, 'get_effective_capability_metadata'):
+                if hasattr(self._camera_monitor, "get_effective_capability_metadata"):
                     try:
-                        capability_metadata = self._camera_monitor.get_effective_capability_metadata(device_path)
+                        capability_metadata = (
+                            self._camera_monitor.get_effective_capability_metadata(
+                                device_path
+                            )
+                        )
                         resolution = capability_metadata.get("resolution", resolution)
                         fps = capability_metadata.get("fps", fps)
-                        
+
                         # Log capability validation status for monitoring
-                        validation_status = capability_metadata.get("validation_status", "none")
+                        validation_status = capability_metadata.get(
+                            "validation_status", "none"
+                        )
                         if validation_status in ["provisional", "confirmed"]:
                             self._logger.debug(
                                 f"Using {validation_status} capability data for {device_path}: "
                                 f"{resolution}@{fps}fps"
                             )
                     except Exception as e:
-                        self._logger.debug(f"Could not get capability metadata for {device_path}: {e}")
-                
+                        self._logger.debug(
+                            f"Could not get capability metadata for {device_path}: {e}"
+                        )
+
                 # Generate stream name and URLs
                 stream_name = self._get_stream_name_from_device_path(device_path)
                 streams = {}
-                
+
                 # Get stream URLs from MediaMTX controller if available and camera connected
                 if self._mediamtx_controller and camera_device.status == "CONNECTED":
                     try:
-                        stream_status = await self._mediamtx_controller.get_stream_status(stream_name)
+                        stream_status = (
+                            await self._mediamtx_controller.get_stream_status(
+                                stream_name
+                            )
+                        )
                         if stream_status.get("status") == "active":
                             streams = {
                                 "rtsp": f"rtsp://localhost:8554/{stream_name}",
                                 "webrtc": f"http://localhost:8889/{stream_name}/webrtc",
-                                "hls": f"http://localhost:8888/{stream_name}"
+                                "hls": f"http://localhost:8888/{stream_name}",
                             }
                     except Exception as e:
-                        self._logger.debug(f"Could not get stream status for {stream_name}: {e}")
-                
+                        self._logger.debug(
+                            f"Could not get stream status for {stream_name}: {e}"
+                        )
+
                 # Build camera info per API specification
                 camera_info = {
                     "device": device_path,
                     "status": camera_device.status,
                     "name": camera_device.name,
                     "resolution": resolution,
                     "fps": fps,
-                    "streams": streams
+                    "streams": streams,
                 }
-                
+
                 cameras.append(camera_info)
-                
+
                 if camera_device.status == "CONNECTED":
                     connected_count += 1
-            
+
             return {
                 "cameras": cameras,
                 "total": len(cameras),
-                "connected": connected_count
+                "connected": connected_count,
             }
-            
+
         except Exception as e:
             self._logger.error(f"Error getting camera list: {e}")
-            return {
-                "cameras": [],
-                "total": 0,
-                "connected": 0
-            }
-
-    async def _method_get_camera_status(self, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
+            return {"cameras": [], "total": 0, "connected": 0}
+
+    async def _method_get_camera_status(
+        self, params: Optional[Dict[str, Any]] = None
+    ) -> Dict[str, Any]:
         """
         Get detailed status for a specific camera device with aggregated real data.
 
         Combines data from camera discovery monitor (with provisional/confirmed capability logic),
         MediaMTX controller (stream status and metrics), and provides graceful fallbacks.
@@ -811,106 +858,120 @@
 
         Returns:
             Dict containing comprehensive camera status per API specification:
                 - device, status, name, resolution, fps, streams, metrics, capabilities
         """
-        if not params or 'device' not in params:
+        if not params or "device" not in params:
             raise ValueError("device parameter is required")
-        
-        device_path = params['device']
-        
+
+        device_path = params["device"]
+
         # Initialize response with architecture defaults
         camera_status = {
             "device": device_path,
             "status": "DISCONNECTED",
             "name": f"Camera {device_path.split('video')[-1] if 'video' in device_path else 'unknown'}",
             "resolution": "1920x1080",  # Architecture default
-            "fps": 30,                   # Architecture default  
+            "fps": 30,  # Architecture default
             "streams": {},
-            "metrics": {
-                "bytes_sent": 0,
-                "readers": 0,
-                "uptime": 0
-            },
-            "capabilities": {
-                "formats": [],
-                "resolutions": []
-            }
+            "metrics": {"bytes_sent": 0, "readers": 0, "uptime": 0},
+            "capabilities": {"formats": [], "resolutions": []},
         }
-        
+
         try:
             # Get camera info from camera monitor
             if self._camera_monitor:
                 connected_cameras = await self._camera_monitor.get_connected_cameras()
                 camera_device = connected_cameras.get(device_path)
-                
+
                 if camera_device:
-                    camera_status.update({
-                        "status": camera_device.status,
-                        "name": camera_device.name
-                    })
-                    
+                    camera_status.update(
+                        {"status": camera_device.status, "name": camera_device.name}
+                    )
+
                     # Get real capability metadata with provisional/confirmed logic
                     if camera_device.status == "CONNECTED":
-                        if hasattr(self._camera_monitor, 'get_effective_capability_metadata'):
+                        if hasattr(
+                            self._camera_monitor, "get_effective_capability_metadata"
+                        ):
                             try:
-                                capability_metadata = self._camera_monitor.get_effective_capability_metadata(device_path)
-                                
+                                capability_metadata = self._camera_monitor.get_effective_capability_metadata(
+                                    device_path
+                                )
+
                                 # Use capability-derived resolution and fps
-                                camera_status.update({
-                                    "resolution": capability_metadata.get("resolution", "1920x1080"),
-                                    "fps": capability_metadata.get("fps", 30)
-                                })
-                                
+                                camera_status.update(
+                                    {
+                                        "resolution": capability_metadata.get(
+                                            "resolution", "1920x1080"
+                                        ),
+                                        "fps": capability_metadata.get("fps", 30),
+                                    }
+                                )
+
                                 # Update capabilities with real detected data
                                 if capability_metadata.get("formats"):
-                                    camera_status["capabilities"]["formats"] = capability_metadata["formats"]
+                                    camera_status["capabilities"]["formats"] = (
+                                        capability_metadata["formats"]
+                                    )
                                 if capability_metadata.get("all_resolutions"):
-                                    camera_status["capabilities"]["resolutions"] = capability_metadata["all_resolutions"]
-                                
+                                    camera_status["capabilities"]["resolutions"] = (
+                                        capability_metadata["all_resolutions"]
+                                    )
+
                                 # Log validation status for monitoring
-                                validation_status = capability_metadata.get("validation_status", "none")
+                                validation_status = capability_metadata.get(
+                                    "validation_status", "none"
+                                )
                                 self._logger.debug(
                                     f"Camera {device_path} using {validation_status} capability data: "
                                     f"{camera_status['resolution']}@{camera_status['fps']}fps"
                                 )
-                                    
+
                             except Exception as e:
-                                self._logger.debug(f"Could not get capability metadata for {device_path}: {e}")
-            
+                                self._logger.debug(
+                                    f"Could not get capability metadata for {device_path}: {e}"
+                                )
+
             # Get stream info and metrics from MediaMTX controller
             if self._mediamtx_controller and camera_status["status"] == "CONNECTED":
                 try:
                     stream_name = self._get_stream_name_from_device_path(device_path)
-                    stream_status = await self._mediamtx_controller.get_stream_status(stream_name)
-                    
+                    stream_status = await self._mediamtx_controller.get_stream_status(
+                        stream_name
+                    )
+
                     if stream_status.get("status") == "active":
                         # Update stream URLs
                         camera_status["streams"] = {
                             "rtsp": f"rtsp://localhost:8554/{stream_name}",
                             "webrtc": f"webrtc://localhost:8002/{stream_name}",
-                            "hls": f"http://localhost:8002/hls/{stream_name}.m3u8"
+                            "hls": f"http://localhost:8002/hls/{stream_name}.m3u8",
                         }
-                        
+
                         # Update metrics from MediaMTX
                         camera_status["metrics"] = {
                             "bytes_sent": stream_status.get("bytes_sent", 0),
                             "readers": stream_status.get("readers", 0),
-                            "uptime": int(time.time())  # Current uptime proxy
+                            "uptime": int(time.time()),  # Current uptime proxy
                         }
-                
+
                 except Exception as e:
-                    self._logger.debug(f"Could not get MediaMTX status for {device_path}: {e}")
-            
+                    self._logger.debug(
+                        f"Could not get MediaMTX status for {device_path}: {e}"
+                    )
+
             return camera_status
-            
+
         except Exception as e:
             self._logger.error(f"Error getting camera status for {device_path}: {e}")
             camera_status["status"] = "ERROR"
             return camera_status
 
-    async def _method_take_snapshot(self, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
+    async def _method_take_snapshot(
+        self, params: Optional[Dict[str, Any]] = None
+    ) -> Dict[str, Any]:
         """
         Capture a snapshot from the specified camera.
 
         Args:
             params: Method parameters containing:
@@ -918,56 +979,60 @@
                 - filename (str, optional): Custom filename
 
         Returns:
             Dict containing snapshot information
         """
-        if not params or 'device' not in params:
+        if not params or "device" not in params:
             raise ValueError("device parameter is required")
-        
-        device_path = params['device']
-        custom_filename = params.get('filename')
-        
+
+        device_path = params["device"]
+        custom_filename = params.get("filename")
+
         if not self._mediamtx_controller:
             return {
                 "device": device_path,
                 "filename": None,
                 "status": "FAILED",
                 "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                 "file_size": 0,
-                "error": "MediaMTX controller not available"
+                "error": "MediaMTX controller not available",
             }
-        
+
         try:
             stream_name = self._get_stream_name_from_device_path(device_path)
             filename = self._generate_filename(device_path, "jpg", custom_filename)
-            
+
             snapshot_result = await self._mediamtx_controller.take_snapshot(
-                stream_name=stream_name,
-                filename=filename
-            )
-            
+                stream_name=stream_name, filename=filename
+            )
+
             return {
                 "device": device_path,
                 "filename": snapshot_result.get("filename", filename),
                 "status": "completed",
                 "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                 "file_size": snapshot_result.get("file_size", 0),
-                "file_path": snapshot_result.get("file_path", f"/opt/camera-service/snapshots/{filename}")
+                "file_path": snapshot_result.get(
+                    "file_path", f"/opt/camera-service/snapshots/{filename}"
+                ),
             }
-            
+
         except Exception as e:
             self._logger.error(f"Error taking snapshot for {device_path}: {e}")
             return {
                 "device": device_path,
-                "filename": custom_filename or self._generate_filename(device_path, "jpg"),
+                "filename": custom_filename
+                or self._generate_filename(device_path, "jpg"),
                 "status": "FAILED",
                 "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                 "file_size": 0,
-                "error": str(e)
+                "error": str(e),
             }
 
-    async def _method_start_recording(self, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
+    async def _method_start_recording(
+        self, params: Optional[Dict[str, Any]] = None
+    ) -> Dict[str, Any]:
         """
         Start recording video from the specified camera.
 
         Args:
             params: Method parameters containing:
@@ -976,106 +1041,110 @@
                 - format (str, optional): Recording format
 
         Returns:
             Dict containing recording session information
         """
-        if not params or 'device' not in params:
+        if not params or "device" not in params:
             raise ValueError("device parameter is required")
-        
-        device_path = params['device']
-        duration = params.get('duration')
-        format_type = params.get('format', 'mp4')
-        
+
+        device_path = params["device"]
+        duration = params.get("duration")
+        format_type = params.get("format", "mp4")
+
         if not self._mediamtx_controller:
             return {
                 "device": device_path,
                 "session_id": None,
                 "filename": None,
                 "status": "FAILED",
                 "start_time": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                 "duration": duration,
                 "format": format_type,
-                "error": "MediaMTX controller not available"
+                "error": "MediaMTX controller not available",
             }
-        
+
         try:
             stream_name = self._get_stream_name_from_device_path(device_path)
             session_id = str(uuid.uuid4())
-            
+
             recording_result = await self._mediamtx_controller.start_recording(
-                stream_name=stream_name,
-                duration=duration,
-                format=format_type
-            )
-            
+                stream_name=stream_name, duration=duration, format=format_type
+            )
+
             return {
                 "device": device_path,
                 "session_id": session_id,
                 "filename": recording_result.get("filename"),
                 "status": "STARTED",
-                "start_time": recording_result.get("start_time", time.strftime("%Y-%m-%dT%H:%M:%SZ")),
+                "start_time": recording_result.get(
+                    "start_time", time.strftime("%Y-%m-%dT%H:%M:%SZ")
+                ),
                 "duration": duration,
-                "format": format_type
+                "format": format_type,
             }
-            
+
         except Exception as e:
             self._logger.error(f"Error starting recording for {device_path}: {e}")
             return {
                 "device": device_path,
                 "session_id": None,
                 "filename": None,
                 "status": "FAILED",
                 "start_time": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                 "duration": duration,
                 "format": format_type,
-                "error": str(e)
+                "error": str(e),
             }
 
-    async def _method_stop_recording(self, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
+    async def _method_stop_recording(
+        self, params: Optional[Dict[str, Any]] = None
+    ) -> Dict[str, Any]:
         """
         Stop active recording for the specified camera.
 
         Args:
             params: Method parameters containing:
                 - device (str): Camera device path
 
         Returns:
             Dict containing recording completion information
         """
-        if not params or 'device' not in params:
+        if not params or "device" not in params:
             raise ValueError("device parameter is required")
-        
-        device_path = params['device']
-        
+
+        device_path = params["device"]
+
         if not self._mediamtx_controller:
             return {
                 "device": device_path,
                 "session_id": None,
                 "filename": None,
                 "status": "FAILED",
                 "start_time": None,
                 "end_time": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                 "duration": 0,
                 "file_size": 0,
-                "error": "MediaMTX controller not available"
+                "error": "MediaMTX controller not available",
             }
-        
+
         try:
             stream_name = self._get_stream_name_from_device_path(device_path)
-            recording_result = await self._mediamtx_controller.stop_recording(stream_name)
-            
+            recording_result = await self._mediamtx_controller.stop_recording(
+                stream_name
+            )
+
             return {
                 "device": device_path,
                 "session_id": recording_result.get("session_id"),
                 "filename": recording_result.get("filename"),
                 "status": "STOPPED",
                 "start_time": recording_result.get("start_time"),
                 "end_time": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                 "duration": recording_result.get("duration", 0),
-                "file_size": recording_result.get("file_size", 0)
+                "file_size": recording_result.get("file_size", 0),
             }
-            
+
         except Exception as e:
             self._logger.error(f"Error stopping recording for {device_path}: {e}")
             return {
                 "device": device_path,
                 "session_id": None,
@@ -1083,11 +1152,11 @@
                 "status": "FAILED",
                 "start_time": None,
                 "end_time": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                 "duration": 0,
                 "file_size": 0,
-                "error": str(e)
+                "error": str(e),
             }
 
     async def notify_camera_status_update(self, params: Dict[str, Any]) -> None:
         """
         Broadcast camera_status_update notification with strict API compliance.
@@ -1099,35 +1168,40 @@
             params: Dictionary containing camera status fields
         """
         if not params:
             self._logger.warning("Camera status update called with empty parameters")
             return
-        
+
         # Validate required fields per API specification
-        required_fields = ['device', 'status']
+        required_fields = ["device", "status"]
         for field in required_fields:
             if field not in params:
-                self._logger.error(f"Camera status update missing required field: {field}")
+                self._logger.error(
+                    f"Camera status update missing required field: {field}"
+                )
                 return
-        
+
         # STRICT API COMPLIANCE: Filter to only allowed fields per specification
-        allowed_fields = {'device', 'status', 'name', 'resolution', 'fps', 'streams'}
+        allowed_fields = {"device", "status", "name", "resolution", "fps", "streams"}
         filtered_params = {k: v for k, v in params.items() if k in allowed_fields}
-        
+
         # Log filtered fields for monitoring compliance
         filtered_out = set(params.keys()) - allowed_fields
         if filtered_out:
-            self._logger.debug(f"Filtered out non-API fields from camera notification: {filtered_out}")
-        
+            self._logger.debug(
+                f"Filtered out non-API fields from camera notification: {filtered_out}"
+            )
+
         try:
             await self.broadcast_notification(
-                method="camera_status_update",
-                params=filtered_params
-            )
-            
-            self._logger.info(f"Broadcasted camera status update for device: {params.get('device')}")
-            
+                method="camera_status_update", params=filtered_params
+            )
+
+            self._logger.info(
+                f"Broadcasted camera status update for device: {params.get('device')}"
+            )
+
         except Exception as e:
             self._logger.error(f"Failed to broadcast camera status update: {e}")
 
     async def notify_recording_status_update(self, params: Dict[str, Any]) -> None:
         """
@@ -1140,55 +1214,60 @@
             params: Dictionary containing recording status fields
         """
         if not params:
             self._logger.warning("Recording status update called with empty parameters")
             return
-        
+
         # Validate required fields per API specification
-        required_fields = ['device', 'status']
+        required_fields = ["device", "status"]
         for field in required_fields:
             if field not in params:
-                self._logger.error(f"Recording status update missing required field: {field}")
+                self._logger.error(
+                    f"Recording status update missing required field: {field}"
+                )
                 return
-        
+
         # STRICT API COMPLIANCE: Filter to only allowed fields per specification
-        allowed_fields = {'device', 'status', 'filename', 'duration'}
+        allowed_fields = {"device", "status", "filename", "duration"}
         filtered_params = {k: v for k, v in params.items() if k in allowed_fields}
-        
+
         # Log filtered fields for monitoring compliance
         filtered_out = set(params.keys()) - allowed_fields
         if filtered_out:
-            self._logger.debug(f"Filtered out non-API fields from recording notification: {filtered_out}")
-        
+            self._logger.debug(
+                f"Filtered out non-API fields from recording notification: {filtered_out}"
+            )
+
         try:
             await self.broadcast_notification(
-                method="recording_status_update",
-                params=filtered_params
-            )
-            
-            self._logger.info(f"Broadcasted recording status update for device: {params.get('device')}, status: {params.get('status')}")
-            
+                method="recording_status_update", params=filtered_params
+            )
+
+            self._logger.info(
+                f"Broadcasted recording status update for device: {params.get('device')}, status: {params.get('status')}"
+            )
+
         except Exception as e:
             self._logger.error(f"Failed to broadcast recording status update: {e}")
 
     def get_connection_count(self) -> int:
         """Get current number of connected clients."""
         return len(self._clients)
 
     def get_server_stats(self) -> Dict[str, Any]:
         """
         Get server statistics and status.
-        
+
         Returns:
             Dictionary containing server metrics
         """
         return {
             "running": self._running,
             "connected_clients": len(self._clients),
             "max_connections": self._max_connections,
-            "registered_methods": len(self._method_handlers)
+            "registered_methods": len(self._method_handlers),
         }
 
     @property
     def is_running(self) -> bool:
         """Check if the server is currently running."""
-        return self._running
\ No newline at end of file
+        return self._running
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_discovery/test_capability_detection.py	2025-08-03 19:27:38.811778+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_discovery/test_capability_detection.py	2025-08-04 15:35:23.119261+00:00
@@ -1,8 +1,9 @@
 import asyncio
 import pytest
 from unittest.mock import Mock, AsyncMock, patch
+
 
 @pytest.mark.asyncio
 async def test_probe_device_capabilities_with_mock(monitor):
     # prepare mocked v4l2 outputs
     mock_info_output = b"Driver name   : uvcvideo\nCard type     : USB Camera\n"
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/src/camera_discovery/hybrid_monitor.py	2025-08-03 19:27:38.806057+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/src/camera_discovery/hybrid_monitor.py	2025-08-04 15:35:23.704382+00:00
@@ -22,34 +22,38 @@
 from ..common.types import CameraDevice
 
 # Optional dependency for udev monitoring
 try:
     import pyudev
+
     HAS_PYUDEV = True
 except ImportError:
     HAS_PYUDEV = False
 
 
 class CameraEvent(Enum):
     """Camera connection events."""
+
     CONNECTED = "CONNECTED"
     DISCONNECTED = "DISCONNECTED"
     STATUS_CHANGED = "STATUS_CHANGED"
 
 
 @dataclass
 class CameraEventData:
     """Data structure for camera events."""
+
     device_path: str
     event_type: CameraEvent
     device_info: Optional[CameraDevice] = None
     timestamp: Optional[float] = None
 
 
 @dataclass
 class CapabilityDetectionResult:
     """Structured result from capability detection."""
+
     device_path: str
     detected: bool
     accessible: bool
     device_name: Optional[str] = None
     driver: Optional[str] = None
@@ -58,11 +62,11 @@
     frame_rates: List[str] = None
     error: Optional[str] = None
     timeout_context: Optional[str] = None
     probe_timestamp: float = 0.0
     structured_diagnostics: Dict[str, Any] = None
-    
+
     def __post_init__(self):
         if self.formats is None:
             self.formats = []
         if self.resolutions is None:
             self.resolutions = []
@@ -75,177 +79,176 @@
 
 
 @dataclass
 class DeviceCapabilityState:
     """Tracks capability validation state for a device."""
+
     device_path: str
     provisional_data: Optional[CapabilityDetectionResult] = None
     confirmed_data: Optional[CapabilityDetectionResult] = None
     consecutive_successes: int = 0
     consecutive_failures: int = 0
     last_probe_time: float = 0.0
     confirmation_threshold: int = 2  # Require N consistent probes for confirmation
     validation_history: List[Dict[str, Any]] = None
-    
+
     # Enhanced frequency-based merge tracking
     format_frequency: Dict[str, int] = None
     resolution_frequency: Dict[str, int] = None
     frame_rate_frequency: Dict[str, int] = None
     stability_threshold: int = 3  # Require N detections for stable capability
-    
+
     def __post_init__(self):
         if self.validation_history is None:
             self.validation_history = []
         if self.format_frequency is None:
             self.format_frequency = {}
         if self.resolution_frequency is None:
             self.resolution_frequency = {}
         if self.frame_rate_frequency is None:
             self.frame_rate_frequency = {}
-    
+
     def get_effective_capability(self) -> Optional[CapabilityDetectionResult]:
         """Get the capability data to use (confirmed or provisional)."""
         return self.confirmed_data if self.confirmed_data else self.provisional_data
-    
+
     def is_confirmed(self) -> bool:
         """Check if current capability data is confirmed."""
         return self.confirmed_data is not None
 
 
 class CameraEventHandler(ABC):
     """Abstract interface for camera event handling."""
-    
+
     @abstractmethod
     async def handle_camera_event(self, event_data: CameraEventData) -> None:
         """
         Handle camera connection/disconnection events.
-        
+
         Args:
             event_data: Event information including device path and type
         """
         pass
 
 
 class HybridCameraMonitor:
     """
     Hybrid camera discovery monitor using udev events and polling fallback.
-    
+
     Implements the Camera Discovery Monitor component from the architecture,
     providing real-time USB camera detection with reliability through dual
     monitoring approaches.
-    
+
     Architecture Decision: Hybrid udev + polling approach provides real-time
     events when available while ensuring discovery completeness through polling.
     Priority order: udev events (real-time) > polling (fallback/validation).
     """
 
     def __init__(
         self,
         device_range: List[int] = None,
         poll_interval: float = 0.1,
         detection_timeout: float = 2.0,
-        enable_capability_detection: bool = True
+        enable_capability_detection: bool = True,
     ):
         """
         Initialize the hybrid camera monitor.
-        
+
         Args:
             device_range: List of video device numbers to monitor (e.g., [0, 1, 2])
             poll_interval: Polling interval in seconds for fallback monitoring
             detection_timeout: Timeout for camera capability detection
             enable_capability_detection: Whether to probe v4l2 capabilities
         """
         self._device_range = device_range or list(range(10))
         self._poll_interval = poll_interval
         self._detection_timeout = detection_timeout
         self._enable_capability_detection = enable_capability_detection
-        
+
         self._logger = logging.getLogger(__name__)
         self._running = False
         self._event_handlers: List[CameraEventHandler] = []
         self._event_callbacks: List[Callable[[CameraEventData], None]] = []
-        
+
         # Internal state tracking with thread safety considerations
         self._known_devices: Dict[str, CameraDevice] = {}
         self._capability_states: Dict[str, DeviceCapabilityState] = {}
         self._monitoring_tasks: List[asyncio.Task] = []
         self._state_lock = asyncio.Lock()  # Protect against race conditions
-        
+
         # Enhanced adaptive polling configuration
         self._base_poll_interval = poll_interval
         self._current_poll_interval = poll_interval
         self._min_poll_interval = max(0.05, poll_interval * 0.1)
         self._max_poll_interval = min(60.0, poll_interval * 50)
         self._last_udev_event_time = 0.0
         self._udev_event_freshness_threshold = 15.0  # seconds
         self._polling_failure_count = 0
         self._max_consecutive_failures = 5
-        
+
         # Enhanced frame rate patterns for robust parsing
         self._frame_rate_patterns = [
             # Standard patterns
-            r'(\d+(?:\.\d+)?)\s*fps\b',                       # 30.000 fps
-            r'(\d+(?:\.\d+)?)\s*FPS\b',                       # 30.000 FPS
-            r'Frame\s*rate[:\s]+(\d+(?:\.\d+)?)',             # Frame rate: 30.0
-            r'(\d+(?:\.\d+)?)\s*Hz\b',                        # 30 Hz
-            r'@(\d+(?:\.\d+)?)\b',                            # 1920x1080@60
-            
+            r"(\d+(?:\.\d+)?)\s*fps\b",  # 30.000 fps
+            r"(\d+(?:\.\d+)?)\s*FPS\b",  # 30.000 FPS
+            r"Frame\s*rate[:\s]+(\d+(?:\.\d+)?)",  # Frame rate: 30.0
+            r"(\d+(?:\.\d+)?)\s*Hz\b",  # 30 Hz
+            r"@(\d+(?:\.\d+)?)\b",  # 1920x1080@60
             # Interval patterns
-            r'Interval:\s*\[1/(\d+(?:\.\d+)?)\]',             # Interval: [1/30]
-            r'\[1/(\d+(?:\.\d+)?)\]',                         # [1/30]
-            r'1/(\d+(?:\.\d+)?)\s*s',                         # 1/30 s
-            
+            r"Interval:\s*\[1/(\d+(?:\.\d+)?)\]",  # Interval: [1/30]
+            r"\[1/(\d+(?:\.\d+)?)\]",  # [1/30]
+            r"1/(\d+(?:\.\d+)?)\s*s",  # 1/30 s
             # More complex patterns
-            r'(\d+(?:\.\d+)?)\s*frame[s]?\s*per\s*second',    # 30 frames per second
-            r'rate:\s*(\d+(?:\.\d+)?)',                       # rate: 30
-            r'fps:\s*(\d+(?:\.\d+)?)',                        # fps: 30
+            r"(\d+(?:\.\d+)?)\s*frame[s]?\s*per\s*second",  # 30 frames per second
+            r"rate:\s*(\d+(?:\.\d+)?)",  # rate: 30
+            r"fps:\s*(\d+(?:\.\d+)?)",  # fps: 30
         ]
-        
+
         # Udev monitoring objects
         self._udev_context: Optional[pyudev.Context] = None
         self._udev_monitor: Optional[pyudev.Monitor] = None
         self._udev_available = HAS_PYUDEV
-        
+
         # Enhanced diagnostic counters for observability
         self._stats = {
-            'udev_events_processed': 0,
-            'udev_events_filtered': 0,
-            'udev_events_skipped': 0,
-            'polling_cycles': 0,
-            'capability_probes_attempted': 0,
-            'capability_probes_successful': 0,
-            'capability_probes_confirmed': 0,
-            'capability_timeouts': 0,
-            'capability_parse_errors': 0,
-            'device_state_changes': 0,
-            'adaptive_poll_adjustments': 0,
-            'provisional_confirmations': 0,
-            'confirmation_failures': 0,
-            'current_poll_interval': poll_interval,
-            'running': False,
-            'active_tasks': 0
+            "udev_events_processed": 0,
+            "udev_events_filtered": 0,
+            "udev_events_skipped": 0,
+            "polling_cycles": 0,
+            "capability_probes_attempted": 0,
+            "capability_probes_successful": 0,
+            "capability_probes_confirmed": 0,
+            "capability_timeouts": 0,
+            "capability_parse_errors": 0,
+            "device_state_changes": 0,
+            "adaptive_poll_adjustments": 0,
+            "provisional_confirmations": 0,
+            "confirmation_failures": 0,
+            "current_poll_interval": poll_interval,
+            "running": False,
+            "active_tasks": 0,
         }
-        
+
         # Initialize deterministic random for jitter
         self._rng = random.Random()
         self._rng.seed(hashlib.md5(str(id(self)).encode()).hexdigest())
-        
+
         if not self._udev_available:
             self._logger.warning(
                 "pyudev not available - falling back to polling-only monitoring",
-                extra={'component': 'hybrid_monitor', 'mode': 'polling_only'}
-            )
-        
+                extra={"component": "hybrid_monitor", "mode": "polling_only"},
+            )
+
         self._logger.debug(
             f"Initialized HybridCameraMonitor with device_range={self._device_range}, "
             f"poll_interval={self._poll_interval}s, udev_available={self._udev_available}",
             extra={
-                'component': 'hybrid_monitor', 
-                'device_range': self._device_range,
-                'poll_interval': self._poll_interval,
-                'udev_available': self._udev_available
-            }
+                "component": "hybrid_monitor",
+                "device_range": self._device_range,
+                "poll_interval": self._poll_interval,
+                "udev_available": self._udev_available,
+            },
         )
 
     def add_event_handler(self, handler: CameraEventHandler) -> None:
         """Add a camera event handler."""
         self._event_handlers.append(handler)
@@ -262,59 +265,66 @@
         return self._running
 
     def get_monitor_stats(self) -> Dict[str, Any]:
         """Get monitoring statistics and diagnostic information."""
         stats = self._stats.copy()
-        stats['known_devices_count'] = len(self._known_devices)
-        stats['capability_states_count'] = len(self._capability_states)
-        stats['active_tasks'] = len([t for t in self._monitoring_tasks if not t.done()])
-        stats['running'] = self._running
+        stats["known_devices_count"] = len(self._known_devices)
+        stats["capability_states_count"] = len(self._capability_states)
+        stats["active_tasks"] = len([t for t in self._monitoring_tasks if not t.done()])
+        stats["running"] = self._running
         return stats
 
     async def start(self) -> None:
         """Start the camera monitoring system."""
         if self._running:
             self._logger.warning("Monitor already running")
             return
 
         self._running = True
-        self._stats['running'] = True
-        self._logger.info("Starting hybrid camera monitor", extra={'component': 'hybrid_monitor'})
+        self._stats["running"] = True
+        self._logger.info(
+            "Starting hybrid camera monitor", extra={"component": "hybrid_monitor"}
+        )
 
         try:
             # Initialize udev monitoring if available
             if self._udev_available:
                 await self._initialize_udev_monitoring()
 
             # Start polling fallback task
             polling_task = asyncio.create_task(self._adaptive_polling_loop())
             self._monitoring_tasks.append(polling_task)
-            
+
             if self._udev_available and self._udev_monitor:
                 udev_task = asyncio.create_task(self._udev_monitoring_loop())
                 self._monitoring_tasks.append(udev_task)
 
-            self._stats['active_tasks'] = len(self._monitoring_tasks)
+            self._stats["active_tasks"] = len(self._monitoring_tasks)
             self._logger.info(
                 f"Monitor started with {len(self._monitoring_tasks)} active tasks",
-                extra={'component': 'hybrid_monitor', 'task_count': len(self._monitoring_tasks)}
+                extra={
+                    "component": "hybrid_monitor",
+                    "task_count": len(self._monitoring_tasks),
+                },
             )
 
         except Exception as e:
             self._logger.error(f"Failed to start monitor: {e}", exc_info=True)
             self._running = False
-            self._stats['running'] = False
+            self._stats["running"] = False
             raise
 
     async def stop(self) -> None:
         """Stop the camera monitoring system."""
         if not self._running:
             return
 
-        self._logger.info("Stopping hybrid camera monitor", extra={'component': 'hybrid_monitor'})
+        self._logger.info(
+            "Stopping hybrid camera monitor", extra={"component": "hybrid_monitor"}
+        )
         self._running = False
-        self._stats['running'] = False
+        self._stats["running"] = False
 
         # Cancel all monitoring tasks
         for task in self._monitoring_tasks:
             if not task.done():
                 task.cancel()
@@ -322,273 +332,281 @@
         # Wait for tasks to complete
         if self._monitoring_tasks:
             await asyncio.gather(*self._monitoring_tasks, return_exceptions=True)
 
         self._monitoring_tasks.clear()
-        
+
         # Cleanup udev resources
         if self._udev_monitor:
             self._udev_monitor = None
         if self._udev_context:
             self._udev_context = None
 
-        self._stats['active_tasks'] = 0
-        self._logger.info("Monitor stopped", extra={'component': 'hybrid_monitor'})
+        self._stats["active_tasks"] = 0
+        self._logger.info("Monitor stopped", extra={"component": "hybrid_monitor"})
 
     async def _initialize_udev_monitoring(self) -> None:
         """Initialize udev monitoring for real-time device events."""
         try:
             self._udev_context = pyudev.Context()
             self._udev_monitor = pyudev.Monitor.from_netlink(self._udev_context)
-            self._udev_monitor.filter_by(subsystem='video4linux')
+            self._udev_monitor.filter_by(subsystem="video4linux")
             self._udev_monitor.start()
-            
+
             self._logger.info(
-                "Udev monitoring initialized", 
-                extra={'component': 'hybrid_monitor', 'subsystem': 'video4linux'}
+                "Udev monitoring initialized",
+                extra={"component": "hybrid_monitor", "subsystem": "video4linux"},
             )
         except Exception as e:
-            self._logger.error(f"Failed to initialize udev monitoring: {e}", exc_info=True)
+            self._logger.error(
+                f"Failed to initialize udev monitoring: {e}", exc_info=True
+            )
             self._udev_available = False
             self._udev_monitor = None
             self._udev_context = None
 
     async def _udev_monitoring_loop(self) -> None:
         """Main udev event monitoring loop."""
         self._logger.debug("Starting udev monitoring loop")
-        
+
         try:
             while self._running and self._udev_monitor:
                 try:
                     # Poll for udev events with timeout
                     device = self._udev_monitor.poll(timeout=1.0)
                     if device:
                         await self._process_udev_device_event(device)
-                        
+
                 except Exception as e:
-                    self._logger.error(f"Error in udev monitoring loop: {e}", exc_info=True)
+                    self._logger.error(
+                        f"Error in udev monitoring loop: {e}", exc_info=True
+                    )
                     await asyncio.sleep(1.0)
-                    
+
         except asyncio.CancelledError:
             self._logger.debug("Udev monitoring loop cancelled")
         except Exception as e:
-            self._logger.error(f"Critical error in udev monitoring loop: {e}", exc_info=True)
+            self._logger.error(
+                f"Critical error in udev monitoring loop: {e}", exc_info=True
+            )
 
     async def _process_udev_device_event(self, device) -> None:
         """Process individual udev device events with enhanced filtering and diagnostics."""
-        device_node = getattr(device, 'device_node', None)
-        action = getattr(device, 'action', None)
-        
+        device_node = getattr(device, "device_node", None)
+        action = getattr(device, "action", None)
+
         structured_event = {
-            'device_node': device_node,
-            'action': action,
-            'timestamp': time.time(),
-            'component': 'hybrid_monitor',
-            'event_type': 'udev'
+            "device_node": device_node,
+            "action": action,
+            "timestamp": time.time(),
+            "component": "hybrid_monitor",
+            "event_type": "udev",
         }
-        
+
         # Enhanced filtering with detailed logging
-        if not device_node or not device_node.startswith('/dev/video'):
-            self._stats['udev_events_filtered'] += 1
+        if not device_node or not device_node.startswith("/dev/video"):
+            self._stats["udev_events_filtered"] += 1
             self._logger.debug(
                 f"Filtered udev event - invalid device node: {device_node}",
-                extra=structured_event
+                extra=structured_event,
             )
             return
 
         # Extract device number for range validation
-        device_match = re.search(r'/dev/video(\d+)', device_node)
+        device_match = re.search(r"/dev/video(\d+)", device_node)
         if not device_match:
-            self._stats['udev_events_filtered'] += 1
+            self._stats["udev_events_filtered"] += 1
             self._logger.debug(
                 f"Filtered udev event - malformed device path: {device_node}",
-                extra=structured_event
+                extra=structured_event,
             )
             return
 
         device_num = int(device_match.group(1))
         if device_num not in self._device_range:
-            self._stats['udev_events_filtered'] += 1
+            self._stats["udev_events_filtered"] += 1
             self._logger.debug(
                 f"Filtered udev event - device {device_num} not in monitored range {self._device_range}",
-                extra=structured_event
+                extra=structured_event,
             )
             return
 
-        self._stats['udev_events_processed'] += 1
+        self._stats["udev_events_processed"] += 1
         self._last_udev_event_time = time.time()
-        
-        structured_event.update({
-            'device_num': device_num,
-            'processed': True
-        })
+
+        structured_event.update({"device_num": device_num, "processed": True})
 
         self._logger.info(
-            f"Processing udev {action} event for {device_node}",
-            extra=structured_event
+            f"Processing udev {action} event for {device_node}", extra=structured_event
         )
 
         try:
             # Process based on action with race condition protection
             async with self._state_lock:
-                if action == 'add':
+                if action == "add":
                     await self._handle_udev_device_added(device_node, device_num)
-                elif action == 'remove':
+                elif action == "remove":
                     await self._handle_udev_device_removed(device_node)
-                elif action == 'change':
+                elif action == "change":
                     await self._handle_udev_device_changed(device_node, device_num)
                 else:
-                    self._stats['udev_events_skipped'] += 1
+                    self._stats["udev_events_skipped"] += 1
                     self._logger.debug(
                         f"Skipped udev event with unknown action: {action}",
-                        extra=structured_event
+                        extra=structured_event,
                     )
 
         except Exception as e:
             self._logger.error(
                 f"Error processing udev {action} event for {device_node}: {e}",
                 extra=structured_event,
-                exc_info=True
-            )
-
-    async def _handle_udev_device_added(self, device_path: str, device_num: int) -> None:
+                exc_info=True,
+            )
+
+    async def _handle_udev_device_added(
+        self, device_path: str, device_num: int
+    ) -> None:
         """Handle udev 'add' event for device connection."""
         # Verify device is actually accessible before creating event
         device_info = await self._create_camera_device_info(device_path, device_num)
-        
+
         if device_info and device_info.status == "CONNECTED":
             event_data = CameraEventData(
                 device_path=device_path,
                 event_type=CameraEvent.CONNECTED,
                 device_info=device_info,
-                timestamp=time.time()
-            )
-            
+                timestamp=time.time(),
+            )
+
             # Update known devices and handle event
             self._known_devices[device_path] = device_info
-            self._stats['device_state_changes'] += 1
+            self._stats["device_state_changes"] += 1
             await self._handle_camera_event(event_data)
-            
+
             self._logger.info(
                 f"Device {device_path} connected via udev event",
                 extra={
-                    'device_path': device_path,
-                    'device_num': device_num,
-                    'event_source': 'udev_add'
-                }
+                    "device_path": device_path,
+                    "device_num": device_num,
+                    "event_source": "udev_add",
+                },
             )
         else:
             self._logger.warning(
                 f"Device {device_path} detected via udev 'add' but not accessible",
                 extra={
-                    'device_path': device_path,
-                    'device_num': device_num,
-                    'accessibility_check': 'failed'
-                }
+                    "device_path": device_path,
+                    "device_num": device_num,
+                    "accessibility_check": "failed",
+                },
             )
 
     async def _handle_udev_device_removed(self, device_path: str) -> None:
         """Handle udev 'remove' event for device disconnection."""
         device_info = self._known_devices.get(device_path)
         event_data = CameraEventData(
             device_path=device_path,
             event_type=CameraEvent.DISCONNECTED,
             device_info=device_info,
-            timestamp=time.time()
-        )
-        
+            timestamp=time.time(),
+        )
+
         # Remove from known devices and capability states
         had_device = device_path in self._known_devices
         if had_device:
             del self._known_devices[device_path]
-            self._stats['device_state_changes'] += 1
-        
+            self._stats["device_state_changes"] += 1
+
         if device_path in self._capability_states:
             del self._capability_states[device_path]
-        
+
         await self._handle_camera_event(event_data)
-        
+
         self._logger.info(
             f"Device {device_path} disconnected via udev event",
             extra={
-                'device_path': device_path,
-                'was_known': had_device,
-                'event_source': 'udev_remove'
-            }
-        )
-
-    async def _handle_udev_device_changed(self, device_path: str, device_num: int) -> None:
+                "device_path": device_path,
+                "was_known": had_device,
+                "event_source": "udev_remove",
+            },
+        )
+
+    async def _handle_udev_device_changed(
+        self, device_path: str, device_num: int
+    ) -> None:
         """Handle udev 'change' event for device property changes."""
         device_info = await self._create_camera_device_info(device_path, device_num)
         old_device_info = self._known_devices.get(device_path)
-        
+
         # Only generate event if status actually changed
-        if device_info and old_device_info and device_info.status != old_device_info.status:
+        if (
+            device_info
+            and old_device_info
+            and device_info.status != old_device_info.status
+        ):
             event_data = CameraEventData(
                 device_path=device_path,
                 event_type=CameraEvent.STATUS_CHANGED,
                 device_info=device_info,
-                timestamp=time.time()
-            )
-            
+                timestamp=time.time(),
+            )
+
             # Update known devices and handle event
             self._known_devices[device_path] = device_info
-            self._stats['device_state_changes'] += 1
+            self._stats["device_state_changes"] += 1
             await self._handle_camera_event(event_data)
-            
+
             self._logger.info(
                 f"Device {device_path} status changed: {old_device_info.status} → {device_info.status}",
                 extra={
-                    'device_path': device_path,
-                    'old_status': old_device_info.status,
-                    'new_status': device_info.status,
-                    'event_source': 'udev_change'
-                }
+                    "device_path": device_path,
+                    "old_status": old_device_info.status,
+                    "new_status": device_info.status,
+                    "event_source": "udev_change",
+                },
             )
         else:
             self._logger.debug(
                 f"No significant status change for {device_path}",
                 extra={
-                    'device_path': device_path,
-                    'event_source': 'udev_change',
-                    'status_check': 'no_change'
-                }
-            )
-
-    async def _create_camera_device_info(self, device_path: str, device_num: int) -> CameraDevice:
+                    "device_path": device_path,
+                    "event_source": "udev_change",
+                    "status_check": "no_change",
+                },
+            )
+
+    async def _create_camera_device_info(
+        self, device_path: str, device_num: int
+    ) -> CameraDevice:
         """Create CameraDevice info for a detected device."""
         # Basic accessibility check
         try:
             path_obj = Path(device_path)
             if not path_obj.exists():
                 return CameraDevice(
                     device_path=device_path,
                     name=f"Camera {device_num}",
-                    status="DISCONNECTED"
+                    status="DISCONNECTED",
                 )
         except Exception as e:
             self._logger.debug(f"Path check failed for {device_path}: {e}")
             return CameraDevice(
-                device_path=device_path,
-                name=f"Camera {device_num}",
-                status="ERROR"
+                device_path=device_path, name=f"Camera {device_num}", status="ERROR"
             )
 
         # Determine status based on basic accessibility
         try:
             # Try to open device for basic validation
-            with open(device_path, 'rb') as f:
+            with open(device_path, "rb") as f:
                 status = "CONNECTED"
         except (OSError, PermissionError):
             status = "DISCONNECTED"
         except Exception:
             status = "ERROR"
 
         device_info = CameraDevice(
-            device_path=device_path,
-            name=f"Camera {device_num}",
-            status=status
+            device_path=device_path, name=f"Camera {device_num}", status=status
         )
 
         # Trigger capability detection if enabled and device is accessible
         if self._enable_capability_detection and status == "CONNECTED":
             try:
@@ -599,199 +617,206 @@
         return device_info
 
     async def _adaptive_polling_loop(self) -> None:
         """
         Adaptive polling fallback loop with enhanced reliability and diagnostics.
-        
+
         Adapts polling frequency based on udev event reliability:
         - Reduces frequency when udev events are working reliably
         - Increases frequency when events are missed or stale
         - Exponential backoff on consecutive failures with jitter
         """
         self._logger.debug("Starting adaptive polling fallback loop")
-        
+
         polling_error_count = 0
-        
+
         try:
             while self._running:
                 loop_start = time.time()
-                
+
                 try:
                     # Perform discovery
                     await self._discover_cameras()
-                    self._stats['polling_cycles'] += 1
+                    self._stats["polling_cycles"] += 1
                     polling_error_count = 0  # Reset on success
                     self._polling_failure_count = 0
-                    
+
                     # Adaptive polling interval adjustment
                     await self._adjust_polling_interval()
-                    
+
                     # Sleep with consideration for loop execution time
                     loop_duration = time.time() - loop_start
                     sleep_time = max(0, self._current_poll_interval - loop_duration)
-                    
+
                     if sleep_time > 0:
                         await asyncio.sleep(sleep_time)
-                    
+
                 except Exception as e:
                     polling_error_count += 1
                     self._polling_failure_count += 1
-                    
+
                     structured_error = {
-                        'component': 'hybrid_monitor',
-                        'error_type': 'polling_discovery',
-                        'error_count': polling_error_count,
-                        'consecutive_failures': self._polling_failure_count
+                        "component": "hybrid_monitor",
+                        "error_type": "polling_discovery",
+                        "error_count": polling_error_count,
+                        "consecutive_failures": self._polling_failure_count,
                     }
-                    
+
                     self._logger.error(
                         f"Polling discovery error (#{polling_error_count}): {e}",
                         extra=structured_error,
-                        exc_info=True
+                        exc_info=True,
                     )
-                    
+
                     if polling_error_count >= self._max_consecutive_failures:
                         self._logger.critical(
                             f"Too many consecutive polling errors ({polling_error_count}), "
                             "stopping polling loop",
-                            extra=structured_error
+                            extra=structured_error,
                         )
                         break
-                    
+
                     # Enhanced exponential backoff with jitter
                     base_backoff = min(
-                        self._base_poll_interval * (2 ** self._polling_failure_count), 
-                        self._max_poll_interval
+                        self._base_poll_interval * (2**self._polling_failure_count),
+                        self._max_poll_interval,
                     )
                     jitter = self._rng.uniform(0.8, 1.2)  # ±20% jitter
                     backoff_interval = base_backoff * jitter
-                    
+
                     self._logger.debug(
                         f"Polling backoff: {backoff_interval:.2f}s (base: {base_backoff:.2f}s, jitter: {jitter:.2f})",
                         extra={
-                            'component': 'hybrid_monitor',
-                            'backoff_interval': backoff_interval,
-                            'base_backoff': base_backoff,
-                            'jitter_factor': jitter
-                        }
+                            "component": "hybrid_monitor",
+                            "backoff_interval": backoff_interval,
+                            "base_backoff": base_backoff,
+                            "jitter_factor": jitter,
+                        },
                     )
                     await asyncio.sleep(backoff_interval)
-                    
+
         except asyncio.CancelledError:
             self._logger.debug("Polling loop cancelled")
         except Exception as e:
             self._logger.error(f"Critical error in polling loop: {e}", exc_info=True)
 
     async def _adjust_polling_interval(self) -> None:
         """
         Adjust polling interval based on udev event reliability with enhanced logic.
-        
+
         Factors considered:
         - Time since last udev event
         - Recent polling failures
         - Overall system responsiveness
         """
         current_time = time.time()
         time_since_udev = current_time - self._last_udev_event_time
-        
+
         old_interval = self._current_poll_interval
-        
+
         # Determine new interval based on udev event freshness
         if time_since_udev > self._udev_event_freshness_threshold:
             # No recent udev events - increase polling frequency
             self._current_poll_interval = max(
-                self._min_poll_interval,
-                self._current_poll_interval * 0.8
+                self._min_poll_interval, self._current_poll_interval * 0.8
             )
         elif time_since_udev < self._udev_event_freshness_threshold / 2:
             # Recent udev events - can reduce polling frequency
             self._current_poll_interval = min(
-                self._max_poll_interval,
-                self._current_poll_interval * 1.2
-            )
-        
+                self._max_poll_interval, self._current_poll_interval * 1.2
+            )
+
         # Factor in recent failures
         if self._polling_failure_count > 0:
             failure_penalty = 1.0 + (self._polling_failure_count * 0.1)
             self._current_poll_interval = min(
-                self._max_poll_interval,
-                self._current_poll_interval * failure_penalty
-            )
-        
+                self._max_poll_interval, self._current_poll_interval * failure_penalty
+            )
+
         # Update stats if interval changed significantly
         if abs(self._current_poll_interval - old_interval) > 0.01:
-            self._stats['adaptive_poll_adjustments'] += 1
-            self._stats['current_poll_interval'] = self._current_poll_interval
-            
+            self._stats["adaptive_poll_adjustments"] += 1
+            self._stats["current_poll_interval"] = self._current_poll_interval
+
             self._logger.debug(
                 f"Adjusted polling interval: {old_interval:.2f}s → {self._current_poll_interval:.2f}s "
                 f"(udev_age: {time_since_udev:.1f}s, failures: {self._polling_failure_count})",
                 extra={
-                    'component': 'hybrid_monitor',
-                    'old_interval': old_interval,
-                    'new_interval': self._current_poll_interval,
-                    'udev_age': time_since_udev,
-                    'failure_count': self._polling_failure_count,
-                    'adjustment_reason': 'adaptive_tuning'
-                }
+                    "component": "hybrid_monitor",
+                    "old_interval": old_interval,
+                    "new_interval": self._current_poll_interval,
+                    "udev_age": time_since_udev,
+                    "failure_count": self._polling_failure_count,
+                    "adjustment_reason": "adaptive_tuning",
+                },
             )
 
     async def _discover_cameras(self) -> None:
         """Discover currently connected cameras via polling."""
         current_devices = {}
-        
+
         for device_num in self._device_range:
             device_path = f"/dev/video{device_num}"
-            
+
             try:
-                device_info = await self._create_camera_device_info(device_path, device_num)
+                device_info = await self._create_camera_device_info(
+                    device_path, device_num
+                )
                 if device_info and device_info.status in ["CONNECTED", "ERROR"]:
                     current_devices[device_path] = device_info
             except Exception as e:
                 self._logger.debug(f"Error checking device {device_path}: {e}")
                 continue
-        
+
         # Compare with known devices and generate events
         await self._process_device_state_changes(current_devices)
 
-    async def _process_device_state_changes(self, current_devices: Dict[str, CameraDevice]) -> None:
+    async def _process_device_state_changes(
+        self, current_devices: Dict[str, CameraDevice]
+    ) -> None:
         """Process changes in device state between polling cycles."""
         async with self._state_lock:
             # Detect new devices
             for device_path, device_info in current_devices.items():
                 if device_path not in self._known_devices:
-                    await self._handle_camera_event(CameraEventData(
-                        device_path=device_path,
-                        event_type=CameraEvent.CONNECTED,
-                        device_info=device_info,
-                        timestamp=time.time()
-                    ))
-                    self._stats['device_state_changes'] += 1
-            
+                    await self._handle_camera_event(
+                        CameraEventData(
+                            device_path=device_path,
+                            event_type=CameraEvent.CONNECTED,
+                            device_info=device_info,
+                            timestamp=time.time(),
+                        )
+                    )
+                    self._stats["device_state_changes"] += 1
+
             # Detect removed devices
             for device_path in list(self._known_devices.keys()):
                 if device_path not in current_devices:
-                    await self._handle_camera_event(CameraEventData(
-                        device_path=device_path,
-                        event_type=CameraEvent.DISCONNECTED,
-                        device_info=self._known_devices[device_path],
-                        timestamp=time.time()
-                    ))
-                    self._stats['device_state_changes'] += 1
-            
+                    await self._handle_camera_event(
+                        CameraEventData(
+                            device_path=device_path,
+                            event_type=CameraEvent.DISCONNECTED,
+                            device_info=self._known_devices[device_path],
+                            timestamp=time.time(),
+                        )
+                    )
+                    self._stats["device_state_changes"] += 1
+
             # Detect status changes for existing devices
             for device_path, device_info in current_devices.items():
                 if device_path in self._known_devices:
                     if self._known_devices[device_path].status != device_info.status:
-                        await self._handle_camera_event(CameraEventData(
-                            device_path=device_path,
-                            event_type=CameraEvent.STATUS_CHANGED,
-                            device_info=device_info,
-                            timestamp=time.time()
-                        ))
-                        self._stats['device_state_changes'] += 1
-            
+                        await self._handle_camera_event(
+                            CameraEventData(
+                                device_path=device_path,
+                                event_type=CameraEvent.STATUS_CHANGED,
+                                device_info=device_info,
+                                timestamp=time.time(),
+                            )
+                        )
+                        self._stats["device_state_changes"] += 1
+
             # Update known devices
             self._known_devices = current_devices.copy()
 
     async def _handle_camera_event(self, event_data: CameraEventData) -> None:
         """Handle camera events by notifying all registered handlers and callbacks."""
@@ -799,182 +824,197 @@
             # Call all event handlers
             for handler in self._event_handlers:
                 try:
                     await handler.handle_camera_event(event_data)
                 except Exception as e:
-                    self._logger.error(f"Error in event handler {handler.__class__.__name__}: {e}", exc_info=True)
-            
+                    self._logger.error(
+                        f"Error in event handler {handler.__class__.__name__}: {e}",
+                        exc_info=True,
+                    )
+
             # Call all event callbacks
             for callback in self._event_callbacks:
                 try:
                     callback(event_data)
                 except Exception as e:
-                    self._logger.error(f"Error in event callback {callback.__name__}: {e}", exc_info=True)
-                    
+                    self._logger.error(
+                        f"Error in event callback {callback.__name__}: {e}",
+                        exc_info=True,
+                    )
+
         except Exception as e:
-            self._logger.error(f"Critical error handling camera event: {e}", exc_info=True)
-
-    async def _probe_device_capabilities(self, device_path: str) -> CapabilityDetectionResult:
+            self._logger.error(
+                f"Critical error handling camera event: {e}", exc_info=True
+            )
+
+    async def _probe_device_capabilities(
+        self, device_path: str
+    ) -> CapabilityDetectionResult:
         """
         Probe device capabilities with enhanced error handling and structured diagnostics.
-        
+
         Args:
             device_path: Path to video device
-            
+
         Returns:
             Structured capability detection result with enhanced diagnostics
         """
-        self._stats['capability_probes_attempted'] += 1
+        self._stats["capability_probes_attempted"] += 1
         probe_start = time.time()
-        
+
         result = CapabilityDetectionResult(
             device_path=device_path,
             detected=False,
             accessible=False,
             probe_timestamp=probe_start,
             structured_diagnostics={
-                'probe_start': probe_start,
-                'timeout_threshold': self._detection_timeout,
-                'parsing_stages': []
-            }
-        )
-        
+                "probe_start": probe_start,
+                "timeout_threshold": self._detection_timeout,
+                "parsing_stages": [],
+            },
+        )
+
         try:
             # Basic device info probe
             device_info = await self._probe_device_info_robust(device_path)
             if device_info:
-                result.device_name = device_info.get('name')
-                result.driver = device_info.get('driver')
+                result.device_name = device_info.get("name")
+                result.driver = device_info.get("driver")
                 result.accessible = True
-                result.structured_diagnostics['device_info_success'] = True
+                result.structured_diagnostics["device_info_success"] = True
             else:
                 result.error = "Failed to probe basic device information"
-                result.structured_diagnostics['device_info_success'] = False
+                result.structured_diagnostics["device_info_success"] = False
                 return result
-            
+
             # Format and resolution probe
             formats_data = await self._probe_device_formats_robust(device_path)
             if formats_data:
-                result.formats = formats_data.get('formats', [])
-                result.resolutions = formats_data.get('resolutions', [])
-                result.structured_diagnostics['formats_found'] = len(result.formats)
-                result.structured_diagnostics['resolutions_found'] = len(result.resolutions)
-            
+                result.formats = formats_data.get("formats", [])
+                result.resolutions = formats_data.get("resolutions", [])
+                result.structured_diagnostics["formats_found"] = len(result.formats)
+                result.structured_diagnostics["resolutions_found"] = len(
+                    result.resolutions
+                )
+
             # Frame rate probe with hierarchical selection
             frame_rates = await self._probe_device_framerates_robust(device_path)
             if frame_rates:
                 result.frame_rates = frame_rates
-                result.structured_diagnostics['frame_rates_found'] = len(frame_rates)
-            
+                result.structured_diagnostics["frame_rates_found"] = len(frame_rates)
+
             # Consider detection successful if we got basic info
-            if result.accessible and (result.formats or result.resolutions or result.frame_rates):
+            if result.accessible and (
+                result.formats or result.resolutions or result.frame_rates
+            ):
                 result.detected = True
-                self._stats['capability_probes_successful'] += 1
-                
+                self._stats["capability_probes_successful"] += 1
+
                 # Update capability validation state
                 await self._update_capability_validation_state(device_path, result)
             else:
                 result.error = "Insufficient capability data detected"
-                
+
         except asyncio.TimeoutError:
-            self._stats['capability_timeouts'] += 1
-            result.error = f"Capability detection timeout after {self._detection_timeout}s"
+            self._stats["capability_timeouts"] += 1
+            result.error = (
+                f"Capability detection timeout after {self._detection_timeout}s"
+            )
             result.timeout_context = "overall_probe_timeout"
             self._logger.warning(
                 f"Capability detection timeout for {device_path}",
                 extra={
-                    'device_path': device_path,
-                    'timeout_duration': self._detection_timeout,
-                    'component': 'capability_detection'
-                }
+                    "device_path": device_path,
+                    "timeout_duration": self._detection_timeout,
+                    "component": "capability_detection",
+                },
             )
         except Exception as e:
-            self._stats['capability_parse_errors'] += 1
+            self._stats["capability_parse_errors"] += 1
             result.error = f"Capability detection error: {str(e)}"
-            result.structured_diagnostics['exception_type'] = type(e).__name__
+            result.structured_diagnostics["exception_type"] = type(e).__name__
             self._logger.error(
                 f"Error probing capabilities for {device_path}: {e}",
-                extra={
-                    'device_path': device_path,
-                    'component': 'capability_detection'
-                },
-                exc_info=True
-            )
-        
+                extra={"device_path": device_path, "component": "capability_detection"},
+                exc_info=True,
+            )
+
         probe_duration = time.time() - probe_start
-        result.structured_diagnostics['probe_duration'] = probe_duration
-        
+        result.structured_diagnostics["probe_duration"] = probe_duration
+
         return result
 
-    async def _probe_device_info_robust(self, device_path: str) -> Optional[Dict[str, str]]:
+    async def _probe_device_info_robust(
+        self, device_path: str
+    ) -> Optional[Dict[str, str]]:
         """Probe basic device information with enhanced error handling."""
         try:
             process = await asyncio.wait_for(
                 asyncio.create_subprocess_exec(
-                    'v4l2-ctl', '--device', device_path, '--info',
+                    "v4l2-ctl",
+                    "--device",
+                    device_path,
+                    "--info",
                     stdout=asyncio.subprocess.PIPE,
-                    stderr=asyncio.subprocess.PIPE
+                    stderr=asyncio.subprocess.PIPE,
                 ),
-                timeout=self._detection_timeout
-            )
-            
+                timeout=self._detection_timeout,
+            )
+
             stdout, stderr = await process.communicate()
-            
+
             if process.returncode != 0:
                 return None
-            
+
             output = stdout.decode()
             info = {}
-            
+
             # Enhanced parsing with multiple patterns
             name_patterns = [
-                r'Card type\s*:\s*(.+)',
-                r'Device name\s*:\s*(.+)',
-                r'Card\s*:\s*(.+)'
+                r"Card type\s*:\s*(.+)",
+                r"Device name\s*:\s*(.+)",
+                r"Card\s*:\s*(.+)",
             ]
-            
-            driver_patterns = [
-                r'Driver name\s*:\s*(.+)',
-                r'Driver\s*:\s*(.+)'
-            ]
-            
+
+            driver_patterns = [r"Driver name\s*:\s*(.+)", r"Driver\s*:\s*(.+)"]
+
             for pattern in name_patterns:
                 match = re.search(pattern, output, re.IGNORECASE)
                 if match:
-                    info['name'] = match.group(1).strip()
+                    info["name"] = match.group(1).strip()
                     break
-            
+
             for pattern in driver_patterns:
                 match = re.search(pattern, output, re.IGNORECASE)
                 if match:
-                    info['driver'] = match.group(1).strip()
+                    info["driver"] = match.group(1).strip()
                     break
-            
+
             return info if info else None
-            
+
         except asyncio.TimeoutError:
             self._logger.debug(f"Timeout probing device info for {device_path}")
             return None
         except Exception as e:
             self._logger.debug(f"Error probing device info for {device_path}: {e}")
             return None
 
     def _extract_frame_rates_from_output(self, output: str) -> Set[str]:
         """
         Extract frame rates from v4l2-ctl output using enhanced parsing strategies.
-        
+
         Args:
             output: Raw v4l2-ctl command output
-            
+
         Returns:
             Set of frame rate strings
         """
         if not output or not output.strip():
             return set()
-        
+
         frame_rates = set()
-        
+
         # Apply all frame rate patterns
         for pattern in self._frame_rate_patterns:
             try:
                 matches = re.findall(pattern, output, re.IGNORECASE)
                 for match in matches:
@@ -990,363 +1030,415 @@
                     except (ValueError, TypeError):
                         continue
             except re.error:
                 # Skip malformed patterns
                 continue
-        
+
         return frame_rates
 
-    async def _probe_device_framerates_robust(self, device_path: str) -> Optional[List[str]]:
+    async def _probe_device_framerates_robust(
+        self, device_path: str
+    ) -> Optional[List[str]]:
         """
         Probe supported frame rates with hierarchical selection and robust parsing.
-        
+
         Enhanced frame rate detection with multiple strategies and preference ordering.
         """
         all_frame_rates = set()
         detection_sources = []
-        
+
         # Enhanced command list with more comprehensive probing
         commands_to_try = [
-            (['v4l2-ctl', '--device', device_path, '--list-framesizes', 'YUYV'], "YUYV framesizes"),
-            (['v4l2-ctl', '--device', device_path, '--list-framesizes', 'MJPG'], "MJPG framesizes"),
-            (['v4l2-ctl', '--device', device_path, '--list-framesizes', 'RGB24'], "RGB24 framesizes"),
-            (['v4l2-ctl', '--device', device_path, '--list-framerates'], "general framerates"),
-            (['v4l2-ctl', '--device', device_path, '--list-formats-ext'], "extended formats"),
-            (['v4l2-ctl', '--device', device_path, '--all'], "all device info")
+            (
+                ["v4l2-ctl", "--device", device_path, "--list-framesizes", "YUYV"],
+                "YUYV framesizes",
+            ),
+            (
+                ["v4l2-ctl", "--device", device_path, "--list-framesizes", "MJPG"],
+                "MJPG framesizes",
+            ),
+            (
+                ["v4l2-ctl", "--device", device_path, "--list-framesizes", "RGB24"],
+                "RGB24 framesizes",
+            ),
+            (
+                ["v4l2-ctl", "--device", device_path, "--list-framerates"],
+                "general framerates",
+            ),
+            (
+                ["v4l2-ctl", "--device", device_path, "--list-formats-ext"],
+                "extended formats",
+            ),
+            (["v4l2-ctl", "--device", device_path, "--all"], "all device info"),
         ]
-        
+
         for cmd, description in commands_to_try:
             try:
                 process = await asyncio.wait_for(
                     asyncio.create_subprocess_exec(
                         *cmd,
                         stdout=asyncio.subprocess.PIPE,
-                        stderr=asyncio.subprocess.PIPE
+                        stderr=asyncio.subprocess.PIPE,
                     ),
-                    timeout=self._detection_timeout
+                    timeout=self._detection_timeout,
                 )
-                
+
                 stdout, stderr = await process.communicate()
-                
+
                 if process.returncode == 0:
                     output = stdout.decode()
                     cmd_frame_rates = self._extract_frame_rates_from_output(output)
-                    
+
                     if cmd_frame_rates:
                         all_frame_rates.update(cmd_frame_rates)
                         detection_sources.append((description, cmd_frame_rates))
                         self._logger.debug(
                             f"Found frame rates from {description}: {sorted(cmd_frame_rates)}",
                             extra={
-                                'device_path': device_path,
-                                'detection_source': description,
-                                'frame_rates': sorted(cmd_frame_rates)
-                            }
+                                "device_path": device_path,
+                                "detection_source": description,
+                                "frame_rates": sorted(cmd_frame_rates),
+                            },
                         )
-                        
+
             except asyncio.TimeoutError:
                 self._logger.debug(f"Timeout getting {description} for {device_path}")
                 continue
             except Exception as e:
-                self._logger.debug(f"Error getting {description} for {device_path}: {e}")
+                self._logger.debug(
+                    f"Error getting {description} for {device_path}: {e}"
+                )
                 continue
-        
+
         # Apply hierarchical frame rate selection
         if all_frame_rates:
-            return self._select_preferred_frame_rates(all_frame_rates, detection_sources, device_path)
+            return self._select_preferred_frame_rates(
+                all_frame_rates, detection_sources, device_path
+            )
         else:
             # Return common default frame rates if detection fails
             default_rates = ["30", "25", "24", "15", "10", "5"]
             self._logger.debug(
                 f"No frame rates detected for {device_path}, using defaults: {default_rates}",
                 extra={
-                    'device_path': device_path,
-                    'fallback_used': True,
-                    'default_rates': default_rates
-                }
+                    "device_path": device_path,
+                    "fallback_used": True,
+                    "default_rates": default_rates,
+                },
             )
             return default_rates
 
     def _select_preferred_frame_rates(
-        self, 
-        all_rates: Set[str], 
-        sources: List[Tuple[str, Set[str]]], 
-        device_path: str
+        self, all_rates: Set[str], sources: List[Tuple[str, Set[str]]], device_path: str
     ) -> List[str]:
         """
         Select preferred frame rates using enhanced hierarchical policy.
-        
+
         Enhanced policy:
         1. Highest stable frame rate preferred for given resolution
         2. Common frame rates (30, 25, 24, 15) prioritized
         3. Rates detected by multiple sources weighted higher
         4. Consistent ordering for deterministic behavior
         """
-        
+
         # Define preference tiers
-        high_priority_rates = {'30', '25', '24'}
-        medium_priority_rates = {'15', '60', '10'}
-        
+        high_priority_rates = {"30", "25", "24"}
+        medium_priority_rates = {"15", "60", "10"}
+
         # Count detection frequency (reliability indicator)
         rate_frequency = {}
         for source_name, source_rates in sources:
             for rate in source_rates:
                 rate_frequency[rate] = rate_frequency.get(rate, 0) + 1
-        
+
         # Sort by multiple criteria
         def rate_sort_key(rate: str) -> Tuple[int, int, float]:
             try:
                 rate_val = float(rate)
-                
+
                 # Priority tier (lower is better)
                 if rate in high_priority_rates:
                     priority = 0
                 elif rate in medium_priority_rates:
                     priority = 1
                 else:
                     priority = 2
-                
+
                 # Detection frequency (higher is better, so negate)
                 frequency = -rate_frequency.get(rate, 1)
-                
+
                 # Rate value for final ordering (higher is better, so negate)
                 rate_value = -rate_val
-                
+
                 return (priority, frequency, rate_value)
             except (ValueError, TypeError):
                 # Invalid rates go to the end
                 return (999, 0, 0)
-        
+
         sorted_rates = sorted(all_rates, key=rate_sort_key)
-        
+
         self._logger.debug(
             f"Selected frame rate order for {device_path}: {sorted_rates}",
             extra={
-                'device_path': device_path,
-                'rate_frequency': rate_frequency,
-                'final_order': sorted_rates,
-                'selection_criteria': 'hierarchical_policy'
-            }
-        )
-        
+                "device_path": device_path,
+                "rate_frequency": rate_frequency,
+                "final_order": sorted_rates,
+                "selection_criteria": "hierarchical_policy",
+            },
+        )
+
         return sorted_rates
 
     async def _update_capability_validation_state(
-        self, 
-        device_path: str, 
-        new_result: CapabilityDetectionResult
+        self, device_path: str, new_result: CapabilityDetectionResult
     ) -> None:
         """
         Update capability validation state with enhanced frequency-based merge logic.
-        
+
         Uses weighted merge based on detection frequency with stability thresholds:
         - Tracks frequency of each capability element across probes
         - Promotes capabilities that meet stability threshold to stable set
         - Filters out one-off detections that may be transient
         - Prevents oscillation through frequency-based consistency validation
-        
+
         Args:
             device_path: Device path
             new_result: New capability detection result
         """
         if device_path not in self._capability_states:
             self._capability_states[device_path] = DeviceCapabilityState(
                 device_path=device_path
             )
-        
+
         state = self._capability_states[device_path]
         state.last_probe_time = time.time()
-        
+
         # Add to validation history
         history_entry = {
-            'timestamp': state.last_probe_time,
-            'detected': new_result.detected,
-            'error': new_result.error,
-            'formats_count': len(new_result.formats),
-            'resolutions_count': len(new_result.resolutions),
-            'frame_rates_count': len(new_result.frame_rates)
+            "timestamp": state.last_probe_time,
+            "detected": new_result.detected,
+            "error": new_result.error,
+            "formats_count": len(new_result.formats),
+            "resolutions_count": len(new_result.resolutions),
+            "frame_rates_count": len(new_result.frame_rates),
         }
         state.validation_history.append(history_entry)
-        
+
         # Keep history manageable
         if len(state.validation_history) > 10:
             state.validation_history = state.validation_history[-10:]
-        
+
         if new_result.detected:
             # Update frequency tracking for all capability elements
             self._update_capability_frequencies(state, new_result)
-            
+
             # Generate frequency-based merged capability data
             merged_result = self._create_frequency_merged_capability(state, new_result)
-            
+
             # Check stability-aware consistency with existing data
             if state.provisional_data and self._is_frequency_based_consistent(
                 state, merged_result
             ):
                 state.consecutive_successes += 1
                 state.consecutive_failures = 0
-                
+
                 # Promote to confirmed if threshold met
                 if state.consecutive_successes >= state.confirmation_threshold:
                     if not state.confirmed_data:
-                        self._stats['provisional_confirmations'] += 1
-                        stable_formats = len([f for f, freq in state.format_frequency.items() if freq >= state.stability_threshold])
-                        stable_resolutions = len([r for r, freq in state.resolution_frequency.items() if freq >= state.stability_threshold])
-                        stable_rates = len([r for r, freq in state.frame_rate_frequency.items() if freq >= state.stability_threshold])
-                        
+                        self._stats["provisional_confirmations"] += 1
+                        stable_formats = len(
+                            [
+                                f
+                                for f, freq in state.format_frequency.items()
+                                if freq >= state.stability_threshold
+                            ]
+                        )
+                        stable_resolutions = len(
+                            [
+                                r
+                                for r, freq in state.resolution_frequency.items()
+                                if freq >= state.stability_threshold
+                            ]
+                        )
+                        stable_rates = len(
+                            [
+                                r
+                                for r, freq in state.frame_rate_frequency.items()
+                                if freq >= state.stability_threshold
+                            ]
+                        )
+
                         self._logger.info(
                             f"Capability data confirmed for {device_path} after {state.consecutive_successes} consistent probes "
                             f"(stable: {stable_formats} formats, {stable_resolutions} resolutions, {stable_rates} rates)",
                             extra={
-                                'device_path': device_path,
-                                'consecutive_successes': state.consecutive_successes,
-                                'stable_capabilities': {
-                                    'formats': stable_formats,
-                                    'resolutions': stable_resolutions,
-                                    'frame_rates': stable_rates
+                                "device_path": device_path,
+                                "consecutive_successes": state.consecutive_successes,
+                                "stable_capabilities": {
+                                    "formats": stable_formats,
+                                    "resolutions": stable_resolutions,
+                                    "frame_rates": stable_rates,
                                 },
-                                'validation_transition': 'provisional_to_confirmed',
-                                'merge_strategy': 'frequency_based'
-                            }
+                                "validation_transition": "provisional_to_confirmed",
+                                "merge_strategy": "frequency_based",
+                            },
                         )
-                    
+
                     state.confirmed_data = merged_result
-                    self._stats['capability_probes_confirmed'] += 1
+                    self._stats["capability_probes_confirmed"] += 1
                 else:
                     self._logger.debug(
                         f"Capability data frequency-consistency maintained for {device_path} "
                         f"({state.consecutive_successes}/{state.confirmation_threshold})",
                         extra={
-                            'device_path': device_path,
-                            'progress': f"{state.consecutive_successes}/{state.confirmation_threshold}",
-                            'validation_status': 'provisional_consistent',
-                            'merge_strategy': 'frequency_based'
-                        }
+                            "device_path": device_path,
+                            "progress": f"{state.consecutive_successes}/{state.confirmation_threshold}",
+                            "validation_status": "provisional_consistent",
+                            "merge_strategy": "frequency_based",
+                        },
                     )
             else:
                 # Inconsistent data - check if it's minor variance or major change
-                variance_score = self._calculate_capability_variance(state, merged_result)
-                
-                if variance_score < 0.3:  # Minor variance - continue with frequency merge
+                variance_score = self._calculate_capability_variance(
+                    state, merged_result
+                )
+
+                if (
+                    variance_score < 0.3
+                ):  # Minor variance - continue with frequency merge
                     state.consecutive_successes += 1
                     state.consecutive_failures = 0
                     self._logger.debug(
                         f"Minor capability variance for {device_path} (score: {variance_score:.2f}), "
                         f"continuing frequency-based merge",
                         extra={
-                            'device_path': device_path,
-                            'variance_score': variance_score,
-                            'validation_action': 'continue_with_variance'
-                        }
+                            "device_path": device_path,
+                            "variance_score": variance_score,
+                            "validation_action": "continue_with_variance",
+                        },
                     )
                 else:
                     # Major variance - reset validation but preserve frequency data
                     if state.provisional_data:
                         self._logger.warning(
                             f"Major capability variance detected for {device_path} (score: {variance_score:.2f}), "
                             f"resetting validation but preserving frequency data",
                             extra={
-                                'device_path': device_path,
-                                'variance_score': variance_score,
-                                'validation_action': 'reset_with_frequency_preservation'
-                            }
+                                "device_path": device_path,
+                                "variance_score": variance_score,
+                                "validation_action": "reset_with_frequency_preservation",
+                            },
                         )
-                    
+
                     state.consecutive_successes = 1
                     state.consecutive_failures = 0
                     state.confirmed_data = None
-                
+
             state.provisional_data = merged_result
         else:
             # Detection failed
             state.consecutive_failures += 1
             state.consecutive_successes = 0
-            
+
             if state.consecutive_failures >= 3:
-                self._stats['confirmation_failures'] += 1
+                self._stats["confirmation_failures"] += 1
                 self._logger.warning(
                     f"Capability detection failing consistently for {device_path} "
                     f"({state.consecutive_failures} failures)",
                     extra={
-                        'device_path': device_path,
-                        'consecutive_failures': state.consecutive_failures,
-                        'validation_status': 'persistent_failure'
-                    }
+                        "device_path": device_path,
+                        "consecutive_failures": state.consecutive_failures,
+                        "validation_status": "persistent_failure",
+                    },
                 )
 
     def _update_capability_frequencies(
-        self, 
-        state: DeviceCapabilityState, 
-        result: CapabilityDetectionResult
+        self, state: DeviceCapabilityState, result: CapabilityDetectionResult
     ) -> None:
         """Update frequency counters for capability elements."""
-        
+
         # Update format frequencies
         for fmt in result.formats:
-            fmt_code = fmt.get('code', '') if isinstance(fmt, dict) else str(fmt)
+            fmt_code = fmt.get("code", "") if isinstance(fmt, dict) else str(fmt)
             if fmt_code:
-                state.format_frequency[fmt_code] = state.format_frequency.get(fmt_code, 0) + 1
-        
+                state.format_frequency[fmt_code] = (
+                    state.format_frequency.get(fmt_code, 0) + 1
+                )
+
         # Update resolution frequencies
         for resolution in result.resolutions:
             if resolution:
-                state.resolution_frequency[resolution] = state.resolution_frequency.get(resolution, 0) + 1
-        
+                state.resolution_frequency[resolution] = (
+                    state.resolution_frequency.get(resolution, 0) + 1
+                )
+
         # Update frame rate frequencies
         for rate in result.frame_rates:
             if rate:
-                state.frame_rate_frequency[rate] = state.frame_rate_frequency.get(rate, 0) + 1
+                state.frame_rate_frequency[rate] = (
+                    state.frame_rate_frequency.get(rate, 0) + 1
+                )
 
     def _create_frequency_merged_capability(
-        self, 
-        state: DeviceCapabilityState, 
-        latest_result: CapabilityDetectionResult
+        self, state: DeviceCapabilityState, latest_result: CapabilityDetectionResult
     ) -> CapabilityDetectionResult:
         """
         Create merged capability result based on frequency analysis.
-        
+
         Includes capabilities that meet stability threshold and weights by frequency.
         """
-        
+
         # Filter capabilities by stability threshold and sort by frequency
         stable_formats = [
             {"code": fmt, "description": f"Format {fmt}"}
-            for fmt, freq in sorted(state.format_frequency.items(), key=lambda x: x[1], reverse=True)
+            for fmt, freq in sorted(
+                state.format_frequency.items(), key=lambda x: x[1], reverse=True
+            )
             if freq >= state.stability_threshold
         ]
-        
+
         stable_resolutions = [
             resolution
-            for resolution, freq in sorted(state.resolution_frequency.items(), key=lambda x: x[1], reverse=True)
+            for resolution, freq in sorted(
+                state.resolution_frequency.items(), key=lambda x: x[1], reverse=True
+            )
             if freq >= state.stability_threshold
         ]
-        
+
         stable_frame_rates = [
             rate
-            for rate, freq in sorted(state.frame_rate_frequency.items(), key=lambda x: x[1], reverse=True)
+            for rate, freq in sorted(
+                state.frame_rate_frequency.items(), key=lambda x: x[1], reverse=True
+            )
             if freq >= state.stability_threshold
         ]
-        
+
         # Include recent detections if they don't conflict with stable set
         recent_formats = []
         for fmt in latest_result.formats:
-            fmt_code = fmt.get('code', '') if isinstance(fmt, dict) else str(fmt)
-            if fmt_code and fmt_code not in [sf['code'] for sf in stable_formats]:
+            fmt_code = fmt.get("code", "") if isinstance(fmt, dict) else str(fmt)
+            if fmt_code and fmt_code not in [sf["code"] for sf in stable_formats]:
                 # Add if it's been seen at least once before or has high confidence
                 if state.format_frequency.get(fmt_code, 0) > 0:
                     recent_formats.append(fmt)
-        
+
         recent_resolutions = [
-            res for res in latest_result.resolutions
-            if res and res not in stable_resolutions and state.resolution_frequency.get(res, 0) > 0
+            res
+            for res in latest_result.resolutions
+            if res
+            and res not in stable_resolutions
+            and state.resolution_frequency.get(res, 0) > 0
         ]
-        
+
         recent_frame_rates = [
-            rate for rate in latest_result.frame_rates
-            if rate and rate not in stable_frame_rates and state.frame_rate_frequency.get(rate, 0) > 0
+            rate
+            for rate in latest_result.frame_rates
+            if rate
+            and rate not in stable_frame_rates
+            and state.frame_rate_frequency.get(rate, 0) > 0
         ]
-        
+
         # Create merged result
         merged_result = CapabilityDetectionResult(
             device_path=latest_result.device_path,
             detected=True,
             accessible=latest_result.accessible,
@@ -1356,217 +1448,220 @@
             resolutions=stable_resolutions + recent_resolutions,
             frame_rates=stable_frame_rates + recent_frame_rates,
             probe_timestamp=latest_result.probe_timestamp,
             structured_diagnostics={
                 **latest_result.structured_diagnostics,
-                'merge_strategy': 'frequency_based',
-                'stable_elements': {
-                    'formats': len(stable_formats),
-                    'resolutions': len(stable_resolutions),
-                    'frame_rates': len(stable_frame_rates)
+                "merge_strategy": "frequency_based",
+                "stable_elements": {
+                    "formats": len(stable_formats),
+                    "resolutions": len(stable_resolutions),
+                    "frame_rates": len(stable_frame_rates),
                 },
-                'recent_elements': {
-                    'formats': len(recent_formats),
-                    'resolutions': len(recent_resolutions),
-                    'frame_rates': len(recent_frame_rates)
+                "recent_elements": {
+                    "formats": len(recent_formats),
+                    "resolutions": len(recent_resolutions),
+                    "frame_rates": len(recent_frame_rates),
                 },
-                'frequency_data': {
-                    'format_frequencies': dict(state.format_frequency),
-                    'resolution_frequencies': dict(state.resolution_frequency),
-                    'frame_rate_frequencies': dict(state.frame_rate_frequency)
-                }
-            }
-        )
-        
+                "frequency_data": {
+                    "format_frequencies": dict(state.format_frequency),
+                    "resolution_frequencies": dict(state.resolution_frequency),
+                    "frame_rate_frequencies": dict(state.frame_rate_frequency),
+                },
+            },
+        )
+
         return merged_result
 
     def _is_frequency_based_consistent(
-        self, 
-        state: DeviceCapabilityState,
-        new_result: CapabilityDetectionResult
+        self, state: DeviceCapabilityState, new_result: CapabilityDetectionResult
     ) -> bool:
         """
         Check consistency using frequency-based stability analysis.
-        
+
         More lenient than intersection-based consistency - allows for capability
         variance as long as core stable capabilities remain consistent.
         """
         if not state.provisional_data or not new_result.detected:
             return False
-        
+
         # Check that stable capabilities (high frequency) remain present
         stable_formats = {
-            fmt for fmt, freq in state.format_frequency.items() 
+            fmt
+            for fmt, freq in state.format_frequency.items()
             if freq >= state.stability_threshold
         }
         stable_resolutions = {
-            res for res, freq in state.resolution_frequency.items() 
+            res
+            for res, freq in state.resolution_frequency.items()
             if freq >= state.stability_threshold
         }
         stable_frame_rates = {
-            rate for rate, freq in state.frame_rate_frequency.items() 
+            rate
+            for rate, freq in state.frame_rate_frequency.items()
             if freq >= state.stability_threshold
         }
-        
+
         # Get current result capability sets
-        current_formats = {fmt.get('code', '') for fmt in new_result.formats if isinstance(fmt, dict)}
+        current_formats = {
+            fmt.get("code", "") for fmt in new_result.formats if isinstance(fmt, dict)
+        }
         current_resolutions = set(new_result.resolutions)
         current_frame_rates = set(new_result.frame_rates)
-        
+
         # Calculate consistency scores for each capability type
-        format_consistency = self._calculate_set_consistency(stable_formats, current_formats)
-        resolution_consistency = self._calculate_set_consistency(stable_resolutions, current_resolutions)
-        rate_consistency = self._calculate_set_consistency(stable_frame_rates, current_frame_rates)
-        
+        format_consistency = self._calculate_set_consistency(
+            stable_formats, current_formats
+        )
+        resolution_consistency = self._calculate_set_consistency(
+            stable_resolutions, current_resolutions
+        )
+        rate_consistency = self._calculate_set_consistency(
+            stable_frame_rates, current_frame_rates
+        )
+
         # Require high consistency for stable capabilities
         min_consistency = 0.7  # 70% of stable capabilities should be present
-        
+
         overall_consistent = (
-            format_consistency >= min_consistency and
-            resolution_consistency >= min_consistency and
-            rate_consistency >= min_consistency
-        )
-        
+            format_consistency >= min_consistency
+            and resolution_consistency >= min_consistency
+            and rate_consistency >= min_consistency
+        )
+
         return overall_consistent
 
-    def _calculate_set_consistency(self, stable_set: Set[str], current_set: Set[str]) -> float:
+    def _calculate_set_consistency(
+        self, stable_set: Set[str], current_set: Set[str]
+    ) -> float:
         """Calculate consistency score between stable and current capability sets."""
         if not stable_set:
             return 1.0  # No stable capabilities to validate against
-        
+
         intersection = stable_set.intersection(current_set)
         return len(intersection) / len(stable_set)
 
     def _calculate_capability_variance(
-        self, 
-        state: DeviceCapabilityState, 
-        new_result: CapabilityDetectionResult
+        self, state: DeviceCapabilityState, new_result: CapabilityDetectionResult
     ) -> float:
         """
         Calculate variance score between frequency-merged capabilities and new result.
-        
+
         Returns:
             float: Variance score (0.0 = identical, 1.0 = completely different)
         """
         if not state.provisional_data:
             return 0.0
-        
+
         prev_result = state.provisional_data
-        
+
         # Calculate variance for each capability type
         format_variance = self._calculate_list_variance(
-            [f.get('code', '') for f in prev_result.formats],
-            [f.get('code', '') for f in new_result.formats]
-        )
-        
+            [f.get("code", "") for f in prev_result.formats],
+            [f.get("code", "") for f in new_result.formats],
+        )
+
         resolution_variance = self._calculate_list_variance(
-            prev_result.resolutions,
-            new_result.resolutions
-        )
-        
+            prev_result.resolutions, new_result.resolutions
+        )
+
         rate_variance = self._calculate_list_variance(
-            prev_result.frame_rates,
-            new_result.frame_rates
-        )
-        
+            prev_result.frame_rates, new_result.frame_rates
+        )
+
         # Weight variance by importance (resolutions and rates more critical than formats)
         weighted_variance = (
-            format_variance * 0.2 +
-            resolution_variance * 0.4 +
-            rate_variance * 0.4
-        )
-        
+            format_variance * 0.2 + resolution_variance * 0.4 + rate_variance * 0.4
+        )
+
         return weighted_variance
 
     def _calculate_list_variance(self, list1: List[str], list2: List[str]) -> float:
         """Calculate variance between two lists (Jaccard distance)."""
         set1, set2 = set(list1), set(list2)
-        
+
         if not set1 and not set2:
             return 0.0
-        
+
         if not set1 or not set2:
             return 1.0
-        
+
         intersection = len(set1.intersection(set2))
         union = len(set1.union(set2))
-        
+
         # Jaccard similarity = intersection / union
         # Jaccard distance = 1 - Jaccard similarity
         return 1.0 - (intersection / union if union > 0 else 0.0)
 
     def _is_capability_data_consistent(
-        self, 
-        data1: CapabilityDetectionResult, 
-        data2: CapabilityDetectionResult
+        self, data1: CapabilityDetectionResult, data2: CapabilityDetectionResult
     ) -> bool:
         """
         Check if two capability detection results are consistent.
-        
+
         Enhanced consistency checking with tolerance for minor variations.
         """
         if not (data1.detected and data2.detected):
             return False
-        
+
         # Check format consistency (allow subset relationships)
-        formats1 = set(f.get('code', '') for f in data1.formats)
-        formats2 = set(f.get('code', '') for f in data2.formats)
-        
+        formats1 = set(f.get("code", "") for f in data1.formats)
+        formats2 = set(f.get("code", "") for f in data2.formats)
+
         if formats1 and formats2:
             # Allow up to 50% difference in formats
             intersection = formats1.intersection(formats2)
             min_formats = min(len(formats1), len(formats2))
             if min_formats > 0 and len(intersection) / min_formats < 0.5:
                 return False
-        
+
         # Check resolution consistency (allow subset relationships)
         resolutions1 = set(data1.resolutions)
         resolutions2 = set(data2.resolutions)
-        
+
         if resolutions1 and resolutions2:
             intersection = resolutions1.intersection(resolutions2)
             min_resolutions = min(len(resolutions1), len(resolutions2))
             if min_resolutions > 0 and len(intersection) / min_resolutions < 0.5:
                 return False
-        
+
         # Check frame rate consistency (more lenient)
         frame_rates1 = set(data1.frame_rates)
         frame_rates2 = set(data2.frame_rates)
-        
+
         if frame_rates1 and frame_rates2:
             intersection = frame_rates1.intersection(frame_rates2)
             min_rates = min(len(frame_rates1), len(frame_rates2))
             if min_rates > 0 and len(intersection) / min_rates < 0.3:
                 return False
-        
+
         return True
 
     def get_effective_capability_metadata(self, device_path: str) -> Dict[str, Any]:
         """
         Get effective capability metadata for a device with enhanced frequency-based diagnostics.
-        
+
         Returns confirmed data if available, otherwise provisional data.
         Includes comprehensive stability indicators and frequency analysis for diagnostics.
         """
         if device_path not in self._capability_states:
             return {
                 "resolution": "1920x1080",  # Architecture default
-                "fps": 30,                  # Architecture default
+                "fps": 30,  # Architecture default
                 "validation_status": "none",
                 "formats": [],
                 "all_resolutions": [],
                 "all_frame_rates": [],
                 "diagnostics": {
                     "reason": "no_capability_state",
                     "has_been_probed": False,
-                    "merge_strategy": "default"
-                }
+                    "merge_strategy": "default",
+                },
             }
-        
+
         capability_state = self._capability_states[device_path]
         effective_data = capability_state.get_effective_capability()
-        
+
         if not effective_data or not effective_data.detected:
             return {
                 "resolution": "1920x1080",
                 "fps": 30,
                 "validation_status": "failed",
@@ -1576,211 +1671,274 @@
                 "all_frame_rates": [],
                 "diagnostics": {
                     "reason": "detection_failed",
                     "consecutive_failures": capability_state.consecutive_failures,
                     "last_error": effective_data.error if effective_data else None,
-                    "merge_strategy": "none"
-                }
+                    "merge_strategy": "none",
+                },
             }
-        
+
         # Select primary resolution and frame rate with frequency-based preference
-        primary_resolution = effective_data.resolutions[0] if effective_data.resolutions else "1920x1080"
-        primary_fps_str = effective_data.frame_rates[0] if effective_data.frame_rates else "30"
-        
+        primary_resolution = (
+            effective_data.resolutions[0] if effective_data.resolutions else "1920x1080"
+        )
+        primary_fps_str = (
+            effective_data.frame_rates[0] if effective_data.frame_rates else "30"
+        )
+
         try:
             primary_fps = int(float(primary_fps_str))
         except (ValueError, TypeError):
             primary_fps = 30
-        
+
         # Calculate stability metrics
-        stable_formats = len([f for f, freq in capability_state.format_frequency.items() 
-                            if freq >= capability_state.stability_threshold])
-        stable_resolutions = len([r for r, freq in capability_state.resolution_frequency.items() 
-                                if freq >= capability_state.stability_threshold])
-        stable_rates = len([r for r, freq in capability_state.frame_rate_frequency.items() 
-                          if freq >= capability_state.stability_threshold])
-        
+        stable_formats = len(
+            [
+                f
+                for f, freq in capability_state.format_frequency.items()
+                if freq >= capability_state.stability_threshold
+            ]
+        )
+        stable_resolutions = len(
+            [
+                r
+                for r, freq in capability_state.resolution_frequency.items()
+                if freq >= capability_state.stability_threshold
+            ]
+        )
+        stable_rates = len(
+            [
+                r
+                for r, freq in capability_state.frame_rate_frequency.items()
+                if freq >= capability_state.stability_threshold
+            ]
+        )
+
         # Calculate frequency-based confidence scores
         total_probes = len(capability_state.validation_history)
-        resolution_confidence = (capability_state.resolution_frequency.get(primary_resolution, 0) / 
-                                max(total_probes, 1)) if total_probes > 0 else 0.0
-        rate_confidence = (capability_state.frame_rate_frequency.get(primary_fps_str, 0) / 
-                         max(total_probes, 1)) if total_probes > 0 else 0.0
-        
+        resolution_confidence = (
+            (
+                capability_state.resolution_frequency.get(primary_resolution, 0)
+                / max(total_probes, 1)
+            )
+            if total_probes > 0
+            else 0.0
+        )
+        rate_confidence = (
+            (
+                capability_state.frame_rate_frequency.get(primary_fps_str, 0)
+                / max(total_probes, 1)
+            )
+            if total_probes > 0
+            else 0.0
+        )
+
         return {
             "resolution": primary_resolution,
             "fps": primary_fps,
-            "validation_status": "confirmed" if capability_state.is_confirmed() else "provisional",
+            "validation_status": (
+                "confirmed" if capability_state.is_confirmed() else "provisional"
+            ),
             "consecutive_successes": capability_state.consecutive_successes,
             "consecutive_failures": capability_state.consecutive_failures,
             "formats": [f.get("code", "") for f in effective_data.formats],
             "all_resolutions": effective_data.resolutions,
             "all_frame_rates": effective_data.frame_rates,
             "device_name": effective_data.device_name,
             "driver": effective_data.driver,
             "diagnostics": {
                 "probe_timestamp": effective_data.probe_timestamp,
-                "probe_duration": effective_data.structured_diagnostics.get('probe_duration'),
+                "probe_duration": effective_data.structured_diagnostics.get(
+                    "probe_duration"
+                ),
                 "validation_history_length": len(capability_state.validation_history),
                 "confirmation_progress": f"{capability_state.consecutive_successes}/{capability_state.confirmation_threshold}",
                 "merge_strategy": "frequency_based",
                 "stability_metrics": {
                     "stable_formats": stable_formats,
                     "stable_resolutions": stable_resolutions,
                     "stable_frame_rates": stable_rates,
-                    "stability_threshold": capability_state.stability_threshold
+                    "stability_threshold": capability_state.stability_threshold,
                 },
                 "frequency_analysis": {
                     "total_probes": total_probes,
                     "primary_resolution_confidence": round(resolution_confidence, 2),
                     "primary_rate_confidence": round(rate_confidence, 2),
-                    "top_formats": sorted(capability_state.format_frequency.items(), 
-                                        key=lambda x: x[1], reverse=True)[:3],
-                    "top_resolutions": sorted(capability_state.resolution_frequency.items(), 
-                                            key=lambda x: x[1], reverse=True)[:3],
-                    "top_frame_rates": sorted(capability_state.frame_rate_frequency.items(), 
-                                            key=lambda x: x[1], reverse=True)[:3]
-                }
-            }
+                    "top_formats": sorted(
+                        capability_state.format_frequency.items(),
+                        key=lambda x: x[1],
+                        reverse=True,
+                    )[:3],
+                    "top_resolutions": sorted(
+                        capability_state.resolution_frequency.items(),
+                        key=lambda x: x[1],
+                        reverse=True,
+                    )[:3],
+                    "top_frame_rates": sorted(
+                        capability_state.frame_rate_frequency.items(),
+                        key=lambda x: x[1],
+                        reverse=True,
+                    )[:3],
+                },
+            },
         }
 
     def get_stream_name_from_device_path(self, device_path: str) -> str:
         """
         Extract deterministic stream name from camera device path.
-        
+
         Args:
             device_path: Camera device path (e.g., /dev/video0)
-            
+
         Returns:
             Stream name for MediaMTX (e.g., camera0)
         """
         try:
             # Primary pattern: /dev/video<number>
-            match = re.search(r'/dev/video(\d+)', device_path)
+            match = re.search(r"/dev/video(\d+)", device_path)
             if match:
                 device_num = match.group(1)
                 return f"camera{device_num}"
-            
+
             # Secondary pattern: any path with video and number
-            match = re.search(r'video(\d+)', device_path, re.IGNORECASE)
+            match = re.search(r"video(\d+)", device_path, re.IGNORECASE)
             if match:
                 device_num = match.group(1)
                 return f"camera{device_num}"
-            
+
             # Tertiary fallback: extract any digits from the path
-            digits = re.findall(r'\d+', device_path)
+            digits = re.findall(r"\d+", device_path)
             if digits:
                 return f"camera{digits[-1]}"
-            
+
             # Final fallback: hash-based deterministic name
             hash_val = abs(hash(device_path)) % 1000
             self._logger.debug(
                 f"Using hash-based stream name for {device_path}: camera_{hash_val}",
                 extra={
-                    'device_path': device_path,
-                    'hash_value': hash_val,
-                    'fallback_used': True
-                }
+                    "device_path": device_path,
+                    "hash_value": hash_val,
+                    "fallback_used": True,
+                },
             )
             return f"camera_{hash_val}"
-            
+
         except Exception as e:
-            self._logger.warning(f"Error extracting stream name from {device_path}: {e}")
+            self._logger.warning(
+                f"Error extracting stream name from {device_path}: {e}"
+            )
             return "camera_unknown"
 
-    async def _probe_device_formats_robust(self, device_path: str) -> Optional[Dict[str, Any]]:
+    async def _probe_device_formats_robust(
+        self, device_path: str
+    ) -> Optional[Dict[str, Any]]:
         """Probe device formats and resolutions with enhanced parsing and error handling."""
         try:
             process = await asyncio.wait_for(
                 asyncio.create_subprocess_exec(
-                    'v4l2-ctl', '--device', device_path, '--list-formats-ext',
+                    "v4l2-ctl",
+                    "--device",
+                    device_path,
+                    "--list-formats-ext",
                     stdout=asyncio.subprocess.PIPE,
-                    stderr=asyncio.subprocess.PIPE
+                    stderr=asyncio.subprocess.PIPE,
                 ),
-                timeout=self._detection_timeout
-            )
-            
+                timeout=self._detection_timeout,
+            )
+
             stdout, stderr = await process.communicate()
-            
+
             if process.returncode != 0:
                 return None
-            
+
             formats_output = stdout.decode()
             formats = []
             resolutions = set()
-            
+
             # Enhanced format parsing with multiple patterns
             current_format = None
-            
-            for line in formats_output.split('\n'):
+
+            for line in formats_output.split("\n"):
                 line = line.strip()
                 if not line:
                     continue
-                
+
                 # Format detection with enhanced patterns
                 format_patterns = [
-                    r'\[(\d+)\]:\s*\'(\w+)\'\s*\(([^)]+)\)',  # [0]: 'YUYV' (YUYV 4:2:2)
-                    r'Index\s*:\s*(\d+)\s*Type\s*:\s*Video\s*Capture\s*Pixel\s*Format\s*:\s*\'(\w+)\'',
-                    r'Pixel\s*Format\s*:\s*\'(\w+)\''
+                    r"\[(\d+)\]:\s*\'(\w+)\'\s*\(([^)]+)\)",  # [0]: 'YUYV' (YUYV 4:2:2)
+                    r"Index\s*:\s*(\d+)\s*Type\s*:\s*Video\s*Capture\s*Pixel\s*Format\s*:\s*\'(\w+)\'",
+                    r"Pixel\s*Format\s*:\s*\'(\w+)\'",
                 ]
-                
+
                 for pattern in format_patterns:
                     match = re.search(pattern, line, re.IGNORECASE)
                     if match:
                         if len(match.groups()) >= 2:
-                            format_code = match.group(2) if len(match.groups()) >= 2 else match.group(1)
-                            format_desc = match.group(3) if len(match.groups()) >= 3 else format_code
+                            format_code = (
+                                match.group(2)
+                                if len(match.groups()) >= 2
+                                else match.group(1)
+                            )
+                            format_desc = (
+                                match.group(3)
+                                if len(match.groups()) >= 3
+                                else format_code
+                            )
                         else:
                             format_code = match.group(1)
                             format_desc = format_code
-                        
+
                         current_format = {
                             "code": format_code,
                             "description": format_desc,
-                            "resolutions": []
+                            "resolutions": [],
                         }
                         formats.append(current_format)
                         break
-                
+
                 # Resolution detection with enhanced patterns
                 if current_format:
                     resolution_patterns = [
-                        r'Size:\s*Discrete\s+(\d+)x(\d+)',                # Size: Discrete 640x480
-                        r'(\d{3,4})\s*x\s*(\d{3,4})',                     # 1920x1080
-                        r'Width\s*(\d+)\s*Height\s*(\d+)',                # Width 1920 Height 1080
-                        r'Resolution:\s*(\d+)\s*x\s*(\d+)'                # Resolution: 1920 x 1080
+                        r"Size:\s*Discrete\s+(\d+)x(\d+)",  # Size: Discrete 640x480
+                        r"(\d{3,4})\s*x\s*(\d{3,4})",  # 1920x1080
+                        r"Width\s*(\d+)\s*Height\s*(\d+)",  # Width 1920 Height 1080
+                        r"Resolution:\s*(\d+)\s*x\s*(\d+)",  # Resolution: 1920 x 1080
                     ]
-                    
+
                     for pattern in resolution_patterns:
                         match = re.search(pattern, line, re.IGNORECASE)
                         if match:
                             width = int(match.group(1))
                             height = int(match.group(2))
-                            
+
                             # Validate reasonable resolution ranges
                             if 160 <= width <= 4096 and 120 <= height <= 3072:
                                 resolution = f"{width}x{height}"
                                 current_format["resolutions"].append(resolution)
                                 resolutions.add(resolution)
                             break
-            
+
             # Fallback resolution extraction if no format-specific parsing worked
             if not resolutions:
-                fallback_resolutions = re.findall(r'(\d{3,4})x(\d{3,4})', formats_output)
+                fallback_resolutions = re.findall(
+                    r"(\d{3,4})x(\d{3,4})", formats_output
+                )
                 for width_str, height_str in fallback_resolutions:
                     width, height = int(width_str), int(height_str)
                     if 160 <= width <= 4096 and 120 <= height <= 3072:
                         resolution = f"{width}x{height}"
                         resolutions.add(resolution)
-            
+
             return {
                 "formats": formats,
-                "resolutions": sorted(list(resolutions), 
-                                    key=lambda r: (int(r.split('x')[0]), int(r.split('x')[1])), 
-                                    reverse=True)  # Sort by resolution, highest first
+                "resolutions": sorted(
+                    list(resolutions),
+                    key=lambda r: (int(r.split("x")[0]), int(r.split("x")[1])),
+                    reverse=True,
+                ),  # Sort by resolution, highest first
             }
-            
+
         except asyncio.TimeoutError:
             self._logger.debug(f"Timeout probing device formats for {device_path}")
             return None
         except Exception as e:
             self._logger.debug(f"Error probing device formats for {device_path}: {e}")
@@ -1788,58 +1946,62 @@
 
     # Test interface methods (enhanced)
     def _get_capability_probe_interface(self):
         """Test hook: Enhanced capability probing interface for comprehensive testing."""
         return {
-            'probe_device_info': self._probe_device_info_robust,
-            'probe_device_formats': self._probe_device_formats_robust,
-            'probe_device_framerates': self._probe_device_framerates_robust,
-            'extract_frame_rates': self._extract_frame_rates_from_output,
-            'select_preferred_rates': self._select_preferred_frame_rates,
-            'update_capability_state': self._update_capability_validation_state,
-            'check_consistency': self._is_capability_data_consistent,
-            'get_effective_metadata': self.get_effective_capability_metadata,
-            'stats': self.get_monitor_stats
+            "probe_device_info": self._probe_device_info_robust,
+            "probe_device_formats": self._probe_device_formats_robust,
+            "probe_device_framerates": self._probe_device_framerates_robust,
+            "extract_frame_rates": self._extract_frame_rates_from_output,
+            "select_preferred_rates": self._select_preferred_frame_rates,
+            "update_capability_state": self._update_capability_validation_state,
+            "check_consistency": self._is_capability_data_consistent,
+            "get_effective_metadata": self.get_effective_capability_metadata,
+            "stats": self.get_monitor_stats,
         }
 
-    def _get_capability_state_for_testing(self, device_path: str) -> Optional[DeviceCapabilityState]:
+    def _get_capability_state_for_testing(
+        self, device_path: str
+    ) -> Optional[DeviceCapabilityState]:
         """Test hook: Get capability state for validation testing."""
         return self._capability_states.get(device_path)
 
-    def _set_capability_state_for_testing(self, device_path: str, state: DeviceCapabilityState) -> None:
+    def _set_capability_state_for_testing(
+        self, device_path: str, state: DeviceCapabilityState
+    ) -> None:
         """Test hook: Set capability state for validation testing."""
         self._capability_states[device_path] = state
 
     async def _inject_test_udev_event(self, device_path: str, action: str) -> None:
         """Test hook: Inject synthetic udev event for testing."""
-        if not hasattr(self, '_test_mode'):
+        if not hasattr(self, "_test_mode"):
             self._logger.warning("Test event injection called outside test mode")
             return
-            
+
         # Create mock udev device for testing
         class MockUdevDevice:
             def __init__(self, device_node: str, action: str):
                 self.device_node = device_node
                 self.action = action
-        
+
         mock_device = MockUdevDevice(device_path, action)
         await self._process_udev_device_event(mock_device)
 
     def _set_test_mode(self, enabled: bool = True) -> None:
         """Test hook: Enable/disable test mode for injection methods."""
         if enabled:
             self._test_mode = True
-        elif hasattr(self, '_test_mode'):
-            delattr(self, '_test_mode')
+        elif hasattr(self, "_test_mode"):
+            delattr(self, "_test_mode")
 
     def _get_adaptive_polling_state_for_testing(self) -> Dict[str, Any]:
         """Test hook: Get adaptive polling state for testing."""
         return {
-            'current_interval': self._current_poll_interval,
-            'base_interval': self._base_poll_interval,
-            'min_interval': self._min_poll_interval,
-            'max_interval': self._max_poll_interval,
-            'last_udev_time': self._last_udev_event_time,
-            'failure_count': self._polling_failure_count,
-            'adjustment_count': self._stats.get('adaptive_poll_adjustments', 0),
-            'freshness_threshold': self._udev_event_freshness_threshold
-        }
\ No newline at end of file
+            "current_interval": self._current_poll_interval,
+            "base_interval": self._base_poll_interval,
+            "min_interval": self._min_poll_interval,
+            "max_interval": self._max_poll_interval,
+            "last_udev_time": self._last_udev_event_time,
+            "failure_count": self._polling_failure_count,
+            "adjustment_count": self._stats.get("adaptive_poll_adjustments", 0),
+            "freshness_threshold": self._udev_event_freshness_threshold,
+        }
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_discovery/test_hybrid_monitor_capability_parsing.py	2025-08-04 02:48:56.066227+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_discovery/test_hybrid_monitor_capability_parsing.py	2025-08-04 15:35:24.720009+00:00
@@ -21,26 +21,26 @@
 from unittest.mock import Mock, AsyncMock, patch, MagicMock
 from typing import Dict, List, Any
 
 # Test imports
 from src.camera_discovery.hybrid_monitor import (
-    HybridCameraMonitor, 
+    HybridCameraMonitor,
     CapabilityDetectionResult,
-    DeviceCapabilityState
+    DeviceCapabilityState,
 )
 
 
 class TestCapabilityParsingVariations:
     """Test capability detection parsing with varied and malformed v4l2-ctl outputs."""
 
     @pytest.fixture
     def monitor(self):
         """Create monitor with capability detection enabled."""
         return HybridCameraMonitor(
-            device_range=[0, 1, 2], 
+            device_range=[0, 1, 2],
             enable_capability_detection=True,
-            detection_timeout=2.0
+            detection_timeout=2.0,
         )
 
     def test_frame_rate_extraction_comprehensive_patterns(self, monitor):
         """Test frame rate extraction from comprehensive v4l2-ctl output patterns."""
         test_cases = [
@@ -48,214 +48,220 @@
             ("30.000 fps", {"30"}),
             ("25.000 FPS", {"25"}),
             ("Frame rate: 60.0", {"60"}),
             ("1920x1080@30", {"30"}),
             ("15 Hz", {"15"}),
-            
             # Interval patterns - Evidence: hybrid_monitor.py lines 545-560
             ("Interval: [1/30]", {"30"}),
             ("[1/25]", {"25"}),
             ("1/15 s", {"15"}),
-            
             # Complex patterns - Evidence: hybrid_monitor.py lines 565-580
             ("30 frames per second", {"30"}),
             ("rate: 25.5", {"25.5"}),
             ("fps: 60", {"60"}),
-            
             # Multiple rates in one output - Real v4l2-ctl scenario
             ("30.000 fps, 25 FPS, [1/15], 60 Hz", {"30", "25", "15", "60"}),
-            
             # Edge cases and bounds testing
             ("", set()),
             ("no frame rates here", set()),
             ("300 fps", {"300"}),  # High rate but valid
             ("1.5 fps", {"1.5"}),  # Low rate but valid
-            ("0 fps", set()),      # Invalid rate (filtered out)
-            ("500 fps", set()),    # Invalid rate (filtered out - over max)
-            
+            ("0 fps", set()),  # Invalid rate (filtered out)
+            ("500 fps", set()),  # Invalid rate (filtered out - over max)
             # Malformed patterns - Error recovery testing
-            ("30.000.000 fps", set()),     # Double decimal
-            ("abc fps", set()),            # Non-numeric
+            ("30.000.000 fps", set()),  # Double decimal
+            ("abc fps", set()),  # Non-numeric
             ("fps without number", set()),  # No number
-            ("30.000 f ps", set()),        # Broken pattern
-            ("-30 fps", set()),           # Negative rate
+            ("30.000 f ps", set()),  # Broken pattern
+            ("-30 fps", set()),  # Negative rate
         ]
-        
+
         for output, expected in test_cases:
             result = monitor._extract_frame_rates_from_output(output)
-            assert result == expected, f"Failed for output: '{output}' - expected {expected}, got {result}"
+            assert (
+                result == expected
+            ), f"Failed for output: '{output}' - expected {expected}, got {result}"
 
     @pytest.mark.asyncio
     async def test_capability_parsing_malformed_v4l2_outputs(self, monitor):
         """Test capability detection resilience against malformed v4l2-ctl outputs."""
-        
+
         malformed_outputs = [
             # Truncated output
             '{"incomplete": "json"',
-            
             # Non-JSON output when JSON expected
             "Error: device busy\nRetry later",
-            
             # Empty output
             "",
-            
             # Binary garbage
-            b'\x00\x01\x02\x03invalid'.decode('utf-8', errors='ignore'),
-            
+            b"\x00\x01\x02\x03invalid".decode("utf-8", errors="ignore"),
             # Partial v4l2-ctl output
             "Driver Info:\n  Driver name   : uvcvideo\n  Card type     :",  # Cut off
-            
             # Mixed encoding issues
             "Camera: Café Français\nResolution: 1920×1080",  # Unicode chars
         ]
-        
-        device_path = "/dev/video0"
-        
+
+        device_path = "/dev/video0"
+
         for malformed_output in malformed_outputs:
             # Mock subprocess to return malformed output
             mock_process = Mock()
             mock_process.stdout = malformed_output
             mock_process.stderr = ""
             mock_process.returncode = 0
-            
-            with patch('subprocess.run', return_value=mock_process):
+
+            with patch("subprocess.run", return_value=mock_process):
                 result = await monitor._probe_device_capabilities(device_path)
-            
+
             # Should handle malformation gracefully
             assert isinstance(result, CapabilityDetectionResult)
-            assert result.detected == (len(result.formats) > 0 or len(result.resolutions) > 0)
+            assert result.detected == (
+                len(result.formats) > 0 or len(result.resolutions) > 0
+            )
             assert result.error is None or "parsing" in result.error.lower()
 
     @pytest.mark.asyncio
     async def test_capability_timeout_handling(self, monitor):
         """Test timeout handling during capability detection."""
-        
-        device_path = "/dev/video0"
-        
+
+        device_path = "/dev/video0"
+
         # Mock subprocess that times out
         async def mock_timeout_subprocess(*args, **kwargs):
             await asyncio.sleep(monitor._detection_timeout + 1.0)  # Exceed timeout
             raise asyncio.TimeoutError("Command timed out")
-        
-        with patch('asyncio.create_subprocess_exec', side_effect=mock_timeout_subprocess):
+
+        with patch(
+            "asyncio.create_subprocess_exec", side_effect=mock_timeout_subprocess
+        ):
             result = await monitor._probe_device_capabilities(device_path)
-        
+
         # Should handle timeout gracefully
         assert isinstance(result, CapabilityDetectionResult)
         assert result.detected == False
         assert result.accessible == False
         assert "timeout" in result.error.lower()
         assert result.timeout_context is not None
 
     @pytest.mark.asyncio
     async def test_subprocess_failure_error_handling(self, monitor):
         """Test subprocess failure handling during capability detection."""
-        
-        device_path = "/dev/video0"
-        
+
+        device_path = "/dev/video0"
+
         # Mock subprocess that fails with non-zero exit code
         mock_process = Mock()
         mock_process.stdout = ""
-        mock_process.stderr = "v4l2-ctl: failed to open /dev/video0: Device or resource busy"
+        mock_process.stderr = (
+            "v4l2-ctl: failed to open /dev/video0: Device or resource busy"
+        )
         mock_process.returncode = 1
-        
-        with patch('subprocess.run', return_value=mock_process):
+
+        with patch("subprocess.run", return_value=mock_process):
             result = await monitor._probe_device_capabilities(device_path)
-        
+
         # Should handle subprocess failure gracefully
         assert isinstance(result, CapabilityDetectionResult)
         assert result.detected == False
         assert result.accessible == False
         assert "busy" in result.error.lower()
 
     def test_resolution_extraction_patterns(self, monitor):
         """Test resolution extraction from various v4l2-ctl output formats."""
-        
+
         test_cases = [
             # Standard resolution patterns
             ("Size: Discrete 1920x1080", ["1920x1080"]),
             ("Size: Discrete 1280x720", ["1280x720"]),
             ("Size: Discrete 640x480", ["640x480"]),
-            
             # Multiple resolutions
-            ("Size: Discrete 1920x1080\nSize: Discrete 1280x720\nSize: Discrete 640x480", 
-             ["1920x1080", "1280x720", "640x480"]),
-            
+            (
+                "Size: Discrete 1920x1080\nSize: Discrete 1280x720\nSize: Discrete 640x480",
+                ["1920x1080", "1280x720", "640x480"],
+            ),
             # Alternative format patterns
             ("Resolution: 1920×1080", ["1920x1080"]),  # Unicode multiplication
-            ("  1280 x 720  ", ["1280x720"]),         # Spaces
-            ("1024*768", ["1024x768"]),               # Asterisk separator
-            
+            ("  1280 x 720  ", ["1280x720"]),  # Spaces
+            ("1024*768", ["1024x768"]),  # Asterisk separator
             # Edge cases
             ("", []),
             ("No resolutions found", []),
             ("Size: Continuous", []),  # Continuous mode (not discrete)
-            
             # Malformed resolutions
-            ("Size: Discrete 1920", []),              # Incomplete
-            ("Size: Discrete x1080", []),             # Missing width
-            ("Size: Discrete abcxdef", []),           # Non-numeric
+            ("Size: Discrete 1920", []),  # Incomplete
+            ("Size: Discrete x1080", []),  # Missing width
+            ("Size: Discrete abcxdef", []),  # Non-numeric
         ]
-        
+
         for output, expected in test_cases:
             result = monitor._extract_resolutions_from_output(output)
             # Convert to list for easier comparison (order may vary)
-            result_list = sorted(list(result)) if isinstance(result, set) else sorted(result)
+            result_list = (
+                sorted(list(result)) if isinstance(result, set) else sorted(result)
+            )
             expected_list = sorted(expected)
-            assert result_list == expected_list, f"Failed for output: '{output}' - expected {expected_list}, got {result_list}"
+            assert (
+                result_list == expected_list
+            ), f"Failed for output: '{output}' - expected {expected_list}, got {result_list}"
 
     def test_format_extraction_patterns(self, monitor):
         """Test pixel format extraction from v4l2-ctl output."""
-        
+
         test_cases = [
             # Standard format patterns
             ("Pixel Format: 'YUYV'", [{"format": "YUYV", "description": "YUYV 4:2:2"}]),
-            ("Pixel Format: 'MJPG'", [{"format": "MJPG", "description": "Motion-JPEG"}]),
-            
+            (
+                "Pixel Format: 'MJPG'",
+                [{"format": "MJPG", "description": "Motion-JPEG"}],
+            ),
             # Multiple formats
-            ("Pixel Format: 'YUYV'\nPixel Format: 'MJPG'", 
-             [{"format": "YUYV", "description": "YUYV 4:2:2"}, 
-              {"format": "MJPG", "description": "Motion-JPEG"}]),
-            
+            (
+                "Pixel Format: 'YUYV'\nPixel Format: 'MJPG'",
+                [
+                    {"format": "YUYV", "description": "YUYV 4:2:2"},
+                    {"format": "MJPG", "description": "Motion-JPEG"},
+                ],
+            ),
             # Format with description
-            ("Pixel Format: 'YUYV' (YUYV 4:2:2)", [{"format": "YUYV", "description": "YUYV 4:2:2"}]),
-            
-            # Edge cases  
+            (
+                "Pixel Format: 'YUYV' (YUYV 4:2:2)",
+                [{"format": "YUYV", "description": "YUYV 4:2:2"}],
+            ),
+            # Edge cases
             ("", []),
             ("No pixel formats", []),
-            
             # Malformed formats
-            ("Pixel Format: ''", []),           # Empty format
-            ("Pixel Format: YUYV", []),         # Missing quotes
-            ("Format: 'UNKNOWN'", []),          # Different pattern
+            ("Pixel Format: ''", []),  # Empty format
+            ("Pixel Format: YUYV", []),  # Missing quotes
+            ("Format: 'UNKNOWN'", []),  # Different pattern
         ]
-        
+
         for output, expected in test_cases:
             result = monitor._extract_formats_from_output(output)
-            assert result == expected, f"Failed for output: '{output}' - expected {expected}, got {result}"
+            assert (
+                result == expected
+            ), f"Failed for output: '{output}' - expected {expected}, got {result}"
 
 
 class TestCapabilityConfirmationLogic:
     """Test provisional → confirmed capability confirmation logic."""
 
     @pytest.fixture
     def monitor_with_confirmation(self):
         """Create monitor with capability confirmation enabled."""
         return HybridCameraMonitor(
-            device_range=[0, 1],
-            enable_capability_detection=True,
-            detection_timeout=1.0
+            device_range=[0, 1], enable_capability_detection=True, detection_timeout=1.0
         )
 
     def test_capability_state_initialization(self, monitor_with_confirmation):
         """Test initial capability state setup for new devices."""
-        
-        device_path = "/dev/video0"
-        
+
+        device_path = "/dev/video0"
+
         # Get or create capability state
         state = monitor_with_confirmation._get_or_create_capability_state(device_path)
-        
+
         assert isinstance(state, DeviceCapabilityState)
         assert state.device_path == device_path
         assert state.provisional_data is None
         assert state.confirmed_data is None
         assert state.consecutive_successes == 0
@@ -263,162 +269,179 @@
         assert state.confirmation_threshold >= 2  # Architecture requirement
         assert not state.is_confirmed()
 
     def test_provisional_capability_recording(self, monitor_with_confirmation):
         """Test recording of provisional capability data."""
-        
-        device_path = "/dev/video0"
-        
+
+        device_path = "/dev/video0"
+
         # Create mock capability result
         capability_result = CapabilityDetectionResult(
             device_path=device_path,
             detected=True,
             accessible=True,
             device_name="Test Camera",
             formats=[{"format": "YUYV", "description": "YUYV 4:2:2"}],
             resolutions=["1920x1080", "1280x720"],
-            frame_rates=["30", "25"]
-        )
-        
+            frame_rates=["30", "25"],
+        )
+
         # Record provisional data
-        monitor_with_confirmation._update_capability_state(device_path, capability_result)
-        
+        monitor_with_confirmation._update_capability_state(
+            device_path, capability_result
+        )
+
         state = monitor_with_confirmation._capability_states[device_path]
         assert state.provisional_data is not None
         assert state.provisional_data == capability_result
         assert state.confirmed_data is None  # Not confirmed yet
         assert state.consecutive_successes == 1
 
     def test_capability_confirmation_threshold(self, monitor_with_confirmation):
         """Test capability confirmation after reaching threshold."""
-        
-        device_path = "/dev/video0"
-        
+
+        device_path = "/dev/video0"
+
         # Create consistent capability result
         capability_result = CapabilityDetectionResult(
             device_path=device_path,
             detected=True,
             accessible=True,
             device_name="Test Camera",
             formats=[{"format": "YUYV", "description": "YUYV 4:2:2"}],
             resolutions=["1920x1080"],
-            frame_rates=["30"]
-        )
-        
+            frame_rates=["30"],
+        )
+
         # Record multiple consistent results to reach confirmation threshold
         confirmation_threshold = 3
         for i in range(confirmation_threshold):
-            monitor_with_confirmation._update_capability_state(device_path, capability_result)
-        
+            monitor_with_confirmation._update_capability_state(
+                device_path, capability_result
+            )
+
         state = monitor_with_confirmation._capability_states[device_path]
         assert state.consecutive_successes == confirmation_threshold
         assert state.confirmed_data is not None
         assert state.is_confirmed()
-        
+
         # Effective capability should return confirmed data
         effective = state.get_effective_capability()
         assert effective == state.confirmed_data
 
     def test_capability_frequency_weighted_merging(self, monitor_with_confirmation):
         """Test frequency-weighted capability merging logic."""
-        
+
         device_path = "/dev/video0"
         state = monitor_with_confirmation._get_or_create_capability_state(device_path)
-        
+
         # Simulate multiple detections with different frame rates
-        frame_rate_detections = ["30", "30", "25", "30", "60", "30"]  # 30 appears 4 times
-        
+        frame_rate_detections = [
+            "30",
+            "30",
+            "25",
+            "30",
+            "60",
+            "30",
+        ]  # 30 appears 4 times
+
         for frame_rate in frame_rate_detections:
             capability_result = CapabilityDetectionResult(
                 device_path=device_path,
                 detected=True,
                 accessible=True,
-                frame_rates=[frame_rate]
+                frame_rates=[frame_rate],
             )
-            
+
             # Update frequency tracking
-            monitor_with_confirmation._update_frequency_tracking(state, capability_result)
-        
+            monitor_with_confirmation._update_frequency_tracking(
+                state, capability_result
+            )
+
         # Verify frequency tracking
         assert state.frame_rate_frequency["30"] == 4  # Most frequent
         assert state.frame_rate_frequency["25"] == 1
         assert state.frame_rate_frequency["60"] == 1
-        
+
         # Most frequent frame rate should be prioritized
-        most_frequent = max(state.frame_rate_frequency.keys(), 
-                          key=lambda x: state.frame_rate_frequency[x])
+        most_frequent = max(
+            state.frame_rate_frequency.keys(),
+            key=lambda x: state.frame_rate_frequency[x],
+        )
         assert most_frequent == "30"
 
     def test_capability_inconsistency_handling(self, monitor_with_confirmation):
         """Test handling of inconsistent capability detections."""
-        
-        device_path = "/dev/video0"
-        
+
+        device_path = "/dev/video0"
+
         # First detection
         result1 = CapabilityDetectionResult(
             device_path=device_path,
             detected=True,
             accessible=True,
             resolutions=["1920x1080"],
-            frame_rates=["30"]
-        )
-        
+            frame_rates=["30"],
+        )
+
         # Inconsistent second detection
         result2 = CapabilityDetectionResult(
             device_path=device_path,
             detected=True,
             accessible=True,
             resolutions=["1280x720"],  # Different resolution
-            frame_rates=["25"]         # Different frame rate
-        )
-        
+            frame_rates=["25"],  # Different frame rate
+        )
+
         # Record both results
         monitor_with_confirmation._update_capability_state(device_path, result1)
         monitor_with_confirmation._update_capability_state(device_path, result2)
-        
+
         state = monitor_with_confirmation._capability_states[device_path]
-        
+
         # Should track both possibilities in frequency tables
         assert "1920x1080" in state.resolution_frequency
         assert "1280x720" in state.resolution_frequency
         assert "30" in state.frame_rate_frequency
         assert "25" in state.frame_rate_frequency
-        
+
         # Confirmation should require consistency
         assert state.consecutive_successes < state.confirmation_threshold
 
     @pytest.mark.asyncio
-    async def test_capability_detection_failure_recovery(self, monitor_with_confirmation):
+    async def test_capability_detection_failure_recovery(
+        self, monitor_with_confirmation
+    ):
         """Test recovery from capability detection failures."""
-        
-        device_path = "/dev/video0"
-        
+
+        device_path = "/dev/video0"
+
         # Simulate detection failure
         failure_result = CapabilityDetectionResult(
             device_path=device_path,
             detected=False,
             accessible=False,
-            error="Device busy"
-        )
-        
+            error="Device busy",
+        )
+
         monitor_with_confirmation._update_capability_state(device_path, failure_result)
-        
+
         state = monitor_with_confirmation._capability_states[device_path]
         assert state.consecutive_failures == 1
         assert state.consecutive_successes == 0
-        
+
         # Simulate successful detection after failure
         success_result = CapabilityDetectionResult(
             device_path=device_path,
             detected=True,
             accessible=True,
             resolutions=["1920x1080"],
-            frame_rates=["30"]
-        )
-        
+            frame_rates=["30"],
+        )
+
         monitor_with_confirmation._update_capability_state(device_path, success_result)
-        
+
         # Should reset failure count and start success count
         assert state.consecutive_failures == 0
         assert state.consecutive_successes == 1
         assert state.provisional_data is not None
 
@@ -428,83 +451,83 @@
 
     @pytest.fixture
     def monitor_integrated(self):
         """Create monitor for integration testing."""
         return HybridCameraMonitor(
-            device_range=[0, 1],
-            enable_capability_detection=True,
-            detection_timeout=1.0
-        )
-
-    @pytest.mark.asyncio
-    async def test_get_effective_capability_metadata_integration(self, monitor_integrated):
+            device_range=[0, 1], enable_capability_detection=True, detection_timeout=1.0
+        )
+
+    @pytest.mark.asyncio
+    async def test_get_effective_capability_metadata_integration(
+        self, monitor_integrated
+    ):
         """Test the get_effective_capability_metadata method integration."""
-        
-        device_path = "/dev/video0"
-        
+
+        device_path = "/dev/video0"
+
         # Setup confirmed capability data
         confirmed_capability = CapabilityDetectionResult(
             device_path=device_path,
             detected=True,
             accessible=True,
             device_name="Confirmed Camera",
             resolutions=["1920x1080", "1280x720"],
-            frame_rates=["30", "25"]
-        )
-        
+            frame_rates=["30", "25"],
+        )
+
         state = monitor_integrated._get_or_create_capability_state(device_path)
         state.confirmed_data = confirmed_capability
         state.consecutive_successes = 5
-        
+
         # Get effective metadata
         metadata = monitor_integrated.get_effective_capability_metadata(device_path)
-        
+
         # Should return metadata based on confirmed capability
         assert metadata is not None
         assert metadata["validation_status"] == "confirmed"
         assert metadata["consecutive_successes"] == 5
         assert metadata["resolution"] in ["1920x1080", "1280x720"]  # Highest priority
         assert metadata["fps"] in [30, 25]  # Highest priority frame rate
         assert "formats" in metadata
         assert "all_resolutions" in metadata
 
-    @pytest.mark.asyncio  
+    @pytest.mark.asyncio
     async def test_provisional_metadata_fallback(self, monitor_integrated):
         """Test metadata fallback to provisional when confirmed unavailable."""
-        
-        device_path = "/dev/video0"
-        
+
+        device_path = "/dev/video0"
+
         # Setup only provisional capability data
         provisional_capability = CapabilityDetectionResult(
             device_path=device_path,
             detected=True,
             accessible=True,
             device_name="Provisional Camera",
             resolutions=["1280x720"],
-            frame_rates=["30"]
-        )
-        
+            frame_rates=["30"],
+        )
+
         state = monitor_integrated._get_or_create_capability_state(device_path)
         state.provisional_data = provisional_capability
         state.consecutive_successes = 1  # Below confirmation threshold
-        
+
         # Get effective metadata
         metadata = monitor_integrated.get_effective_capability_metadata(device_path)
-        
+
         # Should return metadata based on provisional capability
         assert metadata is not None
         assert metadata["validation_status"] == "provisional"
         assert metadata["consecutive_successes"] == 1
         assert metadata["resolution"] == "1280x720"
         assert metadata["fps"] == 30
 
     @pytest.mark.asyncio
     async def test_no_capability_data_fallback(self, monitor_integrated):
         """Test metadata fallback when no capability data available."""
-        
+
         device_path = "/dev/video999"  # Non-existent device
-        
+
         # Get effective metadata for device with no capability data
         metadata = monitor_integrated.get_effective_capability_metadata(device_path)
-        
+
         # Should return None when no capability data available
-        assert metadata is None
\ No newline at end of file
+        assert metadata is None
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_discovery/test_hybrid_monitor_reconciliation.py	2025-08-04 02:49:54.619880+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_discovery/test_hybrid_monitor_reconciliation.py	2025-08-04 15:35:24.740154+00:00
@@ -22,11 +22,11 @@
 from src.camera_discovery.hybrid_monitor import (
     HybridCameraMonitor,
     CapabilityDetectionResult,
     DeviceCapabilityState,
     CameraEventData,
-    CameraEvent
+    CameraEvent,
 )
 from src.camera_service.service_manager import ServiceManager
 from src.common.types import CameraDevice
 
 
@@ -35,430 +35,527 @@
 
     @pytest.fixture
     def mock_dependencies(self):
         """Create mock dependencies for service manager."""
         return {
-            'config': Mock(),
-            'mediamtx_controller': Mock(),
-            'websocket_server': Mock(),
-            'camera_monitor': None  # Will be set per test
+            "config": Mock(),
+            "mediamtx_controller": Mock(),
+            "websocket_server": Mock(),
+            "camera_monitor": None,  # Will be set per test
         }
 
     @pytest.fixture
     def hybrid_monitor(self):
         """Create hybrid monitor for reconciliation testing."""
         return HybridCameraMonitor(
-            device_range=[0, 1, 2],
-            enable_capability_detection=True
-        )
-
-    @pytest.fixture  
+            device_range=[0, 1, 2], enable_capability_detection=True
+        )
+
+    @pytest.fixture
     def service_manager(self, mock_dependencies):
         """Create service manager with mocked dependencies."""
         service_manager = ServiceManager(
-            config=mock_dependencies['config'],
-            mediamtx_controller=mock_dependencies['mediamtx_controller'],
-            websocket_server=mock_dependencies['websocket_server']
+            config=mock_dependencies["config"],
+            mediamtx_controller=mock_dependencies["mediamtx_controller"],
+            websocket_server=mock_dependencies["websocket_server"],
         )
         # Will inject camera_monitor per test
         return service_manager
 
     @pytest.mark.asyncio
-    async def test_confirmed_capability_reconciliation(self, hybrid_monitor, service_manager, mock_dependencies):
+    async def test_confirmed_capability_reconciliation(
+        self, hybrid_monitor, service_manager, mock_dependencies
+    ):
         """Test confirmed capability data flows correctly to service manager."""
-        
+
         device_path = "/dev/video0"
-        
+
         # Setup confirmed capability in hybrid_monitor
         confirmed_capability = CapabilityDetectionResult(
             device_path=device_path,
             detected=True,
             accessible=True,
             device_name="Confirmed Test Camera",
             formats=[{"format": "YUYV", "description": "YUYV 4:2:2"}],
             resolutions=["1920x1080", "1280x720", "640x480"],
-            frame_rates=["30", "25", "15"]
-        )
-        
+            frame_rates=["30", "25", "15"],
+        )
+
         # Create confirmed state in hybrid_monitor
         state = hybrid_monitor._get_or_create_capability_state(device_path)
         state.confirmed_data = confirmed_capability
         state.consecutive_successes = 5  # Above confirmation threshold
-        
+
         # Inject hybrid_monitor into service_manager
         service_manager._camera_monitor = hybrid_monitor
-        
+
         # Create camera event data
         camera_device = CameraDevice(
-            device_path=device_path,
-            name="Test Camera Device",
-            driver="uvcvideo"
-        )
-        event_data = CameraEventData(
-            device_path=device_path,
-            event_type=CameraEvent.CONNECTED,
-            device_info=camera_device
-        )
-        
+            device_path=device_path, name="Test Camera Device", driver="uvcvideo"
+        )
+        event_data = CameraEventData(
+            device_path=device_path,
+            event_type=CameraEvent.CONNECTED,
+            device_info=camera_device,
+        )
+
         # Get metadata from service_manager (this calls hybrid_monitor internally)
         metadata = service_manager._get_enhanced_camera_metadata(event_data)
-        
+
         # Verify reconciliation - confirmed data should propagate
         assert metadata["validation_status"] == "confirmed"
         assert metadata["capability_source"] == "confirmed_capability"
         assert metadata["consecutive_successes"] == 5
-        
+
         # Verify capability data consistency
         hybrid_metadata = hybrid_monitor.get_effective_capability_metadata(device_path)
         assert hybrid_metadata["validation_status"] == metadata["validation_status"]
-        assert hybrid_metadata["consecutive_successes"] == metadata["consecutive_successes"]
-        
+        assert (
+            hybrid_metadata["consecutive_successes"]
+            == metadata["consecutive_successes"]
+        )
+
         # Verify resolution selection consistency (should pick highest available)
         assert metadata["resolution"] == hybrid_metadata["resolution"]
         assert metadata["fps"] == hybrid_metadata["fps"]
-        
+
         print(f"✅ Confirmed capability reconciliation verified:")
         print(f"   - Validation status: {metadata['validation_status']}")
         print(f"   - Resolution: {metadata['resolution']}")
         print(f"   - FPS: {metadata['fps']}")
         print(f"   - Consecutive successes: {metadata['consecutive_successes']}")
 
     @pytest.mark.asyncio
-    async def test_provisional_capability_reconciliation(self, hybrid_monitor, service_manager, mock_dependencies):
+    async def test_provisional_capability_reconciliation(
+        self, hybrid_monitor, service_manager, mock_dependencies
+    ):
         """Test provisional capability data flows correctly to service manager."""
-        
+
         device_path = "/dev/video1"
-        
+
         # Setup provisional capability in hybrid_monitor
         provisional_capability = CapabilityDetectionResult(
             device_path=device_path,
             detected=True,
             accessible=True,
             device_name="Provisional Test Camera",
             formats=[{"format": "MJPG", "description": "Motion-JPEG"}],
             resolutions=["1280x720", "640x480"],
-            frame_rates=["30", "15"]
-        )
-        
-        # Create provisional state in hybrid_monitor  
+            frame_rates=["30", "15"],
+        )
+
+        # Create provisional state in hybrid_monitor
         state = hybrid_monitor._get_or_create_capability_state(device_path)
         state.provisional_data = provisional_capability
         state.consecutive_successes = 1  # Below confirmation threshold
-        
+
         # Inject hybrid_monitor into service_manager
         service_manager._camera_monitor = hybrid_monitor
-        
+
         # Create camera event data
         camera_device = CameraDevice(
-            device_path=device_path,
-            name="Provisional Camera Device", 
-            driver="uvcvideo"
-        )
-        event_data = CameraEventData(
-            device_path=device_path,
-            event_type=CameraEvent.CONNECTED,
-            device_info=camera_device
-        )
-        
+            device_path=device_path, name="Provisional Camera Device", driver="uvcvideo"
+        )
+        event_data = CameraEventData(
+            device_path=device_path,
+            event_type=CameraEvent.CONNECTED,
+            device_info=camera_device,
+        )
+
         # Get metadata from service_manager
         metadata = service_manager._get_enhanced_camera_metadata(event_data)
-        
+
         # Verify reconciliation - provisional data should propagate
         assert metadata["validation_status"] == "provisional"
         assert metadata["capability_source"] == "provisional_capability"
         assert metadata["consecutive_successes"] == 1
-        
-        # Verify capability data consistency  
+
+        # Verify capability data consistency
         hybrid_metadata = hybrid_monitor.get_effective_capability_metadata(device_path)
         assert hybrid_metadata["validation_status"] == metadata["validation_status"]
-        assert hybrid_metadata["consecutive_successes"] == metadata["consecutive_successes"]
-        
+        assert (
+            hybrid_metadata["consecutive_successes"]
+            == metadata["consecutive_successes"]
+        )
+
         # Verify data consistency
         assert metadata["resolution"] == hybrid_metadata["resolution"]
         assert metadata["fps"] == hybrid_metadata["fps"]
-        
+
         print(f"✅ Provisional capability reconciliation verified:")
         print(f"   - Validation status: {metadata['validation_status']}")
         print(f"   - Resolution: {metadata['resolution']}")
         print(f"   - FPS: {metadata['fps']}")
         print(f"   - Consecutive successes: {metadata['consecutive_successes']}")
 
     @pytest.mark.asyncio
-    async def test_no_capability_data_reconciliation(self, hybrid_monitor, service_manager, mock_dependencies):
+    async def test_no_capability_data_reconciliation(
+        self, hybrid_monitor, service_manager, mock_dependencies
+    ):
         """Test fallback behavior when no capability data is available."""
-        
+
         device_path = "/dev/video2"
-        
+
         # Don't setup any capability data - simulate device with no capability info
-        
+
         # Inject hybrid_monitor into service_manager
         service_manager._camera_monitor = hybrid_monitor
-        
+
         # Create camera event data
         camera_device = CameraDevice(
-            device_path=device_path,
-            name="Unknown Capability Camera",
-            driver="unknown"
-        )
-        event_data = CameraEventData(
-            device_path=device_path,
-            event_type=CameraEvent.CONNECTED,
-            device_info=camera_device
-        )
-        
+            device_path=device_path, name="Unknown Capability Camera", driver="unknown"
+        )
+        event_data = CameraEventData(
+            device_path=device_path,
+            event_type=CameraEvent.CONNECTED,
+            device_info=camera_device,
+        )
+
         # Get metadata from service_manager
         metadata = service_manager._get_enhanced_camera_metadata(event_data)
-        
+
         # Verify fallback behavior
         assert metadata["validation_status"] == "none"
-        assert metadata["capability_source"] == "device_info"  # Falls back to device info
+        assert (
+            metadata["capability_source"] == "device_info"
+        )  # Falls back to device info
         assert metadata["consecutive_successes"] == 0
-        
+
         # Should use architecture defaults
         assert metadata["resolution"] == "1920x1080"  # Architecture default
-        assert metadata["fps"] == 30                   # Architecture default
-        
+        assert metadata["fps"] == 30  # Architecture default
+
         # Verify hybrid_monitor also returns None for no capability data
         hybrid_metadata = hybrid_monitor.get_effective_capability_metadata(device_path)
         assert hybrid_metadata is None
-        
+
         print(f"✅ No capability data reconciliation verified:")
         print(f"   - Validation status: {metadata['validation_status']}")
-        print(f"   - Capability source: {metadata['capability_source']}")  
+        print(f"   - Capability source: {metadata['capability_source']}")
         print(f"   - Fallback resolution: {metadata['resolution']}")
         print(f"   - Fallback FPS: {metadata['fps']}")
 
     @pytest.mark.asyncio
-    async def test_capability_state_transition_reconciliation(self, hybrid_monitor, service_manager, mock_dependencies):
+    async def test_capability_state_transition_reconciliation(
+        self, hybrid_monitor, service_manager, mock_dependencies
+    ):
         """Test reconciliation during capability state transitions (provisional → confirmed)."""
-        
+
         device_path = "/dev/video0"
-        
+
         # Setup initial provisional capability
         provisional_capability = CapabilityDetectionResult(
             device_path=device_path,
             detected=True,
             accessible=True,
             device_name="Transitioning Camera",
             resolutions=["1920x1080"],
-            frame_rates=["30"]
-        )
-        
+            frame_rates=["30"],
+        )
+
         state = hybrid_monitor._get_or_create_capability_state(device_path)
         state.provisional_data = provisional_capability
         state.consecutive_successes = 1
-        
+
         # Inject hybrid_monitor into service_manager
         service_manager._camera_monitor = hybrid_monitor
-        
-        camera_device = CameraDevice(device_path=device_path, name="Test Camera", driver="uvcvideo")
-        event_data = CameraEventData(device_path=device_path, event_type=CameraEvent.CONNECTED, device_info=camera_device)
-        
+
+        camera_device = CameraDevice(
+            device_path=device_path, name="Test Camera", driver="uvcvideo"
+        )
+        event_data = CameraEventData(
+            device_path=device_path,
+            event_type=CameraEvent.CONNECTED,
+            device_info=camera_device,
+        )
+
         # Get initial metadata (should be provisional)
         initial_metadata = service_manager._get_enhanced_camera_metadata(event_data)
         assert initial_metadata["validation_status"] == "provisional"
-        
+
         # Simulate additional consistent detections to trigger confirmation
         for _ in range(3):  # Reach confirmation threshold
             hybrid_monitor._update_capability_state(device_path, provisional_capability)
-        
+
         # Get metadata after confirmation
         confirmed_metadata = service_manager._get_enhanced_camera_metadata(event_data)
-        
+
         # Verify transition to confirmed state
         assert confirmed_metadata["validation_status"] == "confirmed"
         assert confirmed_metadata["capability_source"] == "confirmed_capability"
         assert confirmed_metadata["consecutive_successes"] >= 3
-        
+
         # Verify data consistency maintained during transition
         assert confirmed_metadata["resolution"] == initial_metadata["resolution"]
         assert confirmed_metadata["fps"] == initial_metadata["fps"]
-        
+
         print(f"✅ State transition reconciliation verified:")
-        print(f"   - Initial: {initial_metadata['validation_status']} → Final: {confirmed_metadata['validation_status']}")
-        print(f"   - Data consistency maintained: {confirmed_metadata['resolution']}@{confirmed_metadata['fps']}fps")
-
-    @pytest.mark.asyncio
-    async def test_metadata_drift_detection(self, hybrid_monitor, service_manager, mock_dependencies):
+        print(
+            f"   - Initial: {initial_metadata['validation_status']} → Final: {confirmed_metadata['validation_status']}"
+        )
+        print(
+            f"   - Data consistency maintained: {confirmed_metadata['resolution']}@{confirmed_metadata['fps']}fps"
+        )
+
+    @pytest.mark.asyncio
+    async def test_metadata_drift_detection(
+        self, hybrid_monitor, service_manager, mock_dependencies
+    ):
         """Test detection of metadata drift or inconsistencies between components."""
-        
+
         device_path = "/dev/video0"
-        
+
         # Setup capability in hybrid_monitor
         capability = CapabilityDetectionResult(
             device_path=device_path,
             detected=True,
             accessible=True,
             resolutions=["1920x1080", "1280x720"],
-            frame_rates=["30", "25"]
-        )
-        
+            frame_rates=["30", "25"],
+        )
+
         state = hybrid_monitor._get_or_create_capability_state(device_path)
         state.confirmed_data = capability
         state.consecutive_successes = 5
-        
+
         # Inject hybrid_monitor into service_manager
         service_manager._camera_monitor = hybrid_monitor
-        
-        camera_device = CameraDevice(device_path=device_path, name="Test Camera", driver="uvcvideo")
-        event_data = CameraEventData(device_path=device_path, event_type=CameraEvent.CONNECTED, device_info=camera_device)
-        
+
+        camera_device = CameraDevice(
+            device_path=device_path, name="Test Camera", driver="uvcvideo"
+        )
+        event_data = CameraEventData(
+            device_path=device_path,
+            event_type=CameraEvent.CONNECTED,
+            device_info=camera_device,
+        )
+
         # Get metadata from both sources
         hybrid_metadata = hybrid_monitor.get_effective_capability_metadata(device_path)
         service_metadata = service_manager._get_enhanced_camera_metadata(event_data)
-        
+
         # Comprehensive consistency check
         inconsistencies = []
-        
+
         # Check validation status consistency
-        if hybrid_metadata["validation_status"] != service_metadata["validation_status"]:
-            inconsistencies.append(f"Validation status mismatch: hybrid={hybrid_metadata['validation_status']}, service={service_metadata['validation_status']}")
-        
-        # Check consecutive successes consistency  
-        if hybrid_metadata["consecutive_successes"] != service_metadata["consecutive_successes"]:
-            inconsistencies.append(f"Consecutive successes mismatch: hybrid={hybrid_metadata['consecutive_successes']}, service={service_metadata['consecutive_successes']}")
-        
+        if (
+            hybrid_metadata["validation_status"]
+            != service_metadata["validation_status"]
+        ):
+            inconsistencies.append(
+                f"Validation status mismatch: hybrid={hybrid_metadata['validation_status']}, service={service_metadata['validation_status']}"
+            )
+
+        # Check consecutive successes consistency
+        if (
+            hybrid_metadata["consecutive_successes"]
+            != service_metadata["consecutive_successes"]
+        ):
+            inconsistencies.append(
+                f"Consecutive successes mismatch: hybrid={hybrid_metadata['consecutive_successes']}, service={service_metadata['consecutive_successes']}"
+            )
+
         # Check resolution consistency
         if hybrid_metadata["resolution"] != service_metadata["resolution"]:
-            inconsistencies.append(f"Resolution mismatch: hybrid={hybrid_metadata['resolution']}, service={service_metadata['resolution']}")
-        
+            inconsistencies.append(
+                f"Resolution mismatch: hybrid={hybrid_metadata['resolution']}, service={service_metadata['resolution']}"
+            )
+
         # Check FPS consistency
         if hybrid_metadata["fps"] != service_metadata["fps"]:
-            inconsistencies.append(f"FPS mismatch: hybrid={hybrid_metadata['fps']}, service={service_metadata['fps']}")
-        
+            inconsistencies.append(
+                f"FPS mismatch: hybrid={hybrid_metadata['fps']}, service={service_metadata['fps']}"
+            )
+
         # Check format availability consistency
-        hybrid_formats = set(fmt["format"] for fmt in hybrid_metadata.get("formats", []))
-        service_formats = set(fmt["format"] for fmt in service_metadata.get("formats", []))
+        hybrid_formats = set(
+            fmt["format"] for fmt in hybrid_metadata.get("formats", [])
+        )
+        service_formats = set(
+            fmt["format"] for fmt in service_metadata.get("formats", [])
+        )
         if hybrid_formats != service_formats:
-            inconsistencies.append(f"Format mismatch: hybrid={hybrid_formats}, service={service_formats}")
-        
+            inconsistencies.append(
+                f"Format mismatch: hybrid={hybrid_formats}, service={service_formats}"
+            )
+
         # Report any inconsistencies
         if inconsistencies:
             print("❌ Metadata inconsistencies detected:")
             for inconsistency in inconsistencies:
                 print(f"   - {inconsistency}")
-            pytest.fail(f"Metadata drift detected: {len(inconsistencies)} inconsistencies found")
+            pytest.fail(
+                f"Metadata drift detected: {len(inconsistencies)} inconsistencies found"
+            )
         else:
             print("✅ No metadata drift detected - components are consistent")
             print(f"   - Validation status: {hybrid_metadata['validation_status']}")
             print(f"   - Resolution: {hybrid_metadata['resolution']}")
             print(f"   - FPS: {hybrid_metadata['fps']}")
-            print(f"   - Consecutive successes: {hybrid_metadata['consecutive_successes']}")
-
-    @pytest.mark.asyncio
-    async def test_frequency_weighted_reconciliation(self, hybrid_monitor, service_manager, mock_dependencies):
+            print(
+                f"   - Consecutive successes: {hybrid_metadata['consecutive_successes']}"
+            )
+
+    @pytest.mark.asyncio
+    async def test_frequency_weighted_reconciliation(
+        self, hybrid_monitor, service_manager, mock_dependencies
+    ):
         """Test frequency-weighted capability selection reconciliation."""
-        
+
         device_path = "/dev/video0"
-        
+
         # Simulate multiple detections with different frame rates to build frequency data
-        frame_rate_detections = ["30", "30", "25", "30", "60", "30"]  # 30 appears most frequently
-        resolution_detections = ["1920x1080", "1920x1080", "1280x720", "1920x1080"]  # 1920x1080 most frequent
-        
+        frame_rate_detections = [
+            "30",
+            "30",
+            "25",
+            "30",
+            "60",
+            "30",
+        ]  # 30 appears most frequently
+        resolution_detections = [
+            "1920x1080",
+            "1920x1080",
+            "1280x720",
+            "1920x1080",
+        ]  # 1920x1080 most frequent
+
         state = hybrid_monitor._get_or_create_capability_state(device_path)
-        
-        for i, (fps, res) in enumerate(zip(frame_rate_detections, resolution_detections)):
+
+        for i, (fps, res) in enumerate(
+            zip(frame_rate_detections, resolution_detections)
+        ):
             capability = CapabilityDetectionResult(
                 device_path=device_path,
                 detected=True,
                 accessible=True,
                 resolutions=[res],
-                frame_rates=[fps]
-            )
-            
+                frame_rates=[fps],
+            )
+
             # Update frequency tracking
             hybrid_monitor._update_frequency_tracking(state, capability)
             hybrid_monitor._update_capability_state(device_path, capability)
-        
+
         # Inject hybrid_monitor into service_manager
         service_manager._camera_monitor = hybrid_monitor
-        
-        camera_device = CameraDevice(device_path=device_path, name="Frequency Test Camera", driver="uvcvideo")
-        event_data = CameraEventData(device_path=device_path, event_type=CameraEvent.CONNECTED, device_info=camera_device)
-        
+
+        camera_device = CameraDevice(
+            device_path=device_path, name="Frequency Test Camera", driver="uvcvideo"
+        )
+        event_data = CameraEventData(
+            device_path=device_path,
+            event_type=CameraEvent.CONNECTED,
+            device_info=camera_device,
+        )
+
         # Get metadata from service_manager
         metadata = service_manager._get_enhanced_camera_metadata(event_data)
-        
+
         # Verify frequency-weighted selections are consistent
         hybrid_metadata = hybrid_monitor.get_effective_capability_metadata(device_path)
-        
+
         # Should select most frequent values
         assert metadata["fps"] == 30  # Most frequent frame rate
         assert "1920x1080" in metadata["resolution"]  # Most frequent resolution
-        
+
         # Verify consistency between components
         assert metadata["fps"] == hybrid_metadata["fps"]
         assert metadata["resolution"] == hybrid_metadata["resolution"]
-        
+
         print(f"✅ Frequency-weighted reconciliation verified:")
-        print(f"   - Selected FPS: {metadata['fps']} (most frequent from {frame_rate_detections})")
-        print(f"   - Selected resolution: {metadata['resolution']} (most frequent from {resolution_detections})")
+        print(
+            f"   - Selected FPS: {metadata['fps']} (most frequent from {frame_rate_detections})"
+        )
+        print(
+            f"   - Selected resolution: {metadata['resolution']} (most frequent from {resolution_detections})"
+        )
 
 
 class TestReconciliationErrorCases:
     """Test reconciliation behavior under error conditions."""
 
     @pytest.fixture
     def service_manager_with_broken_monitor(self, mock_dependencies):
         """Create service manager with broken camera monitor for error testing."""
         service_manager = ServiceManager(
-            config=mock_dependencies['config'],
-            mediamtx_controller=mock_dependencies['mediamtx_controller'],
-            websocket_server=mock_dependencies['websocket_server']
-        )
-        
+            config=mock_dependencies["config"],
+            mediamtx_controller=mock_dependencies["mediamtx_controller"],
+            websocket_server=mock_dependencies["websocket_server"],
+        )
+
         # Create mock monitor that raises errors
         broken_monitor = Mock()
-        broken_monitor.get_effective_capability_metadata.side_effect = Exception("Monitor error")
+        broken_monitor.get_effective_capability_metadata.side_effect = Exception(
+            "Monitor error"
+        )
         service_manager._camera_monitor = broken_monitor
-        
+
         return service_manager
 
     @pytest.mark.asyncio
-    async def test_reconciliation_with_monitor_error(self, service_manager_with_broken_monitor):
+    async def test_reconciliation_with_monitor_error(
+        self, service_manager_with_broken_monitor
+    ):
         """Test reconciliation fallback when camera monitor raises errors."""
-        
+
         device_path = "/dev/video0"
-        
-        camera_device = CameraDevice(device_path=device_path, name="Error Test Camera", driver="uvcvideo")
-        event_data = CameraEventData(device_path=device_path, event_type=CameraEvent.CONNECTED, device_info=camera_device)
-        
+
+        camera_device = CameraDevice(
+            device_path=device_path, name="Error Test Camera", driver="uvcvideo"
+        )
+        event_data = CameraEventData(
+            device_path=device_path,
+            event_type=CameraEvent.CONNECTED,
+            device_info=camera_device,
+        )
+
         # Should handle monitor error gracefully
-        metadata = service_manager_with_broken_monitor._get_enhanced_camera_metadata(event_data)
-        
+        metadata = service_manager_with_broken_monitor._get_enhanced_camera_metadata(
+            event_data
+        )
+
         # Should fall back to device info and defaults
         assert metadata["validation_status"] == "error"
         assert metadata["capability_source"] == "default"
         assert metadata["resolution"] == "1920x1080"  # Architecture default
-        assert metadata["fps"] == 30                   # Architecture default
-        
+        assert metadata["fps"] == 30  # Architecture default
+
         print(f"✅ Error reconciliation verified:")
         print(f"   - Validation status: {metadata['validation_status']}")
-        print(f"   - Fallback to defaults: {metadata['resolution']}@{metadata['fps']}fps")
-
-    @pytest.mark.asyncio 
+        print(
+            f"   - Fallback to defaults: {metadata['resolution']}@{metadata['fps']}fps"
+        )
+
+    @pytest.mark.asyncio
     async def test_reconciliation_with_missing_monitor(self, mock_dependencies):
         """Test reconciliation when camera monitor is None."""
-        
+
         service_manager = ServiceManager(
-            config=mock_dependencies['config'],
-            mediamtx_controller=mock_dependencies['mediamtx_controller'],
-            websocket_server=mock_dependencies['websocket_server']
+            config=mock_dependencies["config"],
+            mediamtx_controller=mock_dependencies["mediamtx_controller"],
+            websocket_server=mock_dependencies["websocket_server"],
         )
         # Don't set camera_monitor (remains None)
-        
+
         device_path = "/dev/video0"
-        camera_device = CameraDevice(device_path=device_path, name="No Monitor Camera", driver="uvcvideo")
-        event_data = CameraEventData(device_path=device_path, event_type=CameraEvent.CONNECTED, device_info=camera_device)
-        
+        camera_device = CameraDevice(
+            device_path=device_path, name="No Monitor Camera", driver="uvcvideo"
+        )
+        event_data = CameraEventData(
+            device_path=device_path,
+            event_type=CameraEvent.CONNECTED,
+            device_info=camera_device,
+        )
+
         # Should handle missing monitor gracefully
         metadata = service_manager._get_enhanced_camera_metadata(event_data)
-        
+
         # Should fall back to device info
         assert metadata["validation_status"] == "none"
         assert metadata["capability_source"] == "device_info"
         assert metadata["name"] == "No Monitor Camera"  # From device info
-        
+
         print(f"✅ Missing monitor reconciliation verified:")
         print(f"   - Validation status: {metadata['validation_status']}")
         print(f"   - Capability source: {metadata['capability_source']}")
-        print(f"   - Name from device info: {metadata['name']}")
\ No newline at end of file
+        print(f"   - Name from device info: {metadata['name']}")
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_service/__init__.py	2025-08-03 19:27:38.811778+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_service/__init__.py	2025-08-04 15:35:24.761511+00:00
@@ -2,6 +2,6 @@
 """
 Camera service component tests.
 
 Contains unit tests for camera service orchestration, lifecycle management,
 and component coordination including service manager, configuration, and logging.
-"""
\ No newline at end of file
+"""
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_discovery/test_udev_processing.py	2025-08-03 19:27:38.811778+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_discovery/test_udev_processing.py	2025-08-04 15:35:24.785224+00:00
@@ -1,18 +1,21 @@
 import pytest
 from unittest.mock import AsyncMock
+
 
 @pytest.mark.asyncio
 async def test_udev_event_filtering(monitor):
     class MockUdevDevice:
         def __init__(self, device_node, action):
             self.device_node = device_node
             self.action = action
 
     processed = []
+
     async def fake_handle(event_data):
         processed.append(event_data.device_path)
+
     monitor._handle_camera_event = fake_handle
     monitor._create_camera_device_info = AsyncMock()
 
     cases = [
         ("/dev/video0", "add", True),
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_discovery/test_hybrid_monitor_udev_fallback.py	2025-08-04 02:48:15.365773+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_discovery/test_hybrid_monitor_udev_fallback.py	2025-08-04 15:35:24.874021+00:00
@@ -20,30 +20,30 @@
 from pathlib import Path
 from typing import Dict, List, Any
 
 # Test imports
 from src.camera_discovery.hybrid_monitor import (
-    HybridCameraMonitor, 
-    CameraEvent, 
+    HybridCameraMonitor,
+    CameraEvent,
     CameraEventData,
     CapabilityDetectionResult,
-    DeviceCapabilityState
+    DeviceCapabilityState,
 )
 from src.common.types import CameraDevice
 
 
 class TestUdevEventProcessing:
     """Test udev event handling including edge cases and race conditions."""
 
     @pytest.fixture
     def monitor_with_udev(self):
         """Create monitor with udev enabled for event testing."""
-        with patch('src.camera_discovery.hybrid_monitor.HAS_PYUDEV', True):
+        with patch("src.camera_discovery.hybrid_monitor.HAS_PYUDEV", True):
             return HybridCameraMonitor(
                 device_range=[0, 1, 2],
                 poll_interval=0.1,
-                enable_capability_detection=False  # Focus on event processing
+                enable_capability_detection=False,  # Focus on event processing
             )
 
     @pytest.fixture
     def mock_udev_device(self):
         """Create mock udev device for testing."""
@@ -56,219 +56,237 @@
         return mock_device
 
     @pytest.mark.asyncio
     async def test_udev_add_event_processing(self, monitor_with_udev, mock_udev_device):
         """Test udev 'add' event processing and device registration."""
-        
+
         # Mock device availability checks
-        with patch('pathlib.Path.exists', return_value=True), \
-             patch('builtins.open', return_value=Mock()), \
-             patch.object(monitor_with_udev, '_should_monitor_device', return_value=True):
-            
+        with (
+            patch("pathlib.Path.exists", return_value=True),
+            patch("builtins.open", return_value=Mock()),
+            patch.object(
+                monitor_with_udev, "_should_monitor_device", return_value=True
+            ),
+        ):
+
             # Setup event handler to capture events
             captured_events = []
+
             async def capture_event(event_data: CameraEventData):
                 captured_events.append(event_data)
-            
+
             monitor_with_udev.add_event_callback(capture_event)
-            
+
             # Simulate udev add event
             mock_udev_device.action = "add"
             await monitor_with_udev._handle_udev_event(mock_udev_device)
-            
+
             # Verify event processing
             assert len(captured_events) == 1
             event = captured_events[0]
             assert event.event_type == CameraEvent.CONNECTED
             assert event.device_path == "/dev/video0"
-            
+
             # Verify device tracking
             assert "/dev/video0" in monitor_with_udev._known_devices
-            
+
             # Verify stats update
             stats = monitor_with_udev.get_monitor_stats()
-            assert stats['udev_events_processed'] == 1
-
-    @pytest.mark.asyncio
-    async def test_udev_remove_event_processing(self, monitor_with_udev, mock_udev_device):
+            assert stats["udev_events_processed"] == 1
+
+    @pytest.mark.asyncio
+    async def test_udev_remove_event_processing(
+        self, monitor_with_udev, mock_udev_device
+    ):
         """Test udev 'remove' event processing and device cleanup."""
-        
+
         # Pre-populate device
         test_device = CameraDevice(
-            device_path="/dev/video0",
-            name="Test Camera",
-            driver="uvcvideo"
+            device_path="/dev/video0", name="Test Camera", driver="uvcvideo"
         )
         monitor_with_udev._known_devices["/dev/video0"] = test_device
-        
-        captured_events = []
-        async def capture_event(event_data: CameraEventData):
-            captured_events.append(event_data)
-        
+
+        captured_events = []
+
+        async def capture_event(event_data: CameraEventData):
+            captured_events.append(event_data)
+
         monitor_with_udev.add_event_callback(capture_event)
-        
+
         # Simulate udev remove event
         mock_udev_device.action = "remove"
         await monitor_with_udev._handle_udev_event(mock_udev_device)
-        
+
         # Verify event processing
         assert len(captured_events) == 1
         event = captured_events[0]
         assert event.event_type == CameraEvent.DISCONNECTED
         assert event.device_path == "/dev/video0"
-        
+
         # Verify device removal
         assert "/dev/video0" not in monitor_with_udev._known_devices
-        
+
         # Verify capability state cleanup if it exists
         assert "/dev/video0" not in monitor_with_udev._capability_states
 
     @pytest.mark.asyncio
-    async def test_udev_change_event_processing(self, monitor_with_udev, mock_udev_device):
+    async def test_udev_change_event_processing(
+        self, monitor_with_udev, mock_udev_device
+    ):
         """Test udev 'change' event processing for device state updates."""
-        
+
         # Pre-populate device
         test_device = CameraDevice(
-            device_path="/dev/video0",
-            name="Test Camera",
-            driver="uvcvideo"
+            device_path="/dev/video0", name="Test Camera", driver="uvcvideo"
         )
         monitor_with_udev._known_devices["/dev/video0"] = test_device
-        
-        captured_events = []
-        async def capture_event(event_data: CameraEventData):
-            captured_events.append(event_data)
-        
+
+        captured_events = []
+
+        async def capture_event(event_data: CameraEventData):
+            captured_events.append(event_data)
+
         monitor_with_udev.add_event_callback(capture_event)
-        
+
         # Simulate udev change event
         mock_udev_device.action = "change"
-        with patch('pathlib.Path.exists', return_value=True), \
-             patch('builtins.open', return_value=Mock()):
-            
+        with (
+            patch("pathlib.Path.exists", return_value=True),
+            patch("builtins.open", return_value=Mock()),
+        ):
+
             await monitor_with_udev._handle_udev_event(mock_udev_device)
-        
+
         # Verify event processing
         assert len(captured_events) == 1
         event = captured_events[0]
         assert event.event_type == CameraEvent.STATUS_CHANGED
         assert event.device_path == "/dev/video0"
 
     @pytest.mark.asyncio
     async def test_udev_event_race_conditions(self, monitor_with_udev):
         """Test rapid sequential udev events to detect race conditions."""
-        
-        captured_events = []
-        async def capture_event(event_data: CameraEventData):
-            captured_events.append(event_data)
-        
+
+        captured_events = []
+
+        async def capture_event(event_data: CameraEventData):
+            captured_events.append(event_data)
+
         monitor_with_udev.add_event_callback(capture_event)
-        
+
         # Create multiple mock devices
         devices = []
         for i in range(3):
             mock_device = Mock()
             mock_device.device_path = f"/dev/video{i}"
             mock_device.device_node = f"/dev/video{i}"
             mock_device.get.return_value = f"camera_{i}"
             mock_device.action = "add"
             mock_device.subsystem = "video4linux"
             devices.append(mock_device)
-        
-        with patch('pathlib.Path.exists', return_value=True), \
-             patch('builtins.open', return_value=Mock()), \
-             patch.object(monitor_with_udev, '_should_monitor_device', return_value=True):
-            
+
+        with (
+            patch("pathlib.Path.exists", return_value=True),
+            patch("builtins.open", return_value=Mock()),
+            patch.object(
+                monitor_with_udev, "_should_monitor_device", return_value=True
+            ),
+        ):
+
             # Fire rapid sequential events
             tasks = []
             for device in devices:
                 task = asyncio.create_task(monitor_with_udev._handle_udev_event(device))
                 tasks.append(task)
-            
+
             # Wait for all events to process
             await asyncio.gather(*tasks)
-        
+
         # Verify all events processed correctly
         assert len(captured_events) == 3
         assert len(monitor_with_udev._known_devices) == 3
-        
+
         # Verify no race condition artifacts
         device_paths = [event.device_path for event in captured_events]
         assert "/dev/video0" in device_paths
         assert "/dev/video1" in device_paths
         assert "/dev/video2" in device_paths
 
     @pytest.mark.asyncio
     async def test_invalid_device_node_handling(self, monitor_with_udev):
         """Test handling of udev events with invalid or inaccessible device nodes."""
-        
-        captured_events = []
-        async def capture_event(event_data: CameraEventData):
-            captured_events.append(event_data)
-        
+
+        captured_events = []
+
+        async def capture_event(event_data: CameraEventData):
+            captured_events.append(event_data)
+
         monitor_with_udev.add_event_callback(capture_event)
-        
+
         # Create mock device with invalid path
         mock_device = Mock()
         mock_device.device_path = "/dev/video999"  # Out of range
         mock_device.device_node = "/dev/video999"
         mock_device.get.return_value = "invalid_camera"
         mock_device.action = "add"
         mock_device.subsystem = "video4linux"
-        
+
         # Simulate device path doesn't exist
-        with patch('pathlib.Path.exists', return_value=False):
+        with patch("pathlib.Path.exists", return_value=False):
             await monitor_with_udev._handle_udev_event(mock_device)
-        
+
         # Should not generate events for invalid devices
         assert len(captured_events) == 0
         assert len(monitor_with_udev._known_devices) == 0
-        
+
         # Verify stats show filtered event
         stats = monitor_with_udev.get_monitor_stats()
-        assert stats['udev_events_filtered'] > 0
-
-    @pytest.mark.asyncio  
+        assert stats["udev_events_filtered"] > 0
+
+    @pytest.mark.asyncio
     async def test_device_range_filtering(self, monitor_with_udev):
         """Test udev event filtering based on configured device range."""
-        
-        captured_events = []
-        async def capture_event(event_data: CameraEventData):
-            captured_events.append(event_data)
-        
+
+        captured_events = []
+
+        async def capture_event(event_data: CameraEventData):
+            captured_events.append(event_data)
+
         monitor_with_udev.add_event_callback(capture_event)
-        
+
         # Test devices both in and out of range
         test_cases = [
-            ("/dev/video0", True),   # In range
-            ("/dev/video1", True),   # In range  
-            ("/dev/video2", True),   # In range
+            ("/dev/video0", True),  # In range
+            ("/dev/video1", True),  # In range
+            ("/dev/video2", True),  # In range
             ("/dev/video5", False),  # Out of range
-            ("/dev/video10", False), # Out of range
+            ("/dev/video10", False),  # Out of range
         ]
-        
+
         for device_path, should_process in test_cases:
             mock_device = Mock()
             mock_device.device_path = device_path
             mock_device.device_node = device_path
             mock_device.get.return_value = "test_camera"
             mock_device.action = "add"
             mock_device.subsystem = "video4linux"
-            
-            with patch('pathlib.Path.exists', return_value=True), \
-                 patch('builtins.open', return_value=Mock()):
-                
+
+            with (
+                patch("pathlib.Path.exists", return_value=True),
+                patch("builtins.open", return_value=Mock()),
+            ):
+
                 await monitor_with_udev._handle_udev_event(mock_device)
-        
+
         # Only devices in range [0,1,2] should generate events
         processed_devices = [event.device_path for event in captured_events]
         assert "/dev/video0" in processed_devices
         assert "/dev/video1" in processed_devices
         assert "/dev/video2" in processed_devices
         assert "/dev/video5" not in processed_devices
         assert "/dev/video10" not in processed_devices
-        
+
         assert len(captured_events) == 3  # Only 3 in-range devices
 
 
 class TestPollingFallback:
     """Test polling fallback behavior when udev events are missed or stale."""
@@ -277,151 +295,173 @@
     def monitor_polling_fallback(self):
         """Create monitor configured for polling fallback testing."""
         return HybridCameraMonitor(
             device_range=[0, 1],
             poll_interval=0.05,  # Fast polling for testing
-            enable_capability_detection=False
+            enable_capability_detection=False,
         )
 
     @pytest.mark.asyncio
     async def test_polling_fallback_when_udev_stale(self, monitor_polling_fallback):
         """Test polling fallback activation when udev events become stale."""
-        
+
         # Mock initial state - udev events are fresh
         monitor_polling_fallback._last_udev_event_time = time.time()
-        monitor_polling_fallback._udev_event_freshness_threshold = 1.0  # 1 second threshold
-        
-        captured_events = []
-        async def capture_event(event_data: CameraEventData):
-            captured_events.append(event_data)
-        
+        monitor_polling_fallback._udev_event_freshness_threshold = (
+            1.0  # 1 second threshold
+        )
+
+        captured_events = []
+
+        async def capture_event(event_data: CameraEventData):
+            captured_events.append(event_data)
+
         monitor_polling_fallback.add_event_callback(capture_event)
-        
+
         # Mock device discovery to find new device
-        with patch.object(monitor_polling_fallback, '_discover_cameras') as mock_discover:
+        with patch.object(
+            monitor_polling_fallback, "_discover_cameras"
+        ) as mock_discover:
             mock_discover.return_value = None  # Discovery method doesn't return
-            
+
             # Fast-forward time to make udev events stale
-            with patch('time.time', return_value=time.time() + 2.0):
+            with patch("time.time", return_value=time.time() + 2.0):
                 # Run polling cycle
                 await monitor_polling_fallback._polling_monitor()
-            
+
             # Verify polling was triggered due to stale udev events
             mock_discover.assert_called_once()
 
     @pytest.mark.asyncio
     async def test_adaptive_polling_interval_adjustment(self, monitor_polling_fallback):
         """Test adaptive polling interval adjustment based on udev event freshness."""
-        
+
         initial_interval = monitor_polling_fallback._current_poll_interval
-        
+
         # Simulate stale udev events (should increase polling frequency)
-        monitor_polling_fallback._last_udev_event_time = time.time() - 30.0  # Very stale
+        monitor_polling_fallback._last_udev_event_time = (
+            time.time() - 30.0
+        )  # Very stale
         monitor_polling_fallback._udev_event_freshness_threshold = 15.0
-        
+
         # Mock polling cycle execution
-        with patch.object(monitor_polling_fallback, '_discover_cameras'):
+        with patch.object(monitor_polling_fallback, "_discover_cameras"):
             await monitor_polling_fallback._polling_monitor()
-        
+
         # Polling interval should have decreased (higher frequency)
         assert monitor_polling_fallback._current_poll_interval < initial_interval
-        
+
         # Stats should reflect adjustment
         stats = monitor_polling_fallback.get_monitor_stats()
-        assert stats['adaptive_poll_adjustments'] > 0
-        assert stats['current_poll_interval'] == monitor_polling_fallback._current_poll_interval
+        assert stats["adaptive_poll_adjustments"] > 0
+        assert (
+            stats["current_poll_interval"]
+            == monitor_polling_fallback._current_poll_interval
+        )
 
     @pytest.mark.asyncio
     async def test_polling_failure_recovery(self, monitor_polling_fallback):
         """Test polling failure handling and recovery behavior."""
-        
+
         # Mock discovery failures
         failure_count = 0
+
         async def mock_discover_with_failures():
             nonlocal failure_count
             failure_count += 1
             if failure_count <= 3:  # Fail first 3 attempts
                 raise OSError("Mock discovery failure")
             # Succeed on 4th attempt
             return None
-        
-        with patch.object(monitor_polling_fallback, '_discover_cameras', side_effect=mock_discover_with_failures):
-            
+
+        with patch.object(
+            monitor_polling_fallback,
+            "_discover_cameras",
+            side_effect=mock_discover_with_failures,
+        ):
+
             # Run multiple polling cycles
             for _ in range(5):
                 try:
                     await monitor_polling_fallback._polling_monitor()
                 except Exception:
                     pass  # Expected for first few attempts
-        
+
         # Verify failure tracking
-        assert monitor_polling_fallback._polling_failure_count <= monitor_polling_fallback._max_consecutive_failures
-        
+        assert (
+            monitor_polling_fallback._polling_failure_count
+            <= monitor_polling_fallback._max_consecutive_failures
+        )
+
         # Stats should reflect failures and recovery
         stats = monitor_polling_fallback.get_monitor_stats()
-        assert stats['polling_cycles'] >= 5
+        assert stats["polling_cycles"] >= 5
 
     @pytest.mark.asyncio
     async def test_polling_discovers_missed_device(self, monitor_polling_fallback):
         """Test that polling detects devices missed by udev events."""
-        
-        captured_events = []
-        async def capture_event(event_data: CameraEventData):
-            captured_events.append(event_data)
-        
+
+        captured_events = []
+
+        async def capture_event(event_data: CameraEventData):
+            captured_events.append(event_data)
+
         monitor_polling_fallback.add_event_callback(capture_event)
-        
+
         # Mock device that exists but wasn't detected by udev
-        test_devices = {
-            "/dev/video0": ("CONNECTED", "Missed Camera")
-        }
-        
+        test_devices = {"/dev/video0": ("CONNECTED", "Missed Camera")}
+
         def mock_path_exists(path_str):
             return str(path_str) in test_devices
-        
-        def mock_open_device(path, mode='rb'):
+
+        def mock_open_device(path, mode="rb"):
             if str(path) in test_devices and test_devices[str(path)][0] == "CONNECTED":
                 return Mock()
             raise OSError("Device not accessible")
-        
-        with patch('pathlib.Path.exists', side_effect=mock_path_exists), \
-             patch('builtins.open', side_effect=mock_open_device):
-            
+
+        with (
+            patch("pathlib.Path.exists", side_effect=mock_path_exists),
+            patch("builtins.open", side_effect=mock_open_device),
+        ):
+
             # Run discovery cycle
             await monitor_polling_fallback._discover_cameras()
-        
+
         # Should have discovered the missed device
         assert len(captured_events) > 0
         assert any(event.device_path == "/dev/video0" for event in captured_events)
         assert "/dev/video0" in monitor_polling_fallback._known_devices
 
     @pytest.mark.asyncio
     async def test_polling_only_mode_fallback(self, monitor_polling_fallback):
         """Test operation when udev is completely unavailable (polling-only mode)."""
-        
+
         # Disable udev completely
-        with patch('src.camera_discovery.hybrid_monitor.HAS_PYUDEV', False):
+        with patch("src.camera_discovery.hybrid_monitor.HAS_PYUDEV", False):
             monitor_no_udev = HybridCameraMonitor(
                 device_range=[0, 1],
                 poll_interval=0.05,
-                enable_capability_detection=False
+                enable_capability_detection=False,
             )
-            
+
             captured_events = []
+
             async def capture_event(event_data: CameraEventData):
                 captured_events.append(event_data)
-            
+
             monitor_no_udev.add_event_callback(capture_event)
-            
+
             # Mock device existence for polling detection
-            with patch('pathlib.Path.exists', return_value=True), \
-                 patch('builtins.open', return_value=Mock()):
-                
+            with (
+                patch("pathlib.Path.exists", return_value=True),
+                patch("builtins.open", return_value=Mock()),
+            ):
+
                 await monitor_no_udev._discover_cameras()
-            
+
             # Should still detect devices through polling
             assert len(captured_events) > 0
-            
+
             # Verify polling-only stats
             stats = monitor_no_udev.get_monitor_stats()
-            assert stats['polling_cycles'] > 0
-            assert stats['udev_events_processed'] == 0  # No udev in polling-only mode
\ No newline at end of file
+            assert stats["polling_cycles"] > 0
+            assert stats["udev_events_processed"] == 0  # No udev in polling-only mode
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_discovery/test_hybrid_monitor_comprehensive.py	2025-08-03 19:27:38.811778+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_discovery/test_hybrid_monitor_comprehensive.py	2025-08-04 15:35:25.039374+00:00
@@ -1,11 +1,11 @@
 """
 Comprehensive test scaffolds for hybrid camera monitor hardening validation.
 
 Test coverage areas:
 - Capability parsing variations (multiple fps, malformed output)
-- Udev add/remove/change/race condition simulations  
+- Udev add/remove/change/race condition simulations
 - Polling fallback behavior when udev is silent
 - Timeout and subprocess failure handling
 - Provisional/confirmed capability validation logic
 - Adaptive polling with backoff and jitter
 """
@@ -19,15 +19,15 @@
 from pathlib import Path
 from typing import Dict, List, Any
 
 # Test imports
 from src.camera_discovery.hybrid_monitor import (
-    HybridCameraMonitor, 
-    CameraEvent, 
+    HybridCameraMonitor,
+    CameraEvent,
     CameraEventData,
     CapabilityDetectionResult,
-    DeviceCapabilityState
+    DeviceCapabilityState,
 )
 from src.common.types import CameraDevice
 
 
 class TestCapabilityParsingVariations:
@@ -35,13 +35,13 @@
 
     @pytest.fixture
     def monitor(self):
         """Create monitor with capability detection enabled."""
         return HybridCameraMonitor(
-            device_range=[0, 1, 2], 
+            device_range=[0, 1, 2],
             enable_capability_detection=True,
-            detection_timeout=2.0
+            detection_timeout=2.0,
         )
 
     def test_frame_rate_extraction_comprehensive_patterns(self, monitor):
         """Test frame rate extraction from comprehensive v4l2-ctl output patterns."""
         test_cases = [
@@ -49,145 +49,167 @@
             ("30.000 fps", {"30"}),
             ("25.000 FPS", {"25"}),
             ("Frame rate: 60.0", {"60"}),
             ("1920x1080@30", {"30"}),
             ("15 Hz", {"15"}),
-            
             # Interval patterns
             ("Interval: [1/30]", {"30"}),
             ("[1/25]", {"25"}),
             ("1/15 s", {"15"}),
-            
             # Complex patterns
             ("30 frames per second", {"30"}),
             ("rate: 25.5", {"25.5"}),
             ("fps: 60", {"60"}),
-            
             # Multiple rates in one output
             ("30.000 fps, 25 FPS, [1/15], 60 Hz", {"30", "25", "15", "60"}),
-            
             # Edge cases
             ("", set()),
             ("no frame rates here", set()),
             ("300 fps", {"300"}),  # High rate
             ("1.5 fps", {"1.5"}),  # Low rate
-            ("0 fps", set()),      # Invalid rate (filtered out)
-            ("500 fps", set()),    # Invalid rate (filtered out)
-            
+            ("0 fps", set()),  # Invalid rate (filtered out)
+            ("500 fps", set()),  # Invalid rate (filtered out)
             # Malformed patterns
-            ("30.000.000 fps", set()),     # Double decimal
-            ("abc fps", set()),            # Non-numeric
+            ("30.000.000 fps", set()),  # Double decimal
+            ("abc fps", set()),  # Non-numeric
             ("fps without number", set()),  # No number
         ]
-        
+
         for output, expected in test_cases:
             result = monitor._extract_frame_rates_from_output(output)
-            assert result == expected, f"Failed for output: '{output}' - expected {expected}, got {result}"
+            assert (
+                result == expected
+            ), f"Failed for output: '{output}' - expected {expected}, got {result}"
 
     @pytest.mark.asyncio
     async def test_capability_parsing_malformed_v4l2_outputs(self, monitor):
         """Test capability detection resilience against malformed v4l2-ctl outputs."""
-        
+
         malformed_outputs = [
             # Empty/minimal outputs
             ("", False, "empty output"),
             ("v4l2-ctl: error", False, "error output"),
-            
             # Partial outputs
-            ("Card type: USB Camera\nDriver name: uvcvideo", True, "minimal valid info"),
+            (
+                "Card type: USB Camera\nDriver name: uvcvideo",
+                True,
+                "minimal valid info",
+            ),
             ("Some random text without useful info", False, "random text"),
-            
             # Malformed format sections
             ("Format [0]: corrupted data\nSize: invalid", False, "corrupted format"),
-            ("Valid start\n[CORRUPTED MIDDLE SECTION]\nValid end", False, "corrupted middle"),
-            
+            (
+                "Valid start\n[CORRUPTED MIDDLE SECTION]\nValid end",
+                False,
+                "corrupted middle",
+            ),
             # Mixed valid/invalid data
-            ("Card type: Test Camera\nFormat [0]: 'YUYV'\nSize: Discrete 1920x1080\nCorrupted frame rate data", True, "mixed valid/invalid"),
+            (
+                "Card type: Test Camera\nFormat [0]: 'YUYV'\nSize: Discrete 1920x1080\nCorrupted frame rate data",
+                True,
+                "mixed valid/invalid",
+            ),
         ]
-        
+
         for output, should_succeed, description in malformed_outputs:
-            with patch('asyncio.create_subprocess_exec') as mock_subprocess:
+            with patch("asyncio.create_subprocess_exec") as mock_subprocess:
                 # Mock process that returns our test output
                 mock_process = AsyncMock()
                 mock_process.communicate.return_value = (output.encode(), b"")
                 mock_process.returncode = 0
                 mock_subprocess.return_value = mock_process
-                
+
                 result = await monitor._probe_device_capabilities("/dev/video0")
-                
+
                 if should_succeed:
-                    assert result.detected, f"Should have succeeded for {description}: {output}"
+                    assert (
+                        result.detected
+                    ), f"Should have succeeded for {description}: {output}"
                     assert result.accessible, f"Should be accessible for {description}"
                 else:
-                    assert not result.detected or result.error, f"Should have failed for {description}: {output}"
+                    assert (
+                        not result.detected or result.error
+                    ), f"Should have failed for {description}: {output}"
 
     @pytest.mark.asyncio
     async def test_capability_timeout_handling(self, monitor):
         """Test capability detection timeout scenarios with structured diagnostics."""
-        
+
         timeout_scenarios = [
             ("device_info", "_probe_device_info_robust"),
             ("formats", "_probe_device_formats_robust"),
             ("framerates", "_probe_device_framerates_robust"),
             ("overall", "_probe_device_capabilities"),
         ]
-        
+
         for scenario_name, method_name in timeout_scenarios:
             # Test timeout in specific method
             with patch.object(monitor, method_name, side_effect=asyncio.TimeoutError()):
                 result = await monitor._probe_device_capabilities("/dev/video0")
-                
+
                 assert not result.detected, f"Should fail on {scenario_name} timeout"
-                assert "timeout" in result.error.lower(), f"Error should mention timeout for {scenario_name}"
-                assert result.timeout_context is not None, f"Timeout context should be set for {scenario_name}"
-                assert result.structured_diagnostics, f"Should have diagnostics for {scenario_name}"
+                assert (
+                    "timeout" in result.error.lower()
+                ), f"Error should mention timeout for {scenario_name}"
+                assert (
+                    result.timeout_context is not None
+                ), f"Timeout context should be set for {scenario_name}"
+                assert (
+                    result.structured_diagnostics
+                ), f"Should have diagnostics for {scenario_name}"
 
     @pytest.mark.asyncio
     async def test_provisional_confirmed_capability_validation(self, monitor):
         """Test provisional/confirmed capability validation state machine."""
-        
+
         device_path = "/dev/video0"
-        
+
         # Create consistent capability results
         consistent_result = CapabilityDetectionResult(
             device_path=device_path,
             detected=True,
             accessible=True,
             resolutions=["1920x1080", "1280x720"],
             frame_rates=["30", "25", "15"],
-            formats=[{"code": "YUYV", "description": "YUYV 4:2:2"}]
-        )
-        
+            formats=[{"code": "YUYV", "description": "YUYV 4:2:2"}],
+        )
+
         # Test provisional state establishment
-        await monitor._update_capability_validation_state(device_path, consistent_result)
+        await monitor._update_capability_validation_state(
+            device_path, consistent_result
+        )
         state = monitor._get_capability_state_for_testing(device_path)
-        
+
         assert state is not None
         assert state.provisional_data is not None
         assert state.confirmed_data is None
         assert state.consecutive_successes == 1
         assert not state.is_confirmed()
-        
+
         # Test confirmation through consistent results
-        await monitor._update_capability_validation_state(device_path, consistent_result)
-        
+        await monitor._update_capability_validation_state(
+            device_path, consistent_result
+        )
+
         assert state.consecutive_successes == 2
         assert state.confirmed_data is not None
         assert state.is_confirmed()
-        
+
         # Test inconsistent data handling
         inconsistent_result = CapabilityDetectionResult(
             device_path=device_path,
             detected=True,
             accessible=True,
             resolutions=["640x480"],  # Different resolutions
-            frame_rates=["60"],       # Different frame rates
-            formats=[{"code": "MJPG", "description": "MJPEG"}]  # Different format
-        )
-        
-        await monitor._update_capability_validation_state(device_path, inconsistent_result)
-        
+            frame_rates=["60"],  # Different frame rates
+            formats=[{"code": "MJPG", "description": "MJPEG"}],  # Different format
+        )
+
+        await monitor._update_capability_validation_state(
+            device_path, inconsistent_result
+        )
+
         # Should reset to provisional state
         assert state.consecutive_successes == 1
         assert state.confirmed_data is None
         assert not state.is_confirmed()
 
@@ -199,244 +221,256 @@
     def monitor(self):
         """Create monitor with test configuration."""
         return HybridCameraMonitor(
             device_range=[0, 1, 2],
             poll_interval=0.1,
-            enable_capability_detection=False  # Simplify for udev testing
+            enable_capability_detection=False,  # Simplify for udev testing
         )
 
     @pytest.mark.asyncio
     async def test_udev_event_filtering_comprehensive(self, monitor):
         """Test comprehensive udev event filtering scenarios."""
-        
+
         monitor._set_test_mode(True)
-        
+
         # Track statistics before
         initial_stats = monitor.get_monitor_stats()
-        
+
         filter_test_cases = [
             # Valid events (should be processed)
             ("/dev/video0", "add", True, "valid device in range"),
             ("/dev/video1", "remove", True, "valid device removal"),
             ("/dev/video2", "change", True, "valid device change"),
-            
             # Invalid events (should be filtered)
             ("/dev/video5", "add", False, "device outside range"),
             ("/dev/audio0", "add", False, "non-video device"),
             ("/dev/invalid", "add", False, "malformed device path"),
             (None, "add", False, "null device path"),
             ("", "add", False, "empty device path"),
             ("/dev/video", "add", False, "device path without number"),
-            
             # Edge cases
             ("/dev/video0extra", "add", False, "device path with extra text"),
             ("/custom/video0", "add", False, "non-standard device path"),
         ]
-        
+
         processed_count = 0
         filtered_count = 0
-        
+
         for device_path, action, should_process, description in filter_test_cases:
             # Create mock udev device
             mock_device = Mock()
             mock_device.device_node = device_path
             mock_device.action = action
-            
+
             await monitor._process_udev_device_event(mock_device)
-            
+
             if should_process:
                 processed_count += 1
             else:
                 filtered_count += 1
-        
+
         # Verify statistics
         final_stats = monitor.get_monitor_stats()
-        
+
         # Note: Some processed events might not result in state changes if device doesn't exist
-        assert final_stats['udev_events_filtered'] >= initial_stats['udev_events_filtered'] + filtered_count
-        
+        assert (
+            final_stats["udev_events_filtered"]
+            >= initial_stats["udev_events_filtered"] + filtered_count
+        )
+
         monitor._set_test_mode(False)
 
     @pytest.mark.asyncio
     async def test_udev_race_condition_simulation(self, monitor):
         """Test race condition handling in concurrent udev events."""
-        
+
         monitor._set_test_mode(True)
         device_path = "/dev/video0"
-        
+
         # Mock device creation to succeed
-        with patch.object(monitor, '_create_camera_device_info') as mock_create:
+        with patch.object(monitor, "_create_camera_device_info") as mock_create:
             mock_device = CameraDevice(
-                device_path=device_path,
-                name="Test Camera",
-                status="CONNECTED"
+                device_path=device_path, name="Test Camera", status="CONNECTED"
             )
             mock_create.return_value = mock_device
-            
+
             # Simulate rapid add/remove/add sequence (race condition)
             event_sequence = [
                 ("add", 0.01),
                 ("remove", 0.01),
                 ("add", 0.01),
                 ("change", 0.01),
-                ("remove", 0.01)
+                ("remove", 0.01),
             ]
-            
+
             # Execute events concurrently
             tasks = []
             for action, delay in event_sequence:
                 task = asyncio.create_task(
-                    self._simulate_delayed_udev_event(monitor, device_path, action, delay)
+                    self._simulate_delayed_udev_event(
+                        monitor, device_path, action, delay
+                    )
                 )
                 tasks.append(task)
-            
+
             # Wait for all events to complete
             await asyncio.gather(*tasks, return_exceptions=True)
-            
+
             # Verify final state consistency
             stats = monitor.get_monitor_stats()
-            assert stats['udev_events_processed'] >= len(event_sequence)
-            
+            assert stats["udev_events_processed"] >= len(event_sequence)
+
         monitor._set_test_mode(False)
 
-    async def _simulate_delayed_udev_event(self, monitor, device_path: str, action: str, delay: float):
+    async def _simulate_delayed_udev_event(
+        self, monitor, device_path: str, action: str, delay: float
+    ):
         """Simulate udev event with delay for race condition testing."""
         await asyncio.sleep(delay)
         await monitor._inject_test_udev_event(device_path, action)
 
     @pytest.mark.asyncio
     async def test_udev_change_event_status_detection(self, monitor):
         """Test udev 'change' event proper status change detection."""
-        
+
         monitor._set_test_mode(True)
         device_path = "/dev/video0"
-        
+
         # Mock sequence: device appears, status changes, disappears
         status_sequence = ["CONNECTED", "ERROR", "DISCONNECTED"]
-        
+
         for i, status in enumerate(status_sequence):
             mock_device = CameraDevice(
-                device_path=device_path,
-                name="Test Camera",
-                status=status
+                device_path=device_path, name="Test Camera", status=status
             )
-            
-            with patch.object(monitor, '_create_camera_device_info', return_value=mock_device):
+
+            with patch.object(
+                monitor, "_create_camera_device_info", return_value=mock_device
+            ):
                 if i == 0:
                     # First event: add
                     await monitor._inject_test_udev_event(device_path, "add")
                 else:
                     # Subsequent events: change
                     await monitor._inject_test_udev_event(device_path, "change")
-        
+
         monitor._set_test_mode(False)
 
 
 class TestPollingFallbackBehavior:
     """Test polling fallback behavior when udev is silent or unavailable."""
 
     @pytest.fixture
     def monitor_no_udev(self):
         """Create monitor with udev disabled to test polling-only mode."""
-        with patch('src.camera_discovery.hybrid_monitor.HAS_PYUDEV', False):
+        with patch("src.camera_discovery.hybrid_monitor.HAS_PYUDEV", False):
             return HybridCameraMonitor(
                 device_range=[0, 1],
                 poll_interval=0.05,  # Fast polling for testing
-                enable_capability_detection=False
+                enable_capability_detection=False,
             )
 
     @pytest.mark.asyncio
     async def test_polling_only_mode_device_discovery(self, monitor_no_udev):
         """Test device discovery in polling-only mode."""
-        
+
         # Mock device existence
         test_devices = {
             "/dev/video0": ("CONNECTED", "Camera 0"),
-            "/dev/video1": ("DISCONNECTED", "Camera 1")
+            "/dev/video1": ("DISCONNECTED", "Camera 1"),
         }
-        
-        with patch('pathlib.Path.exists') as mock_exists, \
-             patch('builtins.open') as mock_open:
-            
+
+        with (
+            patch("pathlib.Path.exists") as mock_exists,
+            patch("builtins.open") as mock_open,
+        ):
+
             def mock_path_exists(path_str):
                 return str(path_str) in test_devices
-            
-            def mock_open_device(path, mode='rb'):
+
+            def mock_open_device(path, mode="rb"):
                 if path in test_devices and test_devices[path][0] == "CONNECTED":
                     return Mock()  # Successful open
                 else:
                     raise OSError("Device not accessible")
-            
+
             mock_exists.side_effect = mock_path_exists
             mock_open.side_effect = mock_open_device
-            
+
             # Mock event handler to capture events
             event_handler = Mock()
             event_handler.handle_camera_event = AsyncMock()
             monitor_no_udev.add_event_handler(event_handler)
-            
+
             # Run discovery cycle
             await monitor_no_udev._discover_cameras()
-            
+
             # Verify discovery results
             stats = monitor_no_udev.get_monitor_stats()
-            assert stats['polling_cycles'] > 0
-            
+            assert stats["polling_cycles"] > 0
+
             # Should have found devices
             assert len(monitor_no_udev._known_devices) > 0
 
     @pytest.mark.asyncio
     async def test_adaptive_polling_interval_adjustment(self, monitor_no_udev):
         """Test adaptive polling interval adjustment based on udev event freshness."""
-        
+
         # Get initial polling state
         initial_state = monitor_no_udev._get_adaptive_polling_state_for_testing()
-        
+
         # Simulate scenario: no recent udev events (should increase polling frequency)
         current_time = time.time()
         monitor_no_udev._last_udev_event_time = current_time - 20.0  # 20 seconds ago
-        
+
         await monitor_no_udev._adjust_polling_interval()
-        
+
         state_after_stale = monitor_no_udev._get_adaptive_polling_state_for_testing()
-        
+
         # Should have reduced interval (increased frequency)
-        assert state_after_stale['current_interval'] < initial_state['current_interval']
-        
+        assert state_after_stale["current_interval"] < initial_state["current_interval"]
+
         # Simulate scenario: recent udev events (should decrease polling frequency)
         monitor_no_udev._last_udev_event_time = current_time - 2.0  # 2 seconds ago
-        
+
         await monitor_no_udev._adjust_polling_interval()
-        
+
         state_after_fresh = monitor_no_udev._get_adaptive_polling_state_for_testing()
-        
+
         # Should have increased interval (decreased frequency)
-        assert state_after_fresh['current_interval'] > state_after_stale['current_interval']
+        assert (
+            state_after_fresh["current_interval"]
+            > state_after_stale["current_interval"]
+        )
 
     @pytest.mark.asyncio
     async def test_polling_failure_backoff_with_jitter(self, monitor_no_udev):
         """Test polling failure handling with exponential backoff and jitter."""
-        
+
         # Mock discovery to always fail
-        with patch.object(monitor_no_udev, '_discover_cameras', side_effect=Exception("Simulated failure")):
-            
+        with patch.object(
+            monitor_no_udev,
+            "_discover_cameras",
+            side_effect=Exception("Simulated failure"),
+        ):
+
             # Monitor polling state
             initial_failure_count = monitor_no_udev._polling_failure_count
-            
+
             # Simulate several polling failures
             for i in range(3):
                 try:
                     await monitor_no_udev._discover_cameras()
                 except:
                     monitor_no_udev._polling_failure_count += 1
-            
+
             # Verify failure count increased
             assert monitor_no_udev._polling_failure_count > initial_failure_count
-            
+
             # Test backoff calculation
             state = monitor_no_udev._get_adaptive_polling_state_for_testing()
-            assert state['failure_count'] > 0
+            assert state["failure_count"] > 0
 
 
 class TestTimeoutAndSubprocessFailureHandling:
     """Test timeout and subprocess failure handling with structured error reporting."""
 
@@ -444,224 +478,245 @@
     def monitor(self):
         """Create monitor for timeout testing."""
         return HybridCameraMonitor(
             device_range=[0],
             detection_timeout=0.5,  # Short timeout for testing
-            enable_capability_detection=True
+            enable_capability_detection=True,
         )
 
     @pytest.mark.asyncio
     async def test_subprocess_timeout_handling(self, monitor):
         """Test subprocess timeout handling with proper error contexts."""
-        
+
         device_path = "/dev/video0"
-        
+
         # Test timeout in different subprocess operations
         timeout_scenarios = [
-            ("v4l2-ctl device info", ['v4l2-ctl', '--device', device_path, '--info']),
-            ("v4l2-ctl formats", ['v4l2-ctl', '--device', device_path, '--list-formats-ext']),
-            ("v4l2-ctl framerates", ['v4l2-ctl', '--device', device_path, '--list-framesizes', 'YUYV']),
+            ("v4l2-ctl device info", ["v4l2-ctl", "--device", device_path, "--info"]),
+            (
+                "v4l2-ctl formats",
+                ["v4l2-ctl", "--device", device_path, "--list-formats-ext"],
+            ),
+            (
+                "v4l2-ctl framerates",
+                ["v4l2-ctl", "--device", device_path, "--list-framesizes", "YUYV"],
+            ),
         ]
-        
+
         for scenario_name, cmd in timeout_scenarios:
-            with patch('asyncio.create_subprocess_exec') as mock_subprocess:
+            with patch("asyncio.create_subprocess_exec") as mock_subprocess:
                 # Mock subprocess that never completes
                 mock_process = AsyncMock()
                 mock_process.communicate = AsyncMock(side_effect=asyncio.TimeoutError())
                 mock_subprocess.return_value = mock_process
-                
+
                 result = await monitor._probe_device_capabilities(device_path)
-                
+
                 assert not result.detected, f"Should fail for {scenario_name} timeout"
-                assert "timeout" in result.error.lower(), f"Error should mention timeout for {scenario_name}"
+                assert (
+                    "timeout" in result.error.lower()
+                ), f"Error should mention timeout for {scenario_name}"
                 assert result.structured_diagnostics is not None
-                assert 'timeout_threshold' in result.structured_diagnostics
+                assert "timeout_threshold" in result.structured_diagnostics
 
     @pytest.mark.asyncio
     async def test_subprocess_failure_handling(self, monitor):
         """Test subprocess failure handling with structured diagnostics."""
-        
+
         device_path = "/dev/video0"
-        
+
         # Test different subprocess failure modes
         failure_scenarios = [
             (1, b"", b"Device not found", "device_not_found"),
             (2, b"", b"Permission denied", "permission_denied"),
             (127, b"", b"Command not found", "command_not_found"),
         ]
-        
+
         for return_code, stdout, stderr, error_type in failure_scenarios:
-            with patch('asyncio.create_subprocess_exec') as mock_subprocess:
+            with patch("asyncio.create_subprocess_exec") as mock_subprocess:
                 mock_process = AsyncMock()
                 mock_process.communicate.return_value = (stdout, stderr)
                 mock_process.returncode = return_code
                 mock_subprocess.return_value = mock_process
-                
+
                 result = await monitor._probe_device_capabilities(device_path)
-                
+
                 # Should handle gracefully without crashing
                 assert result is not None
                 assert result.structured_diagnostics is not None
-                
+
                 # Error context should be captured
                 if return_code != 0:
                     assert not result.detected or result.error
 
     @pytest.mark.asyncio
     async def test_concurrent_capability_probes_handling(self, monitor):
         """Test handling of concurrent capability probe requests."""
-        
+
         device_paths = ["/dev/video0", "/dev/video1", "/dev/video2"]
-        
+
         # Mock successful probe responses
-        with patch.object(monitor, '_probe_device_info_robust') as mock_info, \
-             patch.object(monitor, '_probe_device_formats_robust') as mock_formats, \
-             patch.object(monitor, '_probe_device_framerates_robust') as mock_rates:
-            
+        with (
+            patch.object(monitor, "_probe_device_info_robust") as mock_info,
+            patch.object(monitor, "_probe_device_formats_robust") as mock_formats,
+            patch.object(monitor, "_probe_device_framerates_robust") as mock_rates,
+        ):
+
             mock_info.return_value = {"name": "Test Camera", "driver": "uvcvideo"}
             mock_formats.return_value = {"formats": [], "resolutions": ["1920x1080"]}
             mock_rates.return_value = ["30", "15"]
-            
+
             # Launch concurrent probes
             tasks = [
                 monitor._probe_device_capabilities(device_path)
                 for device_path in device_paths
             ]
-            
+
             results = await asyncio.gather(*tasks, return_exceptions=True)
-            
+
             # Verify all probes completed
             assert len(results) == len(device_paths)
-            
+
             # Verify no exceptions were raised
             for result in results:
-                assert not isinstance(result, Exception), f"Unexpected exception: {result}"
-                assert hasattr(result, 'detected')
+                assert not isinstance(
+                    result, Exception
+                ), f"Unexpected exception: {result}"
+                assert hasattr(result, "detected")
 
 
 class TestIntegrationAndLifecycle:
     """Integration tests for full monitor lifecycle and component coordination."""
 
     @pytest.fixture
     def monitor(self):
         """Create monitor for integration testing."""
         return HybridCameraMonitor(
-            device_range=[0, 1],
-            poll_interval=0.05,
-            enable_capability_detection=True
+            device_range=[0, 1], poll_interval=0.05, enable_capability_detection=True
         )
 
     @pytest.mark.asyncio
     async def test_monitor_full_lifecycle(self, monitor):
         """Test complete monitor lifecycle from startup to shutdown."""
-        
+
         # Verify initial state
         assert not monitor.is_running
         initial_stats = monitor.get_monitor_stats()
-        assert initial_stats['running'] is False
-        assert initial_stats['active_tasks'] == 0
-        
+        assert initial_stats["running"] is False
+        assert initial_stats["active_tasks"] == 0
+
         # Start monitor
-        with patch('src.camera_discovery.hybrid_monitor.HAS_PYUDEV', False):  # Disable udev for testing
+        with patch(
+            "src.camera_discovery.hybrid_monitor.HAS_PYUDEV", False
+        ):  # Disable udev for testing
             await monitor.start()
-            
+
             assert monitor.is_running
             running_stats = monitor.get_monitor_stats()
-            assert running_stats['running'] is True
-            assert running_stats['active_tasks'] > 0
-            
+            assert running_stats["running"] is True
+            assert running_stats["active_tasks"] > 0
+
             # Let monitor run briefly
             await asyncio.sleep(0.1)
-            
+
             # Verify some activity occurred
             activity_stats = monitor.get_monitor_stats()
-            assert activity_stats['polling_cycles'] > 0
-            
+            assert activity_stats["polling_cycles"] > 0
+
             # Stop monitor
             await monitor.stop()
-            
+
             assert not monitor.is_running
             final_stats = monitor.get_monitor_stats()
-            assert final_stats['running'] is False
-            assert final_stats['active_tasks'] == 0
+            assert final_stats["running"] is False
+            assert final_stats["active_tasks"] == 0
 
     @pytest.mark.asyncio
     async def test_end_to_end_device_workflow(self, monitor):
         """Test end-to-end device detection and capability integration workflow."""
-        
+
         monitor._set_test_mode(True)
         device_path = "/dev/video0"
-        
+
         # Mock capability detection
         mock_capability = CapabilityDetectionResult(
             device_path=device_path,
             detected=True,
             accessible=True,
             resolutions=["1920x1080", "1280x720"],
             frame_rates=["30", "25", "15"],
-            formats=[{"code": "YUYV", "description": "YUYV 4:2:2"}]
-        )
-        
-        with patch.object(monitor, '_probe_device_capabilities', return_value=mock_capability):
+            formats=[{"code": "YUYV", "description": "YUYV 4:2:2"}],
+        )
+
+        with patch.object(
+            monitor, "_probe_device_capabilities", return_value=mock_capability
+        ):
             # Create event handler to capture events
             captured_events = []
-            
+
             def capture_event(event_data):
                 captured_events.append(event_data)
-            
+
             monitor.add_event_callback(capture_event)
-            
+
             # Simulate device connection
             await monitor._inject_test_udev_event(device_path, "add")
-            
+
             # Verify event was captured
             assert len(captured_events) > 0
             connect_event = captured_events[-1]
             assert connect_event.event_type == CameraEvent.CONNECTED
             assert connect_event.device_path == device_path
-            
+
             # Verify capability metadata is available
             metadata = monitor.get_effective_capability_metadata(device_path)
             assert metadata["resolution"] == "1920x1080"
             assert metadata["fps"] == 30
             assert metadata["validation_status"] in ["provisional", "confirmed"]
-            
+
             # Simulate device disconnection
             await monitor._inject_test_udev_event(device_path, "remove")
-            
+
             # Verify disconnect event
             disconnect_event = captured_events[-1]
             assert disconnect_event.event_type == CameraEvent.DISCONNECTED
-            
+
         monitor._set_test_mode(False)
 
 
 # Utility functions for test setup and validation
+
 
 def create_mock_v4l2_output(formats=None, resolutions=None, frame_rates=None):
     """Create mock v4l2-ctl output for testing."""
-    output_lines = ["Driver info:", "Card type    : Test Camera", "Driver name  : uvcvideo"]
-    
+    output_lines = [
+        "Driver info:",
+        "Card type    : Test Camera",
+        "Driver name  : uvcvideo",
+    ]
+
     if formats:
         output_lines.append("Supported formats:")
         for i, fmt in enumerate(formats):
             output_lines.append(f"[{i}]: '{fmt}' (Test Format)")
-    
+
     if resolutions:
         output_lines.append("Supported resolutions:")
         for res in resolutions:
             output_lines.append(f"Size: Discrete {res}")
-    
+
     if frame_rates:
         output_lines.append("Supported frame rates:")
         for rate in frame_rates:
             output_lines.append(f"{rate} fps")
-    
+
     return "\n".join(output_lines)
 
 
-def assert_capability_consistency(result1: CapabilityDetectionResult, result2: CapabilityDetectionResult):
+def assert_capability_consistency(
+    result1: CapabilityDetectionResult, result2: CapabilityDetectionResult
+):
     """Assert that two capability detection results are consistent."""
     assert result1.detected == result2.detected
     if result1.detected and result2.detected:
         # Allow some variation but check core consistency
         assert bool(result1.resolutions) == bool(result2.resolutions)
@@ -672,6 +727,6 @@
 pytest_plugins = ["pytest_asyncio"]
 
 
 if __name__ == "__main__":
     # Allow running tests directly
-    pytest.main([__file__, "-v", "--tb=short"])
\ No newline at end of file
+    pytest.main([__file__, "-v", "--tb=short"])
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_service/test_config_manager.py	2025-08-04 02:27:19.626254+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_service/test_config_manager.py	2025-08-04 15:35:25.208664+00:00
@@ -10,43 +10,50 @@
 import pytest
 from pathlib import Path
 from unittest.mock import Mock, patch, mock_open
 
 from camera_service.config import (
-    ConfigManager, load_config, get_config_manager,
-    Config, ServerConfig, MediaMTXConfig, CameraConfig,
-    LoggingConfig, RecordingConfig, SnapshotConfig
+    ConfigManager,
+    load_config,
+    get_config_manager,
+    Config,
+    ServerConfig,
+    MediaMTXConfig,
+    CameraConfig,
+    LoggingConfig,
+    RecordingConfig,
+    SnapshotConfig,
 )
 
 
 class TestConfigDataClasses:
     """Test configuration data class defaults and validation."""
-    
+
     def test_server_config_defaults(self):
         """Test ServerConfig default values."""
         config = ServerConfig()
         assert config.host == "0.0.0.0"
         assert config.port == 8002
         assert config.websocket_path == "/ws"
         assert config.max_connections == 100
-    
+
     def test_camera_config_device_range_default(self):
         """Test CameraConfig device_range default initialization."""
         config = CameraConfig()
         assert config.device_range == list(range(10))
         assert config.poll_interval == 0.1
         assert config.enable_capability_detection is True
 
 
 class TestConfigManager:
     """Test ConfigManager configuration loading and management."""
-    
+
     @pytest.fixture
     def config_manager(self):
         """Create a fresh ConfigManager instance for testing."""
         return ConfigManager()
-    
+
     @pytest.fixture
     def sample_yaml_config(self):
         """Sample YAML configuration for testing."""
         return """
 server:
@@ -64,97 +71,107 @@
 
 logging:
   level: "DEBUG"
   file_enabled: true
 """
-    
+
     def test_config_manager_initialization(self, config_manager):
         """Test ConfigManager initializes with proper defaults."""
         assert config_manager._config is None
         assert config_manager._config_path is None
         assert config_manager._update_callbacks == []
         assert config_manager._observer is None
-    
+
     def test_load_config_from_file(self, config_manager, sample_yaml_config):
         """Test loading configuration from YAML file."""
         # TODO: HIGH: Test YAML config file loading [Story:S14]
         # TODO: HIGH: Mock file system and YAML parsing [Story:S14]
         # TODO: HIGH: Verify Config object creation with proper values [Story:S14]
-        with patch('builtins.open', mock_open(read_data=sample_yaml_config)), \
-             patch('os.path.exists', return_value=True):
-            
-            config = config_manager.load_config('test_config.yaml')
-            
+        with (
+            patch("builtins.open", mock_open(read_data=sample_yaml_config)),
+            patch("os.path.exists", return_value=True),
+        ):
+
+            config = config_manager.load_config("test_config.yaml")
+
             assert isinstance(config, Config)
             assert config.server.host == "127.0.0.1"
             assert config.server.port == 8003
             assert config.mediamtx.api_port == 9998
             assert config.camera.poll_interval == 0.2
             assert config.logging.level == "DEBUG"
-    
+
     def test_load_config_file_not_found(self, config_manager):
         """Test configuration loading when no file found uses defaults."""
         # TODO: HIGH: Test fallback to defaults when no config file exists [Story:S14]
         # TODO: HIGH: Mock _find_config_file to raise FileNotFoundError [Story:S14]
-        with patch.object(config_manager, '_find_config_file', side_effect=FileNotFoundError("No config found")):
-            
+        with patch.object(
+            config_manager,
+            "_find_config_file",
+            side_effect=FileNotFoundError("No config found"),
+        ):
+
             # Should not raise exception, should use defaults
             config = config_manager.load_config()
             assert isinstance(config, Config)
             assert config.server.port == 8002  # Default value
-    
+
     def test_load_config_malformed_yaml(self, config_manager):
         """Test configuration loading with malformed YAML uses defaults."""
         # TODO: HIGH: Test malformed YAML handling with fallback [Story:S14]
         # TODO: HIGH: Mock file with invalid YAML content [Story:S14]
         malformed_yaml = "server:\n  host: [\n  invalid yaml"
-        
-        with patch('builtins.open', mock_open(read_data=malformed_yaml)), \
-             patch('os.path.exists', return_value=True):
-            
+
+        with (
+            patch("builtins.open", mock_open(read_data=malformed_yaml)),
+            patch("os.path.exists", return_value=True),
+        ):
+
             # Should not raise exception, should use defaults and log error
-            config = config_manager.load_config('malformed.yaml')
+            config = config_manager.load_config("malformed.yaml")
             assert isinstance(config, Config)
             assert config.server.port == 8002  # Default value
-    
+
     def test_environment_variable_overrides(self, config_manager):
         """Test environment variable overrides for configuration."""
         # TODO: HIGH: Test environment variable override functionality [Story:S14]
         # TODO: HIGH: Mock environment variables and verify override [Story:S14]
         env_vars = {
-            'CAMERA_SERVICE_SERVER_HOST': '192.168.1.100',
-            'CAMERA_SERVICE_SERVER_PORT': '8004',
-            'CAMERA_SERVICE_CAMERA_POLL_INTERVAL': '0.5'
+            "CAMERA_SERVICE_SERVER_HOST": "192.168.1.100",
+            "CAMERA_SERVICE_SERVER_PORT": "8004",
+            "CAMERA_SERVICE_CAMERA_POLL_INTERVAL": "0.5",
         }
-        
-        with patch('builtins.open', mock_open(read_data="{}")), \
-             patch('os.path.exists', return_value=True), \
-             patch.dict(os.environ, env_vars):
-            
-            config = config_manager.load_config('test_config.yaml')
-            
-            assert config.server.host == '192.168.1.100'
+
+        with (
+            patch("builtins.open", mock_open(read_data="{}")),
+            patch("os.path.exists", return_value=True),
+            patch.dict(os.environ, env_vars),
+        ):
+
+            config = config_manager.load_config("test_config.yaml")
+
+            assert config.server.host == "192.168.1.100"
             assert config.server.port == 8004
             assert config.camera.poll_interval == 0.5
-    
+
     def test_invalid_environment_variable_override(self, config_manager):
         """Test handling of invalid environment variable values."""
         # TODO: HIGH: Test invalid environment variable handling with fallback [Story:S14]
         # TODO: HIGH: Verify invalid values are logged but service continues [Story:S14]
-        env_vars = {
-            'CAMERA_SERVICE_SERVER_PORT': 'invalid_port'
-        }
-        
-        with patch('builtins.open', mock_open(read_data="{}")), \
-             patch('os.path.exists', return_value=True), \
-             patch.dict(os.environ, env_vars):
-            
+        env_vars = {"CAMERA_SERVICE_SERVER_PORT": "invalid_port"}
+
+        with (
+            patch("builtins.open", mock_open(read_data="{}")),
+            patch("os.path.exists", return_value=True),
+            patch.dict(os.environ, env_vars),
+        ):
+
             # Should not crash, should use default and log error
-            config = config_manager.load_config('test_config.yaml')
+            config = config_manager.load_config("test_config.yaml")
             assert isinstance(config, Config)
             assert config.server.port == 8002  # Default value
-    
+
     def test_configuration_validation(self, config_manager):
         """Test configuration validation with invalid values."""
         # TODO: HIGH: Test configuration validation [Story:S14]
         # TODO: HIGH: Test port range validation [Story:S14]
         # TODO: HIGH: Test logging level validation [Story:S14]
@@ -162,119 +179,126 @@
 server:
   port: 70000  # Invalid port
 logging:
   level: "INVALID_LEVEL"  # Invalid log level
 """
-        
-        with patch('builtins.open', mock_open(read_data=invalid_config)), \
-             patch('os.path.exists', return_value=True):
-            
+
+        with (
+            patch("builtins.open", mock_open(read_data=invalid_config)),
+            patch("os.path.exists", return_value=True),
+        ):
+
             with pytest.raises(ValueError):
-                config_manager.load_config('invalid_config.yaml')
-    
+                config_manager.load_config("invalid_config.yaml")
+
     def test_config_update_runtime(self, config_manager, sample_yaml_config):
         """Test runtime configuration updates."""
         # TODO: MEDIUM: Test runtime config updates [Story:S14]
         # TODO: MEDIUM: Mock initial config load then update [Story:S14]
         # TODO: MEDIUM: Verify update callbacks are triggered [Story:S14]
-        with patch('builtins.open', mock_open(read_data=sample_yaml_config)), \
-             patch('os.path.exists', return_value=True):
-            
-            config = config_manager.load_config('test_config.yaml')
-            
-            updates = {
-                'server': {'port': 8005},
-                'camera': {'poll_interval': 0.3}
-            }
-            
+        with (
+            patch("builtins.open", mock_open(read_data=sample_yaml_config)),
+            patch("os.path.exists", return_value=True),
+        ):
+
+            config = config_manager.load_config("test_config.yaml")
+
+            updates = {"server": {"port": 8005}, "camera": {"poll_interval": 0.3}}
+
             updated_config = config_manager.update_config(updates)
-            
+
             assert updated_config.server.port == 8005
             assert updated_config.camera.poll_interval == 0.3
-    
+
     def test_config_update_validation_failure(self, config_manager, sample_yaml_config):
         """Test runtime config update with invalid values."""
         # TODO: MEDIUM: Test update validation failure with rollback [Story:S14]
         # TODO: MEDIUM: Verify rollback to previous config [Story:S14]
-        with patch('builtins.open', mock_open(read_data=sample_yaml_config)), \
-             patch('os.path.exists', return_value=True):
-            
-            original_config = config_manager.load_config('test_config.yaml')
+        with (
+            patch("builtins.open", mock_open(read_data=sample_yaml_config)),
+            patch("os.path.exists", return_value=True),
+        ):
+
+            original_config = config_manager.load_config("test_config.yaml")
             original_port = original_config.server.port
-            
-            invalid_updates = {
-                'server': {'port': 100000}  # Invalid port
-            }
-            
+
+            invalid_updates = {"server": {"port": 100000}}  # Invalid port
+
             with pytest.raises(ValueError):
                 config_manager.update_config(invalid_updates)
-            
+
             # Should rollback to original config
             current_config = config_manager.get_config()
             assert current_config.server.port == original_port
-    
+
     def test_hot_reload_start_stop(self, config_manager):
         """Test hot reload functionality start and stop."""
         # TODO: MEDIUM: Test hot reload start/stop [Story:S14]
         # TODO: MEDIUM: Mock watchdog Observer [Story:S14]
         # Only test if watchdog is available
         pytest.importorskip("watchdog")
-        
-        with patch('camera_service.config.Observer') as mock_observer:
+
+        with patch("camera_service.config.Observer") as mock_observer:
             mock_observer_instance = Mock()
             mock_observer.return_value = mock_observer_instance
-            
+
             config_manager.start_hot_reload()
             mock_observer_instance.start.assert_called_once()
-            
+
             config_manager.stop_hot_reload()
             mock_observer_instance.stop.assert_called_once()
-    
+
     def test_hot_reload_without_watchdog(self, config_manager):
         """Test hot reload when watchdog is not available."""
         # TODO: MEDIUM: Test hot reload graceful degradation [Story:S14]
         # TODO: MEDIUM: Mock HAS_WATCHDOG to False [Story:S14]
-        with patch('camera_service.config.HAS_WATCHDOG', False):
+        with patch("camera_service.config.HAS_WATCHDOG", False):
             # Should log warning but not crash
             config_manager.start_hot_reload()
             # No observer should be created
             assert config_manager._observer is None
-    
-    def test_hot_reload_file_change_simulation(self, config_manager, sample_yaml_config):
+
+    def test_hot_reload_file_change_simulation(
+        self, config_manager, sample_yaml_config
+    ):
         """Test hot reload file change detection and reload."""
         # TODO: MEDIUM: Test hot reload file change simulation [Story:S14]
         # TODO: MEDIUM: Mock file system events and verify reload [Story:S14]
         pytest.importorskip("watchdog")
-        
-        with patch('builtins.open', mock_open(read_data=sample_yaml_config)), \
-             patch('os.path.exists', return_value=True), \
-             patch('camera_service.config.Observer') as mock_observer:
-            
+
+        with (
+            patch("builtins.open", mock_open(read_data=sample_yaml_config)),
+            patch("os.path.exists", return_value=True),
+            patch("camera_service.config.Observer") as mock_observer,
+        ):
+
             # Load initial config
-            config_manager.load_config('test_config.yaml')
-            
+            config_manager.load_config("test_config.yaml")
+
             # Start hot reload
             config_manager.start_hot_reload()
-            
+
             # Verify observer was configured
             mock_observer.assert_called_once()
-    
+
     def test_default_config_fallback(self, config_manager):
         """Test fallback to default configuration when all else fails."""
         # TODO: HIGH: Test complete fallback to defaults [Story:S14]
         # TODO: HIGH: Mock all config sources to fail [Story:S14]
-        with patch.object(config_manager, '_find_config_file', side_effect=FileNotFoundError()):
-            
+        with patch.object(
+            config_manager, "_find_config_file", side_effect=FileNotFoundError()
+        ):
+
             config = config_manager.load_config()
-            
+
             # Should get default configuration
             assert isinstance(config, Config)
             assert config.server.host == "0.0.0.0"
             assert config.server.port == 8002
             assert config.mediamtx.api_port == 9997
             assert config.camera.poll_interval == 0.1
-    
+
     def test_comprehensive_validation_error_accumulation(self, config_manager):
         """Test that validation accumulates multiple errors."""
         # TODO: HIGH: Test validation error accumulation [Story:S14]
         # TODO: HIGH: Verify multiple validation errors are collected [Story:S14]
         invalid_config = """
@@ -286,51 +310,53 @@
 logging:
   level: "INVALID"  # Invalid level
 snapshots:
   quality: 150  # Invalid quality
 """
-        
-        with patch('builtins.open', mock_open(read_data=invalid_config)), \
-             patch('os.path.exists', return_value=True):
-            
+
+        with (
+            patch("builtins.open", mock_open(read_data=invalid_config)),
+            patch("os.path.exists", return_value=True),
+        ):
+
             with pytest.raises(ValueError) as exc_info:
-                config_manager.load_config('invalid_config.yaml')
-            
+                config_manager.load_config("invalid_config.yaml")
+
             # Should contain multiple validation errors
             error_message = str(exc_info.value)
             assert "port" in error_message
             assert "level" in error_message or "quality" in error_message
 
 
 class TestConfigurationIntegration:
     """Integration tests for configuration loading and management."""
-    
+
     def test_load_config_function(self):
         """Test module-level load_config function."""
         # TODO: MEDIUM: Test module-level config loading [Story:S14]
         # TODO: MEDIUM: Mock file system for function test [Story:S14]
-        with patch('camera_service.config._config_manager') as mock_manager:
+        with patch("camera_service.config._config_manager") as mock_manager:
             mock_config = Mock()
             mock_manager.load_config.return_value = mock_config
-            
-            result = load_config('test.yaml')
-            
-            mock_manager.load_config.assert_called_once_with('test.yaml')
+
+            result = load_config("test.yaml")
+
+            mock_manager.load_config.assert_called_once_with("test.yaml")
             assert result == mock_config
-    
+
     def test_get_config_manager_function(self):
         """Test module-level get_config_manager function."""
         # TODO: LOW: Test config manager accessor [Story:S14]
         manager = get_config_manager()
         assert isinstance(manager, ConfigManager)
-    
+
     def test_get_current_config_function(self):
         """Test module-level get_current_config function."""
         # TODO: LOW: Test current config accessor [Story:S14]
-        with patch('camera_service.config._config_manager') as mock_manager:
+        with patch("camera_service.config._config_manager") as mock_manager:
             mock_config = Mock()
             mock_manager.get_config.return_value = mock_config
-            
+
             result = get_current_config()
-            
+
             mock_manager.get_config.assert_called_once()
-            assert result == mock_config
\ No newline at end of file
+            assert result == mock_config
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_controller_configuration.py	2025-08-03 19:27:38.812474+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_controller_configuration.py	2025-08-04 15:35:25.476503+00:00
@@ -26,11 +26,11 @@
             rtsp_port=8554,
             webrtc_port=8889,
             hls_port=8888,
             config_path="/tmp/test_config.yml",
             recordings_path="/tmp/recordings",
-            snapshots_path="/tmp/snapshots"
+            snapshots_path="/tmp/snapshots",
         )
         # Mock session
         controller._session = Mock()
         return controller
 
@@ -46,16 +46,16 @@
     async def test_configuration_validation_unknown_keys(self, controller):
         """Test validation rejects unknown configuration keys."""
         invalid_config = {
             "logLevel": "info",  # Valid
             "unknownKey": "value",  # Invalid
-            "anotherUnknown": 123  # Invalid
+            "anotherUnknown": 123,  # Invalid
         }
-        
+
         with pytest.raises(ValueError) as exc_info:
             await controller.update_configuration(invalid_config)
-        
+
         # Verify error message lists unknown keys
         error_msg = str(exc_info.value)
         assert "Unknown configuration keys" in error_msg
         assert "unknownKey" in error_msg
         assert "anotherUnknown" in error_msg
@@ -63,50 +63,57 @@
     @pytest.mark.asyncio
     async def test_configuration_validation_type_errors(self, controller):
         """Test validation catches type mismatches."""
         invalid_configs = [
             {"logLevel": 123},  # Should be string
-            {"api": "true"},   # Should be boolean
+            {"api": "true"},  # Should be boolean
             {"readTimeout": "not_a_number"},  # Should be int/str number
-            {"readBufferCount": "invalid"}  # Should be int
+            {"readBufferCount": "invalid"},  # Should be int
         ]
-        
+
         for invalid_config in invalid_configs:
             with pytest.raises(ValueError, match="Invalid type"):
                 await controller.update_configuration(invalid_config)
 
     @pytest.mark.asyncio
     async def test_configuration_validation_value_constraints(self, controller):
         """Test validation enforces value constraints (min/max, allowed values)."""
         invalid_configs = [
             {"logLevel": "invalid_level"},  # Not in allowed values
             {"readTimeout": 0},  # Below minimum
-            {"readTimeout": 500},  # Above maximum  
+            {"readTimeout": 500},  # Above maximum
             {"readBufferCount": 0},  # Below minimum
             {"readBufferCount": 10000},  # Above maximum
-            {"udpMaxPayloadSize": 500}  # Below minimum
+            {"udpMaxPayloadSize": 500},  # Below minimum
         ]
-        
+
         for invalid_config in invalid_configs:
             with pytest.raises(ValueError) as exc_info:
                 await controller.update_configuration(invalid_config)
-            
+
             # Verify specific constraint violation is mentioned
             error_msg = str(exc_info.value)
-            assert any(keyword in error_msg for keyword in 
-                      ["Invalid value", "too small", "too large", "allowed values"])
+            assert any(
+                keyword in error_msg
+                for keyword in [
+                    "Invalid value",
+                    "too small",
+                    "too large",
+                    "allowed values",
+                ]
+            )
 
     @pytest.mark.asyncio
     async def test_configuration_validation_pattern_matching(self, controller):
         """Test validation enforces string patterns (e.g., IP addresses)."""
         invalid_patterns = [
             {"apiAddress": "invalid_ip"},
             {"metricsAddress": "not.an.ip.address"},
             {"rtspAddress": "999.999.999.999"},  # Invalid IP
-            {"webrtcAddress": "localhost:abc"}  # Invalid port format
+            {"webrtcAddress": "localhost:abc"},  # Invalid port format
         ]
-        
+
         for invalid_config in invalid_patterns:
             with pytest.raises(ValueError, match="Invalid format"):
                 await controller.update_configuration(invalid_config)
 
     @pytest.mark.asyncio
@@ -115,75 +122,74 @@
         # Configuration with multiple validation errors
         invalid_config = {
             "logLevel": "invalid_level",  # Bad value
             "api": "not_boolean",  # Bad type
             "readTimeout": -1,  # Below minimum
-            "unknownKey": "value"  # Unknown key
+            "unknownKey": "value",  # Unknown key
         }
-        
+
         with pytest.raises(ValueError) as exc_info:
             await controller.update_configuration(invalid_config)
-        
+
         # Verify multiple errors are reported
         error_msg = str(exc_info.value)
         # Should mention unknown keys error
         assert "Unknown configuration keys" in error_msg
-        
+
         # Try validation-only errors (excluding unknown keys)
         validation_only_config = {
             "logLevel": "invalid_level",
-            "api": "not_boolean", 
-            "readTimeout": -1
+            "api": "not_boolean",
+            "readTimeout": -1,
         }
-        
+
         with pytest.raises(ValueError) as exc_info:
             await controller.update_configuration(validation_only_config)
-        
+
         error_msg = str(exc_info.value)
         # Should accumulate multiple validation errors
         error_count = error_msg.count(";")  # Errors separated by semicolons
         assert error_count >= 2  # At least 2 validation errors
 
     @pytest.mark.asyncio
     async def test_configuration_update_api_failure_safe_fallback(self, controller):
         """Test safe fallback behavior when MediaMTX API fails during update."""
         # Valid configuration
-        valid_config = {
-            "logLevel": "debug",
-            "api": True
-        }
-        
+        valid_config = {"logLevel": "debug", "api": True}
+
         # Mock API failure
         error_response = self._mock_response(500, text_data="Internal Server Error")
         controller._session.post = AsyncMock(return_value=error_response)
-        
+
         # Should raise ValueError (not crash system)
         with pytest.raises(ValueError, match="Failed to update configuration"):
             await controller.update_configuration(valid_config)
-        
+
         # Controller should remain in usable state
         assert controller._session is not None
 
     @pytest.mark.asyncio
     async def test_configuration_update_network_error_handling(self, controller):
         """Test network error handling during configuration update."""
         valid_config = {"logLevel": "info"}
-        
+
         # Mock network error
-        controller._session.post = AsyncMock(side_effect=aiohttp.ClientError("Connection refused"))
-        
+        controller._session.post = AsyncMock(
+            side_effect=aiohttp.ClientError("Connection refused")
+        )
+
         with pytest.raises(ConnectionError, match="MediaMTX unreachable"):
             await controller.update_configuration(valid_config)
 
     @pytest.mark.asyncio
     async def test_configuration_validation_empty_updates(self, controller):
         """Test validation handles empty or None configuration updates."""
         # Empty config
         with pytest.raises(ValueError, match="Configuration updates are required"):
             await controller.update_configuration({})
-        
-        # None config  
+
+        # None config
         with pytest.raises(ValueError, match="Configuration updates are required"):
             await controller.update_configuration(None)
 
     @pytest.mark.asyncio
     async def test_configuration_validation_valid_config_success(self, controller):
@@ -191,49 +197,71 @@
         valid_config = {
             "logLevel": "debug",
             "api": True,
             "readTimeout": 30,
             "readBufferCount": 512,
-            "apiAddress": "127.0.0.1:9997"
+            "apiAddress": "127.0.0.1:9997",
         }
-        
+
         # Mock successful API response
         success_response = self._mock_response(200)
         controller._session.post = AsyncMock(return_value=success_response)
-        
+
         result = await controller.update_configuration(valid_config)
-        
+
         assert result is True
         # Verify API was called with correct config
         controller._session.post.assert_called_once()
         call_args = controller._session.post.call_args
-        assert call_args.kwargs['json'] == valid_config
+        assert call_args.kwargs["json"] == valid_config
 
     @pytest.mark.asyncio
     async def test_configuration_update_without_session(self, controller):
         """Test configuration update fails gracefully when controller not started."""
         controller._session = None
-        
+
         with pytest.raises(ConnectionError, match="MediaMTX controller not started"):
             await controller.update_configuration({"logLevel": "info"})
 
     def test_configuration_validation_schema_completeness(self, controller):
         """Test validation schema covers all expected MediaMTX configuration options."""
         # Test that validation schema includes key MediaMTX settings
         valid_config_keys = [
-            "logLevel", "logDestinations", "readTimeout", "writeTimeout",
-            "readBufferCount", "udpMaxPayloadSize", "runOnConnect", 
-            "runOnConnectRestart", "api", "apiAddress", "metrics",
-            "metricsAddress", "pprof", "pprofAddress", "rtsp", "rtspAddress",
-            "rtspsAddress", "rtmp", "rtmpAddress", "rtmps", "rtmpsAddress",
-            "hls", "hlsAddress", "hlsAllowOrigin", "webrtc", "webrtcAddress"
+            "logLevel",
+            "logDestinations",
+            "readTimeout",
+            "writeTimeout",
+            "readBufferCount",
+            "udpMaxPayloadSize",
+            "runOnConnect",
+            "runOnConnectRestart",
+            "api",
+            "apiAddress",
+            "metrics",
+            "metricsAddress",
+            "pprof",
+            "pprofAddress",
+            "rtsp",
+            "rtspAddress",
+            "rtspsAddress",
+            "rtmp",
+            "rtmpAddress",
+            "rtmps",
+            "rtmpsAddress",
+            "hls",
+            "hlsAddress",
+            "hlsAllowOrigin",
+            "webrtc",
+            "webrtcAddress",
         ]
-        
+
         # Each should be either accepted or rejected with specific error
         for key in valid_config_keys:
-            test_config = {key: "test_value"}  # May be wrong type, but key should be recognized
-            
+            test_config = {
+                key: "test_value"
+            }  # May be wrong type, but key should be recognized
+
             try:
                 # This will likely fail due to wrong type/value, but should not fail due to unknown key
                 asyncio.run(controller.update_configuration(test_config))
             except ValueError as e:
                 # Should be type/value error, not unknown key error
@@ -244,21 +272,24 @@
 
     @pytest.mark.asyncio
     async def test_configuration_validation_correlation_id_logging(self, controller):
         """Test configuration validation includes correlation IDs in logging."""
         correlation_ids = []
-        
+
         def mock_set_correlation_id(cid):
             correlation_ids.append(cid)
-        
-        with patch('src.mediamtx_wrapper.controller.set_correlation_id', side_effect=mock_set_correlation_id):
+
+        with patch(
+            "src.mediamtx_wrapper.controller.set_correlation_id",
+            side_effect=mock_set_correlation_id,
+        ):
             # Mock successful response
             success_response = self._mock_response(200)
             controller._session.post = AsyncMock(return_value=success_response)
-            
+
             await controller.update_configuration({"logLevel": "info"})
-        
+
         # Verify correlation ID was set
         assert len(correlation_ids) > 0
         assert all(isinstance(cid, str) and len(cid) > 0 for cid in correlation_ids)
 
 
@@ -267,6 +298,6 @@
 # - Test validation of different parameter types and constraints
 # - Test error accumulation and reporting without system crashes
 # - Verify correlation IDs are set for configuration operations
 # - Test both successful and failed configuration updates
 # - Verify safe fallback behavior on API/network failures
-# - Test validation schema completeness for MediaMTX options
\ No newline at end of file
+# - Test validation schema completeness for MediaMTX options
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_service/test_logging_config.py	2025-08-04 02:43:33.150609+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_service/test_logging_config.py	2025-08-04 15:35:25.502537+00:00
@@ -21,428 +21,471 @@
     ConsoleFormatter,
     setup_logging,
     get_correlation_filter,
     set_correlation_id,
     get_correlation_id,
-    _parse_file_size
+    _parse_file_size,
 )
 
 
 class TestCorrelationIdFilter:
     """Test CorrelationIdFilter for thread-local correlation tracking."""
-    
+
     def test_correlation_filter_basic_functionality(self):
         """Test correlation filter sets and retrieves correlation IDs."""
         # TODO: HIGH: Test correlation filter basic operations [Story:S14]
         correlation_filter = CorrelationIdFilter()
-        
+
         # Initially no correlation ID
         assert correlation_filter.get_correlation_id() is None
-        
+
         # Set correlation ID
-        correlation_filter.set_correlation_id('test-123')
-        assert correlation_filter.get_correlation_id() == 'test-123'
-        
+        correlation_filter.set_correlation_id("test-123")
+        assert correlation_filter.get_correlation_id() == "test-123"
+
         # Filter should add correlation ID to record
         record = logging.LogRecord(
-            name='test', level=logging.INFO, pathname='',
-            lineno=0, msg='Test message', args=(), exc_info=None
-        )
-        
+            name="test",
+            level=logging.INFO,
+            pathname="",
+            lineno=0,
+            msg="Test message",
+            args=(),
+            exc_info=None,
+        )
+
         assert correlation_filter.filter(record) is True
-        assert hasattr(record, 'correlation_id')
-        assert record.correlation_id == 'test-123'
-    
+        assert hasattr(record, "correlation_id")
+        assert record.correlation_id == "test-123"
+
     def test_correlation_filter_thread_isolation(self):
         """Test correlation IDs are isolated between threads."""
         # TODO: MEDIUM: Test thread isolation for correlation IDs [Story:S14]
         import threading
-        
+
         correlation_filter = CorrelationIdFilter()
         results = {}
-        
+
         def thread_func(thread_id, expected_id):
             correlation_filter.set_correlation_id(expected_id)
             # Small delay to ensure threads overlap
             import time
+
             time.sleep(0.01)
             results[thread_id] = correlation_filter.get_correlation_id()
-        
+
         # Start multiple threads with different correlation IDs
         threads = []
         for i in range(3):
-            thread = threading.Thread(target=thread_func, args=(i, f'thread-{i}'))
+            thread = threading.Thread(target=thread_func, args=(i, f"thread-{i}"))
             threads.append(thread)
             thread.start()
-        
+
         # Wait for all threads to complete
         for thread in threads:
             thread.join()
-        
+
         # Each thread should have its own correlation ID
-        assert results[0] == 'thread-0'
-        assert results[1] == 'thread-1' 
-        assert results[2] == 'thread-2'
+        assert results[0] == "thread-0"
+        assert results[1] == "thread-1"
+        assert results[2] == "thread-2"
 
 
 class TestJsonFormatter:
     """Test JsonFormatter for structured production logging."""
-    
+
     def test_json_formatter_basic_record(self):
         """Test JSON formatter with basic log record."""
         # TODO: HIGH: Test JSON formatter basic output structure [Story:S14]
         formatter = JsonFormatter()
         record = logging.LogRecord(
-            name='test.module', level=logging.INFO, pathname='test.py',
-            lineno=42, msg='Test message', args=(), exc_info=None
-        )
-        record.module = 'test_module'
-        record.funcName = 'test_function'
-        
+            name="test.module",
+            level=logging.INFO,
+            pathname="test.py",
+            lineno=42,
+            msg="Test message",
+            args=(),
+            exc_info=None,
+        )
+        record.module = "test_module"
+        record.funcName = "test_function"
+
         result = formatter.format(record)
         log_data = json.loads(result)
-        
-        assert log_data['level'] == 'INFO'
-        assert log_data['logger'] == 'test.module'
-        assert log_data['message'] == 'Test message'
-        assert log_data['module'] == 'test_module'
-        assert log_data['function'] == 'test_function'
-        assert log_data['line'] == 42
-        assert 'timestamp' in log_data
-    
+
+        assert log_data["level"] == "INFO"
+        assert log_data["logger"] == "test.module"
+        assert log_data["message"] == "Test message"
+        assert log_data["module"] == "test_module"
+        assert log_data["function"] == "test_function"
+        assert log_data["line"] == 42
+        assert "timestamp" in log_data
+
     def test_json_formatter_with_correlation_id(self):
         """Test JSON formatter includes correlation ID."""
         # TODO: HIGH: Test JSON formatter correlation ID inclusion [Story:S14]
         formatter = JsonFormatter()
         record = logging.LogRecord(
-            name='test', level=logging.INFO, pathname='',
-            lineno=0, msg='Test message', args=(), exc_info=None
-        )
-        record.correlation_id = 'test-correlation-123'
-        
+            name="test",
+            level=logging.INFO,
+            pathname="",
+            lineno=0,
+            msg="Test message",
+            args=(),
+            exc_info=None,
+        )
+        record.correlation_id = "test-correlation-123"
+
         result = formatter.format(record)
         log_data = json.loads(result)
-        
-        assert log_data['correlation_id'] == 'test-correlation-123'
-    
+
+        assert log_data["correlation_id"] == "test-correlation-123"
+
     def test_json_formatter_with_exception(self):
         """Test JSON formatter with exception information."""
         # TODO: HIGH: Test JSON formatter exception handling [Story:S14]
         formatter = JsonFormatter()
-        
+
         try:
             raise ValueError("Test exception")
         except ValueError:
             exc_info = True
-        
-        record = logging.LogRecord(
-            name='test', level=logging.ERROR, pathname='', lineno=0,
-            msg='Error occurred', args=(), exc_info=exc_info
-        )
-        
+
+        record = logging.LogRecord(
+            name="test",
+            level=logging.ERROR,
+            pathname="",
+            lineno=0,
+            msg="Error occurred",
+            args=(),
+            exc_info=exc_info,
+        )
+
         result = formatter.format(record)
         log_data = json.loads(result)
-        
-        assert 'exception' in log_data
-        assert 'Test exception' in log_data['exception']
-    
+
+        assert "exception" in log_data
+        assert "Test exception" in log_data["exception"]
+
     def test_json_formatter_extra_fields(self):
         """Test JSON formatter includes extra fields from log record."""
         # TODO: MEDIUM: Test JSON formatter extra fields handling [Story:S14]
         formatter = JsonFormatter()
         record = logging.LogRecord(
-            name='test', level=logging.INFO, pathname='', lineno=0,
-            msg='Test message', args=(), exc_info=None
-        )
-        record.custom_field = 'custom_value'
+            name="test",
+            level=logging.INFO,
+            pathname="",
+            lineno=0,
+            msg="Test message",
+            args=(),
+            exc_info=None,
+        )
+        record.custom_field = "custom_value"
         record.user_id = 12345
-        
+
         result = formatter.format(record)
         log_data = json.loads(result)
-        
-        assert log_data['custom_field'] == 'custom_value'
-        assert log_data['user_id'] == 12345
+
+        assert log_data["custom_field"] == "custom_value"
+        assert log_data["user_id"] == 12345
 
 
 class TestConsoleFormatter:
     """Test ConsoleFormatter for development logging."""
-    
+
     def test_console_formatter_basic_record(self):
         """Test console formatter with basic log record."""
         # TODO: HIGH: Test console formatter readable output [Story:S14]
-        formatter = ConsoleFormatter('%(levelname)s - %(name)s - %(message)s')
-        record = logging.LogRecord(
-            name='test.module', level=logging.INFO, pathname='',
-            lineno=0, msg='Test message', args=(), exc_info=None
-        )
-        
-        result = formatter.format(record)
-        
-        assert 'INFO' in result
-        assert 'test.module' in result
-        assert 'Test message' in result
-    
+        formatter = ConsoleFormatter("%(levelname)s - %(name)s - %(message)s")
+        record = logging.LogRecord(
+            name="test.module",
+            level=logging.INFO,
+            pathname="",
+            lineno=0,
+            msg="Test message",
+            args=(),
+            exc_info=None,
+        )
+
+        result = formatter.format(record)
+
+        assert "INFO" in result
+        assert "test.module" in result
+        assert "Test message" in result
+
     def test_console_formatter_with_correlation_id(self):
         """Test console formatter includes correlation ID."""
         # TODO: HIGH: Test console formatter correlation ID display [Story:S14]
-        formatter = ConsoleFormatter('%(levelname)s - %(name)s - %(message)s')
-        record = logging.LogRecord(
-            name='test.module', level=logging.INFO, pathname='',
-            lineno=0, msg='Test message', args=(), exc_info=None
-        )
-        record.correlation_id = 'test-456'
-        
-        result = formatter.format(record)
-        
-        assert '[test-456]' in result
-        assert 'Test message' in result
+        formatter = ConsoleFormatter("%(levelname)s - %(name)s - %(message)s")
+        record = logging.LogRecord(
+            name="test.module",
+            level=logging.INFO,
+            pathname="",
+            lineno=0,
+            msg="Test message",
+            args=(),
+            exc_info=None,
+        )
+        record.correlation_id = "test-456"
+
+        result = formatter.format(record)
+
+        assert "[test-456]" in result
+        assert "Test message" in result
 
 
 class TestFileSizeParsing:
     """Test file size parsing utility function."""
-    
+
     def test_parse_file_size_basic_units(self):
         """Test parsing of standard file size units."""
         # TODO: MEDIUM: Test file size parsing accuracy [Story:S14]
-        assert _parse_file_size('1024B') == 1024
-        assert _parse_file_size('1KB') == 1024
-        assert _parse_file_size('1MB') == 1024 ** 2
-        assert _parse_file_size('1GB') == 1024 ** 3
-        assert _parse_file_size('10MB') == 10 * 1024 ** 2
-    
+        assert _parse_file_size("1024B") == 1024
+        assert _parse_file_size("1KB") == 1024
+        assert _parse_file_size("1MB") == 1024**2
+        assert _parse_file_size("1GB") == 1024**3
+        assert _parse_file_size("10MB") == 10 * 1024**2
+
     def test_parse_file_size_invalid_formats(self):
         """Test file size parsing with invalid formats."""
         # TODO: MEDIUM: Test file size parsing error handling [Story:S14]
         with pytest.raises(ValueError):
-            _parse_file_size('invalid')
-        
+            _parse_file_size("invalid")
+
         with pytest.raises(ValueError):
-            _parse_file_size('10XB')
+            _parse_file_size("10XB")
 
 
 class TestSetupLogging:
     """Test setup_logging function configuration."""
-    
+
     @pytest.fixture
     def logging_config(self):
         """Create a LoggingConfig for testing."""
         return LoggingConfig(
-            level='INFO',
-            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
+            level="INFO",
+            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
             file_enabled=False,
-            file_path='/tmp/test.log',
-            max_file_size='1MB',
-            backup_count=3
-        )
-    
+            file_path="/tmp/test.log",
+            max_file_size="1MB",
+            backup_count=3,
+        )
+
     def test_setup_logging_development_mode(self, logging_config):
         """Test logging setup in development mode."""
         # TODO: HIGH: Test development mode console formatter usage [Story:S14]
         # Clear any existing handlers
         root_logger = logging.getLogger()
         for handler in root_logger.handlers[:]:
             root_logger.removeHandler(handler)
-        
+
         setup_logging(logging_config, development_mode=True)
-        
+
         # Should have console handler
         assert len(root_logger.handlers) >= 1
-        
+
         # Should have correlation filter
         handler = root_logger.handlers[0]
-        correlation_filters = [f for f in handler.filters if isinstance(f, CorrelationIdFilter)]
+        correlation_filters = [
+            f for f in handler.filters if isinstance(f, CorrelationIdFilter)
+        ]
         assert len(correlation_filters) == 1
-        
+
         # Should use console formatter in development mode
         assert isinstance(handler.formatter, ConsoleFormatter)
-    
+
     def test_setup_logging_production_mode(self, logging_config):
         """Test logging setup in production mode."""
         # TODO: HIGH: Test production mode JSON formatter usage [Story:S14]
         # Clear any existing handlers
         root_logger = logging.getLogger()
         for handler in root_logger.handlers[:]:
             root_logger.removeHandler(handler)
-        
+
         setup_logging(logging_config, development_mode=False)
-        
+
         # Should use JSON formatter in production mode
         handler = root_logger.handlers[0]
         assert isinstance(handler.formatter, JsonFormatter)
-    
+
     def test_setup_logging_auto_mode_detection(self, logging_config):
         """Test automatic development/production mode detection."""
         # TODO: MEDIUM: Test environment-based mode detection [Story:S14]
-        with patch.dict(os.environ, {'CAMERA_SERVICE_ENV': 'development'}):
+        with patch.dict(os.environ, {"CAMERA_SERVICE_ENV": "development"}):
             # Clear any existing handlers
             root_logger = logging.getLogger()
             for handler in root_logger.handlers[:]:
                 root_logger.removeHandler(handler)
-            
+
             setup_logging(logging_config, development_mode=None)
-            
+
             # Should detect development mode and use console formatter
             handler = root_logger.handlers[0]
             assert isinstance(handler.formatter, ConsoleFormatter)
-    
+
     def test_setup_logging_with_rotation(self):
         """Test logging setup with file rotation enabled."""
         # TODO: HIGH: Test log rotation handler creation [Story:S14]
         config = LoggingConfig(
-            level='DEBUG',
+            level="DEBUG",
             file_enabled=True,
-            file_path='/tmp/test_rotation.log',
-            max_file_size='1MB',
-            backup_count=3
-        )
-        
+            file_path="/tmp/test_rotation.log",
+            max_file_size="1MB",
+            backup_count=3,
+        )
+
         with tempfile.TemporaryDirectory() as temp_dir:
-            config.file_path = str(Path(temp_dir) / 'test.log')
-            
+            config.file_path = str(Path(temp_dir) / "test.log")
+
             # Clear any existing handlers
             root_logger = logging.getLogger()
             for handler in root_logger.handlers[:]:
                 root_logger.removeHandler(handler)
-            
+
             setup_logging(config, development_mode=True)
-            
+
             # Should have both console and file handlers
             assert len(root_logger.handlers) >= 2
-            
+
             # File handler should be RotatingFileHandler
-            file_handlers = [h for h in root_logger.handlers 
-                           if isinstance(h, logging.handlers.RotatingFileHandler)]
+            file_handlers = [
+                h
+                for h in root_logger.handlers
+                if isinstance(h, logging.handlers.RotatingFileHandler)
+            ]
             assert len(file_handlers) == 1
-            
+
             # File should be created
             assert Path(config.file_path).exists()
-    
+
     def test_setup_logging_level_configuration(self, logging_config):
         """Test logging level is set correctly."""
         # TODO: MEDIUM: Test logging level configuration [Story:S14]
-        logging_config.level = 'DEBUG'
-        
+        logging_config.level = "DEBUG"
+
         # Clear any existing handlers
         root_logger = logging.getLogger()
         for handler in root_logger.handlers[:]:
             root_logger.removeHandler(handler)
-        
+
         setup_logging(logging_config)
-        
+
         assert root_logger.level == logging.DEBUG
-    
+
     def test_setup_logging_creates_log_directory(self):
         """Test that logging setup creates log directory if needed."""
         # TODO: MEDIUM: Test log directory creation [Story:S14]
         with tempfile.TemporaryDirectory() as temp_dir:
-            log_path = Path(temp_dir) / 'logs' / 'service.log'
-            config = LoggingConfig(
-                file_enabled=True,
-                file_path=str(log_path)
-            )
-            
+            log_path = Path(temp_dir) / "logs" / "service.log"
+            config = LoggingConfig(file_enabled=True, file_path=str(log_path))
+
             # Clear any existing handlers
             root_logger = logging.getLogger()
             for handler in root_logger.handlers[:]:
                 root_logger.removeHandler(handler)
-            
+
             setup_logging(config)
-            
+
             # Directory should be created
             assert log_path.parent.exists()
-    
+
     def test_setup_logging_rotation_fallback(self):
         """Test fallback to basic FileHandler when rotation config is invalid."""
         # TODO: MEDIUM: Test rotation configuration fallback [Story:S14]
         config = LoggingConfig(
             file_enabled=True,
-            file_path='/tmp/test_fallback.log',
-            max_file_size='invalid_size',  # Invalid size should trigger fallback
-            backup_count=3
-        )
-        
+            file_path="/tmp/test_fallback.log",
+            max_file_size="invalid_size",  # Invalid size should trigger fallback
+            backup_count=3,
+        )
+
         with tempfile.TemporaryDirectory() as temp_dir:
-            config.file_path = str(Path(temp_dir) / 'test.log')
-            
+            config.file_path = str(Path(temp_dir) / "test.log")
+
             # Clear any existing handlers
             root_logger = logging.getLogger()
             for handler in root_logger.handlers[:]:
                 root_logger.removeHandler(handler)
-            
+
             setup_logging(config, development_mode=True)
-            
+
             # Should still have file handler (basic FileHandler as fallback)
-            file_handlers = [h for h in root_logger.handlers 
-                           if isinstance(h, logging.FileHandler)]
+            file_handlers = [
+                h for h in root_logger.handlers if isinstance(h, logging.FileHandler)
+            ]
             assert len(file_handlers) == 1
 
 
 class TestGlobalHelperFunctions:
     """Test global helper functions for correlation ID management."""
-    
+
     def test_get_correlation_filter_function(self):
         """Test get_correlation_filter function."""
         # TODO: MEDIUM: Test correlation filter retrieval [Story:S14]
-        with patch('logging.getLogger') as mock_get_logger:
+        with patch("logging.getLogger") as mock_get_logger:
             mock_logger = Mock()
             mock_handler = Mock()
             mock_filter = CorrelationIdFilter()
             mock_handler.filters = [mock_filter]
             mock_logger.handlers = [mock_handler]
             mock_get_logger.return_value = mock_logger
-            
+
             result = get_correlation_filter()
-            
+
             assert result is mock_filter
-    
+
     def test_get_correlation_filter_not_found(self):
         """Test get_correlation_filter when no filter exists."""
         # TODO: MEDIUM: Test correlation filter not found case [Story:S14]
-        with patch('logging.getLogger') as mock_get_logger:
+        with patch("logging.getLogger") as mock_get_logger:
             mock_logger = Mock()
             mock_logger.handlers = []
             mock_get_logger.return_value = mock_logger
-            
+
             result = get_correlation_filter()
-            
+
             assert result is None
-    
+
     def test_set_correlation_id_function(self):
         """Test global set_correlation_id function."""
         # TODO: MEDIUM: Test global correlation ID setting [Story:S14]
-        with patch('camera_service.logging_config.get_correlation_filter') as mock_get_filter:
+        with patch(
+            "camera_service.logging_config.get_correlation_filter"
+        ) as mock_get_filter:
             mock_filter = Mock()
             mock_get_filter.return_value = mock_filter
-            
-            result = set_correlation_id('global-test-id')
-            
-            mock_filter.set_correlation_id.assert_called_once_with('global-test-id')
+
+            result = set_correlation_id("global-test-id")
+
+            mock_filter.set_correlation_id.assert_called_once_with("global-test-id")
             assert result is True
-    
+
     def test_get_correlation_id_function(self):
         """Test global get_correlation_id function."""
         # TODO: MEDIUM: Test global correlation ID retrieval [Story:S14]
-        with patch('camera_service.logging_config.get_correlation_filter') as mock_get_filter:
+        with patch(
+            "camera_service.logging_config.get_correlation_filter"
+        ) as mock_get_filter:
             mock_filter = Mock()
-            mock_filter.get_correlation_id.return_value = 'current-id'
+            mock_filter.get_correlation_id.return_value = "current-id"
             mock_get_filter.return_value = mock_filter
-            
+
             result = get_correlation_id()
-            
-            assert result == 'current-id'
+
+            assert result == "current-id"
 
 
 class TestLoggingIntegration:
     """Integration tests for logging functionality."""
-    
+
     def test_end_to_end_logging_flow(self):
         """Test complete logging flow with correlation IDs."""
         # TODO: LOW: Test end-to-end logging integration [Story:S14]
         # This would test the complete flow:
         # 1. Setup logging
         # 2. Set correlation ID
         # 3. Log messages
         # 4. Verify output format includes correlation ID
         # 5. Verify structured/console format as appropriate
-        
+
         # Implementation deferred until integration test phase
-        pass
\ No newline at end of file
+        pass
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_service/test_service_manager_lifecycle.py	2025-08-04 14:26:20.048201+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_camera_service/test_service_manager_lifecycle.py	2025-08-04 15:35:25.757915+00:00
@@ -10,11 +10,19 @@
 import asyncio
 import uuid
 from unittest.mock import Mock, AsyncMock, patch
 
 from src.camera_service.service_manager import ServiceManager
-from src.camera_service.config import Config, ServerConfig, MediaMTXConfig, CameraConfig, LoggingConfig, RecordingConfig, SnapshotConfig
+from src.camera_service.config import (
+    Config,
+    ServerConfig,
+    MediaMTXConfig,
+    CameraConfig,
+    LoggingConfig,
+    RecordingConfig,
+    SnapshotConfig,
+)
 from src.camera_discovery.hybrid_monitor import CameraEventData, CameraEvent
 from src.common.types import CameraDevice
 
 
 class TestServiceManagerLifecycle:
@@ -24,22 +32,24 @@
     def mock_config(self):
         """Create mock configuration for testing."""
         return Config(
             server=ServerConfig(host="localhost", port=8002),
             mediamtx=MediaMTXConfig(
-                host="localhost", 
+                host="localhost",
                 api_port=9997,
                 rtsp_port=8554,
                 webrtc_port=8889,
                 hls_port=8888,
                 recordings_path="/tmp/recordings",
-                snapshots_path="/tmp/snapshots"
+                snapshots_path="/tmp/snapshots",
             ),
-            camera=CameraConfig(device_range=[0, 1, 2], enable_capability_detection=True),  
+            camera=CameraConfig(
+                device_range=[0, 1, 2], enable_capability_detection=True
+            ),
             logging=LoggingConfig(),
             recording=RecordingConfig(),
-            snapshots=SnapshotConfig()
+            snapshots=SnapshotConfig(),
         )
 
     @pytest.fixture
     def service_manager(self, mock_config):
         """Create service manager instance for testing."""
@@ -47,38 +57,36 @@
 
     @pytest.fixture
     def mock_camera_event_connected(self):
         """Create mock camera connection event."""
         camera_device = CameraDevice(
-            device="/dev/video0",
-            name="Test Camera 0",
-            status="CONNECTED"
+            device="/dev/video0", name="Test Camera 0", status="CONNECTED"
         )
         return CameraEventData(
             device_path="/dev/video0",
             event_type=CameraEvent.CONNECTED,
             device_info=camera_device,
-            timestamp=1234567890.0
+            timestamp=1234567890.0,
         )
 
     @pytest.fixture
     def mock_camera_event_disconnected(self):
         """Create mock camera disconnection event."""
         camera_device = CameraDevice(
-            device="/dev/video0",
-            name="Test Camera 0",
-            status="DISCONNECTED"
+            device="/dev/video0", name="Test Camera 0", status="DISCONNECTED"
         )
         return CameraEventData(
             device_path="/dev/video0",
             event_type=CameraEvent.DISCONNECTED,
             device_info=camera_device,
-            timestamp=1234567891.0
-        )
-
-    @pytest.mark.asyncio
-    async def test_camera_connect_orchestration_sequence(self, service_manager, mock_camera_event_connected):
+            timestamp=1234567891.0,
+        )
+
+    @pytest.mark.asyncio
+    async def test_camera_connect_orchestration_sequence(
+        self, service_manager, mock_camera_event_connected
+    ):
         """
         Test camera connection orchestration follows correct sequence:
         1. Stream name generation
         2. MediaMTX stream creation
         3. Capability metadata retrieval
@@ -86,214 +94,248 @@
         """
         # Mock dependencies
         mock_mediamtx = Mock()
         mock_mediamtx.create_stream = AsyncMock(return_value={})
         service_manager._mediamtx_controller = mock_mediamtx
-        
+
         mock_websocket = Mock()
         mock_websocket.notify_camera_status_update = AsyncMock()
         service_manager._websocket_server = mock_websocket
-        
+
         # Mock camera monitor with confirmed capability data
         mock_camera_monitor = Mock()
-        mock_camera_monitor.get_effective_capability_metadata = Mock(return_value={
-            "resolution": "1280x720",
-            "fps": 25,
-            "validation_status": "confirmed",
-            "consecutive_successes": 10,
-            "formats": ["YUYV"],
-            "all_resolutions": ["1280x720", "640x480"]
-        })
-        service_manager._camera_monitor = mock_camera_monitor
-        
+        mock_camera_monitor.get_effective_capability_metadata = Mock(
+            return_value={
+                "resolution": "1280x720",
+                "fps": 25,
+                "validation_status": "confirmed",
+                "consecutive_successes": 10,
+                "formats": ["YUYV"],
+                "all_resolutions": ["1280x720", "640x480"],
+            }
+        )
+        service_manager._camera_monitor = mock_camera_monitor
+
         # Execute camera connection handling
-        with patch('src.camera_service.service_manager.set_correlation_id'):
+        with patch("src.camera_service.service_manager.set_correlation_id"):
             await service_manager.handle_camera_event(mock_camera_event_connected)
-        
+
         # Verify orchestration sequence
         # 1. MediaMTX stream creation was called
         mock_mediamtx.create_stream.assert_called_once()
         stream_config = mock_mediamtx.create_stream.call_args[0][0]
         assert stream_config.name == "camera0"
         assert stream_config.source == "/dev/video0"
-        
+
         # 2. Notification was sent with enhanced metadata
         mock_websocket.notify_camera_status_update.assert_called_once()
         notification_params = mock_websocket.notify_camera_status_update.call_args[0][0]
-        
+
         # Verify notification includes provisional/confirmed metadata flags
         assert notification_params["device"] == "/dev/video0"
         assert notification_params["status"] == "CONNECTED"
-        assert notification_params["resolution"] == "1280x720"  # From confirmed capability
-        assert notification_params["fps"] == 25                 # From confirmed capability
+        assert (
+            notification_params["resolution"] == "1280x720"
+        )  # From confirmed capability
+        assert notification_params["fps"] == 25  # From confirmed capability
         assert notification_params["metadata_validation"] == "confirmed"
         assert notification_params["metadata_source"] == "confirmed_capability"
         assert notification_params["metadata_provisional"] is False
         assert notification_params["metadata_confirmed"] is True
         assert "streams" in notification_params
         assert "rtsp" in notification_params["streams"]
 
     @pytest.mark.asyncio
-    async def test_camera_disconnect_orchestration_sequence(self, service_manager, mock_camera_event_disconnected):
+    async def test_camera_disconnect_orchestration_sequence(
+        self, service_manager, mock_camera_event_disconnected
+    ):
         """
         Test camera disconnection orchestration follows correct sequence:
         1. MediaMTX stream deletion
         2. Metadata retrieval (cached/default)
         3. Notification broadcasting with empty streams
         """
         # Mock dependencies
         mock_mediamtx = Mock()
         mock_mediamtx.delete_stream = AsyncMock(return_value=True)
         service_manager._mediamtx_controller = mock_mediamtx
-        
+
         mock_websocket = Mock()
         mock_websocket.notify_camera_status_update = AsyncMock()
         service_manager._websocket_server = mock_websocket
-        
+
         # Mock camera monitor (for metadata fallback)
         mock_camera_monitor = Mock()
-        mock_camera_monitor.get_effective_capability_metadata = Mock(return_value={
-            "resolution": "1920x1080",
-            "fps": 30,
-            "validation_status": "none"
-        })
-        service_manager._camera_monitor = mock_camera_monitor
-        
+        mock_camera_monitor.get_effective_capability_metadata = Mock(
+            return_value={
+                "resolution": "1920x1080",
+                "fps": 30,
+                "validation_status": "none",
+            }
+        )
+        service_manager._camera_monitor = mock_camera_monitor
+
         # Execute camera disconnection handling
-        with patch('src.camera_service.service_manager.set_correlation_id'):
+        with patch("src.camera_service.service_manager.set_correlation_id"):
             await service_manager.handle_camera_event(mock_camera_event_disconnected)
-        
+
         # Verify orchestration sequence
         # 1. MediaMTX stream deletion was called
         mock_mediamtx.delete_stream.assert_called_once_with("camera0")
-        
+
         # 2. Notification was sent with disconnected status
         mock_websocket.notify_camera_status_update.assert_called_once()
         notification_params = mock_websocket.notify_camera_status_update.call_args[0][0]
         assert notification_params["device"] == "/dev/video0"
         assert notification_params["status"] == "DISCONNECTED"
         assert notification_params["resolution"] == ""  # Empty for disconnected
-        assert notification_params["fps"] == 0          # Zero for disconnected
-        assert notification_params["streams"] == {}     # Empty streams
+        assert notification_params["fps"] == 0  # Zero for disconnected
+        assert notification_params["streams"] == {}  # Empty streams
         assert notification_params["metadata_validation"] == "none"
         assert notification_params["metadata_provisional"] is False
         assert notification_params["metadata_confirmed"] is False
 
     @pytest.mark.asyncio
-    async def test_metadata_propagation_provisional_capability(self, service_manager, mock_camera_event_connected):
+    async def test_metadata_propagation_provisional_capability(
+        self, service_manager, mock_camera_event_connected
+    ):
         """Test metadata propagation uses provisional capability data with appropriate logging."""
         # Mock camera monitor with provisional capability data
         mock_camera_monitor = Mock()
-        mock_camera_monitor.get_effective_capability_metadata = Mock(return_value={
-            "resolution": "640x480",
-            "fps": 15,
-            "validation_status": "provisional",
-            "consecutive_successes": 2,
-            "formats": ["MJPEG"]
-        })
-        service_manager._camera_monitor = mock_camera_monitor
-        
+        mock_camera_monitor.get_effective_capability_metadata = Mock(
+            return_value={
+                "resolution": "640x480",
+                "fps": 15,
+                "validation_status": "provisional",
+                "consecutive_successes": 2,
+                "formats": ["MJPEG"],
+            }
+        )
+        service_manager._camera_monitor = mock_camera_monitor
+
         # Mock other dependencies
         service_manager._mediamtx_controller = Mock()
         service_manager._mediamtx_controller.create_stream = AsyncMock(return_value={})
         service_manager._websocket_server = Mock()
         service_manager._websocket_server.notify_camera_status_update = AsyncMock()
-        
+
         # Execute with logging capture
-        with patch('src.camera_service.service_manager.set_correlation_id'):
+        with patch("src.camera_service.service_manager.set_correlation_id"):
             await service_manager.handle_camera_event(mock_camera_event_connected)
-        
+
         # Verify provisional capability data is used with proper flags
-        notification_params = service_manager._websocket_server.notify_camera_status_update.call_args[0][0]
-        assert notification_params["resolution"] == "640x480"   # From provisional capability
-        assert notification_params["fps"] == 15                 # From provisional capability
+        notification_params = (
+            service_manager._websocket_server.notify_camera_status_update.call_args[0][
+                0
+            ]
+        )
+        assert (
+            notification_params["resolution"] == "640x480"
+        )  # From provisional capability
+        assert notification_params["fps"] == 15  # From provisional capability
         assert notification_params["metadata_validation"] == "provisional"
         assert notification_params["metadata_source"] == "provisional_capability"
         assert notification_params["metadata_provisional"] is True
         assert notification_params["metadata_confirmed"] is False
 
     @pytest.mark.asyncio
-    async def test_mediamtx_controller_failure_recovery(self, service_manager, mock_camera_event_connected):
+    async def test_mediamtx_controller_failure_recovery(
+        self, service_manager, mock_camera_event_connected
+    ):
         """Test recovery behavior when MediaMTX controller fails during stream creation."""
         # Mock MediaMTX controller that fails stream creation
         mock_mediamtx = Mock()
-        mock_mediamtx.create_stream = AsyncMock(side_effect=Exception("MediaMTX connection failed"))
+        mock_mediamtx.create_stream = AsyncMock(
+            side_effect=Exception("MediaMTX connection failed")
+        )
         service_manager._mediamtx_controller = mock_mediamtx
-        
+
         # Mock other dependencies
         mock_websocket = Mock()
         mock_websocket.notify_camera_status_update = AsyncMock()
         service_manager._websocket_server = mock_websocket
-        
-        mock_camera_monitor = Mock()
-        mock_camera_monitor.get_effective_capability_metadata = Mock(return_value={
-            "resolution": "1920x1080",
-            "fps": 30,
-            "validation_status": "confirmed"
-        })
-        service_manager._camera_monitor = mock_camera_monitor
-        
+
+        mock_camera_monitor = Mock()
+        mock_camera_monitor.get_effective_capability_metadata = Mock(
+            return_value={
+                "resolution": "1920x1080",
+                "fps": 30,
+                "validation_status": "confirmed",
+            }
+        )
+        service_manager._camera_monitor = mock_camera_monitor
+
         # Execute camera connection handling (should not raise exception)
-        with patch('src.camera_service.service_manager.set_correlation_id'):
+        with patch("src.camera_service.service_manager.set_correlation_id"):
             await service_manager.handle_camera_event(mock_camera_event_connected)
-        
+
         # Verify notification still sent despite MediaMTX failure
         mock_websocket.notify_camera_status_update.assert_called_once()
         notification_params = mock_websocket.notify_camera_status_update.call_args[0][0]
         assert notification_params["device"] == "/dev/video0"
         assert notification_params["status"] == "CONNECTED"
         assert notification_params["streams"] == {}  # Empty due to MediaMTX failure
 
     @pytest.mark.asyncio
-    async def test_missing_mediamtx_controller_defensive_behavior(self, service_manager, mock_camera_event_connected):
+    async def test_missing_mediamtx_controller_defensive_behavior(
+        self, service_manager, mock_camera_event_connected
+    ):
         """Test defensive behavior when MediaMTX controller is not available."""
         # No MediaMTX controller
         service_manager._mediamtx_controller = None
-        
+
         # Mock other dependencies
         mock_websocket = Mock()
         mock_websocket.notify_camera_status_update = AsyncMock()
         service_manager._websocket_server = mock_websocket
-        
-        mock_camera_monitor = Mock()
-        mock_camera_monitor.get_effective_capability_metadata = Mock(return_value={
-            "resolution": "1920x1080",
-            "fps": 30,
-            "validation_status": "none"
-        })
-        service_manager._camera_monitor = mock_camera_monitor
-        
+
+        mock_camera_monitor = Mock()
+        mock_camera_monitor.get_effective_capability_metadata = Mock(
+            return_value={
+                "resolution": "1920x1080",
+                "fps": 30,
+                "validation_status": "none",
+            }
+        )
+        service_manager._camera_monitor = mock_camera_monitor
+
         # Execute camera connection handling (should not crash)
-        with patch('src.camera_service.service_manager.set_correlation_id'):
+        with patch("src.camera_service.service_manager.set_correlation_id"):
             await service_manager.handle_camera_event(mock_camera_event_connected)
-        
+
         # Verify notification still sent with warning logged
         mock_websocket.notify_camera_status_update.assert_called_once()
         notification_params = mock_websocket.notify_camera_status_update.call_args[0][0]
         assert notification_params["streams"] == {}  # No streams without MediaMTX
 
     @pytest.mark.asyncio
-    async def test_capability_detection_error_fallback(self, service_manager, mock_camera_event_connected):
+    async def test_capability_detection_error_fallback(
+        self, service_manager, mock_camera_event_connected
+    ):
         """Test fallback behavior when capability detection throws exception."""
         # Mock camera monitor that raises exception
         mock_camera_monitor = Mock()
-        mock_camera_monitor.get_effective_capability_metadata = Mock(side_effect=Exception("Capability detection failed"))
-        service_manager._camera_monitor = mock_camera_monitor
-        
+        mock_camera_monitor.get_effective_capability_metadata = Mock(
+            side_effect=Exception("Capability detection failed")
+        )
+        service_manager._camera_monitor = mock_camera_monitor
+
         # Mock other dependencies
         service_manager._mediamtx_controller = Mock()
         service_manager._mediamtx_controller.create_stream = AsyncMock(return_value={})
         service_manager._websocket_server = Mock()
         service_manager._websocket_server.notify_camera_status_update = AsyncMock()
-        
+
         # Execute capability metadata retrieval
-        with patch('src.camera_service.service_manager.set_correlation_id'):
-            metadata = await service_manager._get_enhanced_camera_metadata(mock_camera_event_connected)
-        
+        with patch("src.camera_service.service_manager.set_correlation_id"):
+            metadata = await service_manager._get_enhanced_camera_metadata(
+                mock_camera_event_connected
+            )
+
         # Verify fallback to defaults with error annotation
         assert metadata["resolution"] == "1920x1080"  # Architecture default
-        assert metadata["fps"] == 30                   # Architecture default
+        assert metadata["fps"] == 30  # Architecture default
         assert metadata["validation_status"] == "error"
         assert metadata["capability_source"] == "default"
 
     def test_stream_name_generation_deterministic(self, service_manager):
         """Test stream name generation is deterministic for various device paths."""
@@ -302,13 +344,13 @@
             ("/dev/video15", "camera15"),
             ("/dev/video999", "camera999"),
             ("/custom/video5", "camera5"),
             ("/path/with/multiple/video2/segments", "camera2"),
             ("/no/numbers/here", "camera_"),  # Will get hash-based name
-            ("", "camera_unknown")
+            ("", "camera_unknown"),
         ]
-        
+
         for device_path, expected_prefix in test_cases:
             result = service_manager._get_stream_name_from_device_path(device_path)
             if expected_prefix.endswith("_"):
                 # Hash-based names should start with expected prefix
                 assert result.startswith(expected_prefix)
@@ -318,188 +360,239 @@
 
     @pytest.mark.asyncio
     async def test_service_lifecycle_startup_shutdown_sequence(self, service_manager):
         """Test complete service lifecycle startup and shutdown sequence."""
         # Mock all components
-        with patch('src.camera_service.service_manager.MediaMTXController') as MockMediaMTX, \
-             patch('src.camera_discovery.hybrid_monitor.HybridCameraMonitor') as MockCameraMonitor, \
-             patch('src.camera_service.service_manager.HealthMonitor') as MockHealthMonitor, \
-             patch('src.websocket_server.server.WebSocketJsonRpcServer') as MockWebSocketServer, \
-             patch('src.camera_service.service_manager.set_correlation_id'), \
-             patch('src.camera_service.service_manager.get_correlation_id', return_value="test-correlation-id"):
-            
+        with (
+            patch(
+                "src.camera_service.service_manager.MediaMTXController"
+            ) as MockMediaMTX,
+            patch(
+                "src.camera_discovery.hybrid_monitor.HybridCameraMonitor"
+            ) as MockCameraMonitor,
+            patch(
+                "src.camera_service.service_manager.HealthMonitor"
+            ) as MockHealthMonitor,
+            patch(
+                "src.websocket_server.server.WebSocketJsonRpcServer"
+            ) as MockWebSocketServer,
+            patch("src.camera_service.service_manager.set_correlation_id"),
+            patch(
+                "src.camera_service.service_manager.get_correlation_id",
+                return_value="test-correlation-id",
+            ),
+        ):
+
             # Setup mock instances
             mock_mediamtx_instance = Mock()
             mock_mediamtx_instance.start = AsyncMock()
-            mock_mediamtx_instance.health_check = AsyncMock(return_value={"status": "healthy"})
+            mock_mediamtx_instance.health_check = AsyncMock(
+                return_value={"status": "healthy"}
+            )
             mock_mediamtx_instance.stop = AsyncMock()
             MockMediaMTX.return_value = mock_mediamtx_instance
-            
+
             mock_camera_instance = Mock()
             mock_camera_instance.start = AsyncMock()
             mock_camera_instance.stop = AsyncMock()
             mock_camera_instance.add_event_handler = Mock()
             mock_camera_instance.remove_event_handler = Mock()
             MockCameraMonitor.return_value = mock_camera_instance
-            
+
             mock_health_instance = Mock()
             mock_health_instance.start = AsyncMock()
             mock_health_instance.stop = AsyncMock()
             MockHealthMonitor.return_value = mock_health_instance
-            
+
             mock_websocket_instance = Mock()
             mock_websocket_instance.start = AsyncMock()
             mock_websocket_instance.stop = AsyncMock()
             MockWebSocketServer.return_value = mock_websocket_instance
-            
+
             # Test startup sequence
             await service_manager.start()
-            
+
             # Verify startup order
             mock_mediamtx_instance.start.assert_called_once()
             mock_camera_instance.start.assert_called_once()
             mock_health_instance.start.assert_called_once()
             mock_websocket_instance.start.assert_called_once()
-            
+
             assert service_manager.is_running is True
-            
+
             # Test shutdown sequence
             await service_manager.stop()
-            
+
             # Verify shutdown order (reverse of startup)
             mock_websocket_instance.stop.assert_called_once()
             mock_health_instance.stop.assert_called_once()
             mock_camera_instance.stop.assert_called_once()
             mock_mediamtx_instance.stop.assert_called_once()
-            
+
             assert service_manager.is_running is False
 
     @pytest.mark.asyncio
-    async def test_correlation_id_propagation_lifecycle(self, service_manager, mock_camera_event_connected):
+    async def test_correlation_id_propagation_lifecycle(
+        self, service_manager, mock_camera_event_connected
+    ):
         """Test correlation ID propagation through camera event lifecycle."""
         # Mock dependencies
         mock_camera_monitor = Mock()
-        mock_camera_monitor.get_effective_capability_metadata = Mock(return_value={
-            "resolution": "1280x720",
-            "fps": 25,
-            "validation_status": "confirmed",
-            "consecutive_successes": 8
-        })
-        service_manager._camera_monitor = mock_camera_monitor
-        
+        mock_camera_monitor.get_effective_capability_metadata = Mock(
+            return_value={
+                "resolution": "1280x720",
+                "fps": 25,
+                "validation_status": "confirmed",
+                "consecutive_successes": 8,
+            }
+        )
+        service_manager._camera_monitor = mock_camera_monitor
+
         mock_mediamtx = Mock()
         mock_mediamtx.create_stream = AsyncMock(return_value={})
         service_manager._mediamtx_controller = mock_mediamtx
-        
+
         mock_websocket = Mock()
         mock_websocket.notify_camera_status_update = AsyncMock()
         service_manager._websocket_server = mock_websocket
-        
+
         # Execute with correlation ID tracking
-        with patch('src.camera_service.service_manager.set_correlation_id') as mock_set_corr, \
-             patch('src.camera_service.service_manager.get_correlation_id', return_value="test-correlation-123"):
-            
+        with (
+            patch(
+                "src.camera_service.service_manager.set_correlation_id"
+            ) as mock_set_corr,
+            patch(
+                "src.camera_service.service_manager.get_correlation_id",
+                return_value="test-correlation-123",
+            ),
+        ):
+
             await service_manager.handle_camera_event(mock_camera_event_connected)
-            
+
             # Verify correlation ID was set during event handling
             assert mock_set_corr.called
 
-    @pytest.mark.asyncio 
+    @pytest.mark.asyncio
     async def test_startup_failure_cleanup(self, service_manager):
         """Test cleanup of partially started components on startup failure."""
-        with patch('src.camera_service.service_manager.MediaMTXController') as MockMediaMTX, \
-             patch('src.camera_discovery.hybrid_monitor.HybridCameraMonitor') as MockCameraMonitor, \
-             patch('src.camera_service.service_manager.HealthMonitor') as MockHealthMonitor, \
-             patch('src.websocket_server.server.WebSocketJsonRpcServer') as MockWebSocketServer, \
-             patch('src.camera_service.service_manager.set_correlation_id'), \
-             patch('src.camera_service.service_manager.get_correlation_id', return_value="test-correlation-id"):
-            
+        with (
+            patch(
+                "src.camera_service.service_manager.MediaMTXController"
+            ) as MockMediaMTX,
+            patch(
+                "src.camera_discovery.hybrid_monitor.HybridCameraMonitor"
+            ) as MockCameraMonitor,
+            patch(
+                "src.camera_service.service_manager.HealthMonitor"
+            ) as MockHealthMonitor,
+            patch(
+                "src.websocket_server.server.WebSocketJsonRpcServer"
+            ) as MockWebSocketServer,
+            patch("src.camera_service.service_manager.set_correlation_id"),
+            patch(
+                "src.camera_service.service_manager.get_correlation_id",
+                return_value="test-correlation-id",
+            ),
+        ):
+
             # Setup MediaMTX to succeed
             mock_mediamtx_instance = Mock()
             mock_mediamtx_instance.start = AsyncMock()
-            mock_mediamtx_instance.health_check = AsyncMock(return_value={"status": "healthy"})
+            mock_mediamtx_instance.health_check = AsyncMock(
+                return_value={"status": "healthy"}
+            )
             mock_mediamtx_instance.stop = AsyncMock()
             MockMediaMTX.return_value = mock_mediamtx_instance
-            
+
             # Setup camera monitor to succeed
             mock_camera_instance = Mock()
             mock_camera_instance.start = AsyncMock()
             mock_camera_instance.stop = AsyncMock()
             mock_camera_instance.add_event_handler = Mock()
             mock_camera_instance.remove_event_handler = Mock()
             MockCameraMonitor.return_value = mock_camera_instance
-            
+
             # Setup health monitor to fail
             mock_health_instance = Mock()
-            mock_health_instance.start = AsyncMock(side_effect=Exception("Health monitor startup failed"))
+            mock_health_instance.start = AsyncMock(
+                side_effect=Exception("Health monitor startup failed")
+            )
             mock_health_instance.stop = AsyncMock()
             MockHealthMonitor.return_value = mock_health_instance
-            
+
             # Setup websocket server
             mock_websocket_instance = Mock()
             mock_websocket_instance.start = AsyncMock()
             mock_websocket_instance.stop = AsyncMock()
             MockWebSocketServer.return_value = mock_websocket_instance
-            
+
             # Attempt startup - should fail and cleanup
             with pytest.raises(Exception, match="Health monitor startup failed"):
                 await service_manager.start()
-            
+
             # Verify cleanup was performed
             mock_camera_instance.stop.assert_called_once()
             mock_mediamtx_instance.stop.assert_called_once()
-            
+
             assert service_manager.is_running is False
 
     @pytest.mark.asyncio
-    async def test_notification_metadata_fields_comprehensive(self, service_manager, mock_camera_event_connected):
+    async def test_notification_metadata_fields_comprehensive(
+        self, service_manager, mock_camera_event_connected
+    ):
         """Test that notifications include all required metadata fields for observability."""
         # Mock camera monitor with rich capability data
         mock_camera_monitor = Mock()
-        mock_camera_monitor.get_effective_capability_metadata = Mock(return_value={
-            "resolution": "1920x1080",
-            "fps": 30,
-            "validation_status": "confirmed", 
-            "consecutive_successes": 15,
-            "formats": ["YUYV", "MJPEG"],
-            "all_resolutions": ["1920x1080", "1280x720", "640x480"]
-        })
-        service_manager._camera_monitor = mock_camera_monitor
-        
+        mock_camera_monitor.get_effective_capability_metadata = Mock(
+            return_value={
+                "resolution": "1920x1080",
+                "fps": 30,
+                "validation_status": "confirmed",
+                "consecutive_successes": 15,
+                "formats": ["YUYV", "MJPEG"],
+                "all_resolutions": ["1920x1080", "1280x720", "640x480"],
+            }
+        )
+        service_manager._camera_monitor = mock_camera_monitor
+
         # Mock other dependencies
         service_manager._mediamtx_controller = Mock()
         service_manager._mediamtx_controller.create_stream = AsyncMock(return_value={})
         service_manager._websocket_server = Mock()
         service_manager._websocket_server.notify_camera_status_update = AsyncMock()
-        
+
         # Execute camera connection event
-        with patch('src.camera_service.service_manager.set_correlation_id'):
+        with patch("src.camera_service.service_manager.set_correlation_id"):
             await service_manager.handle_camera_event(mock_camera_event_connected)
-        
+
         # Verify comprehensive notification metadata
-        notification_params = service_manager._websocket_server.notify_camera_status_update.call_args[0][0]
-        
+        notification_params = (
+            service_manager._websocket_server.notify_camera_status_update.call_args[0][
+                0
+            ]
+        )
+
         # Core API fields
         assert "device" in notification_params
         assert "status" in notification_params
         assert "name" in notification_params
         assert "resolution" in notification_params
         assert "fps" in notification_params
         assert "streams" in notification_params
-        
+
         # Enhanced metadata fields for observability
         assert "metadata_validation" in notification_params
         assert "metadata_source" in notification_params
         assert "metadata_provisional" in notification_params
         assert "metadata_confirmed" in notification_params
-        
+
         # Verify values are correct
         assert notification_params["metadata_validation"] == "confirmed"
         assert notification_params["metadata_source"] == "confirmed_capability"
         assert notification_params["metadata_provisional"] is False
         assert notification_params["metadata_confirmed"] is True
 
 
 # TODO: HIGH: Add integration tests with real camera monitor instance [Story:E1/S5]
 # TODO: MEDIUM: Add performance tests for camera event processing latency [Story:E1/S5]
 # TODO: MEDIUM: Add stress tests for rapid connect/disconnect sequences [Story:E1/S5]
-# TODO: LOW: Add tests for unknown camera event types [Story:E1/S5]
\ No newline at end of file
+# TODO: LOW: Add tests for unknown camera event types [Story:E1/S5]
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_controller_health_monitoring.py	2025-08-03 19:27:38.812956+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_controller_health_monitoring.py	2025-08-04 15:35:25.767341+00:00
@@ -1,10 +1,10 @@
 # tests/unit/test_mediamtx_wrapper/test_controller_health_monitoring.py
 """
 Test health monitoring circuit breaker activation/recovery and adaptive backoff.
 
-Test policy: Verify configurable circuit breaker behavior, exponential backoff 
+Test policy: Verify configurable circuit breaker behavior, exponential backoff
 with jitter, state transitions, and recovery logging.
 """
 
 import pytest
 import asyncio
@@ -34,15 +34,15 @@
             health_check_interval=0.1,
             health_failure_threshold=3,
             health_circuit_breaker_timeout=1.0,
             health_max_backoff_interval=2.0,
             backoff_base_multiplier=2.0,
-            backoff_jitter_range=(1.0, 1.0)  # No jitter for predictable testing
+            backoff_jitter_range=(1.0, 1.0),  # No jitter for predictable testing
         )
         return controller
 
-    @pytest.fixture  
+    @pytest.fixture
     def mock_session(self):
         """Create mock aiohttp session."""
         session = Mock()
         session.get = AsyncMock()
         session.close = AsyncMock()
@@ -58,243 +58,299 @@
 
     def test_configurable_circuit_breaker_parameters(self):
         """Test circuit breaker uses configurable parameters, not hardcoded values."""
         # Test with different threshold values
         controller1 = MediaMTXController(
-            host="localhost", api_port=9997, rtsp_port=8554,
-            webrtc_port=8889, hls_port=8888, config_path="/tmp/config.yml",
-            recordings_path="/tmp/recordings", snapshots_path="/tmp/snapshots",
+            host="localhost",
+            api_port=9997,
+            rtsp_port=8554,
+            webrtc_port=8889,
+            hls_port=8888,
+            config_path="/tmp/config.yml",
+            recordings_path="/tmp/recordings",
+            snapshots_path="/tmp/snapshots",
             health_failure_threshold=5,  # Custom threshold
             health_circuit_breaker_timeout=30,  # Custom timeout
             health_max_backoff_interval=60,  # Custom max backoff
-            health_recovery_confirmation_threshold=2  # Custom recovery confirmation
-        )
-        
+            health_recovery_confirmation_threshold=2,  # Custom recovery confirmation
+        )
+
         controller2 = MediaMTXController(
-            host="localhost", api_port=9997, rtsp_port=8554,
-            webrtc_port=8889, hls_port=8888, config_path="/tmp/config.yml", 
-            recordings_path="/tmp/recordings", snapshots_path="/tmp/snapshots",
+            host="localhost",
+            api_port=9997,
+            rtsp_port=8554,
+            webrtc_port=8889,
+            hls_port=8888,
+            config_path="/tmp/config.yml",
+            recordings_path="/tmp/recordings",
+            snapshots_path="/tmp/snapshots",
             health_failure_threshold=2,  # Different threshold
             health_circuit_breaker_timeout=10,  # Different timeout
             health_max_backoff_interval=30,  # Different max backoff
-            health_recovery_confirmation_threshold=4  # Different recovery confirmation
-        )
-        
+            health_recovery_confirmation_threshold=4,  # Different recovery confirmation
+        )
+
         # Verify different controllers use their configured values
         assert controller1._health_failure_threshold == 5
         assert controller1._health_circuit_breaker_timeout == 30
         assert controller1._health_max_backoff_interval == 60
         assert controller1._health_recovery_confirmation_threshold == 2
-        
+
         assert controller2._health_failure_threshold == 2
         assert controller2._health_circuit_breaker_timeout == 10
         assert controller2._health_max_backoff_interval == 30
         assert controller2._health_recovery_confirmation_threshold == 4
 
     @pytest.mark.asyncio
-    async def test_circuit_breaker_recovery_confirmation_threshold(self, controller_fast_timers, mock_session):
+    async def test_circuit_breaker_recovery_confirmation_threshold(
+        self, controller_fast_timers, mock_session
+    ):
         """Test circuit breaker requires N consecutive successes before full reset."""
         controller = controller_fast_timers
         controller._session = mock_session
-        
+
         # Configure for 2 consecutive successes required for recovery
         controller._health_recovery_confirmation_threshold = 2
-        
+
         # Mock sequence: failures (trigger CB) → timeout → success → failure → success → success (full recovery)
         failure_response = self._mock_response(500, text_data="Service Unavailable")
-        success_response = self._mock_response(200, {"serverVersion": "1.0.0", "serverUptime": 1200})
-        
+        success_response = self._mock_response(
+            200, {"serverVersion": "1.0.0", "serverUptime": 1200}
+        )
+
         responses = [
-            failure_response, failure_response, failure_response,  # Trigger circuit breaker
-            success_response,   # First success during recovery
-            failure_response,   # Failure interrupts recovery 
-            success_response,   # Success again
-            success_response    # Second consecutive success - should fully recover
+            failure_response,
+            failure_response,
+            failure_response,  # Trigger circuit breaker
+            success_response,  # First success during recovery
+            failure_response,  # Failure interrupts recovery
+            success_response,  # Success again
+            success_response,  # Second consecutive success - should fully recover
         ]
         mock_session.get.side_effect = responses
-        
+
         await controller.start()
         await asyncio.sleep(0.6)  # Let health checks run through sequence
         await controller.stop()
-        
+
         # Verify circuit breaker was activated and recovered
-        assert controller._health_state['circuit_breaker_activations'] > 0
-        assert controller._health_state['recovery_count'] > 0
-
-    @pytest.mark.asyncio
-    async def test_recovery_confirmation_reset_on_failure(self, controller_fast_timers, mock_session, caplog):
+        assert controller._health_state["circuit_breaker_activations"] > 0
+        assert controller._health_state["recovery_count"] > 0
+
+    @pytest.mark.asyncio
+    async def test_recovery_confirmation_reset_on_failure(
+        self, controller_fast_timers, mock_session, caplog
+    ):
         """Test recovery confirmation progress resets when failure occurs during recovery."""
         controller = controller_fast_timers
         controller._session = mock_session
         controller._health_recovery_confirmation_threshold = 3
-        
+
         failure_response = self._mock_response(500, text_data="Error")
         success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+
         # Pattern: failures → CB timeout → success → success → failure → success (restart confirmation)
-        responses = [failure_response] * 4 + [success_response, success_response, failure_response, success_response] * 3
+        responses = [failure_response] * 4 + [
+            success_response,
+            success_response,
+            failure_response,
+            success_response,
+        ] * 3
         mock_session.get.side_effect = responses
-        
-        with caplog.at_level('INFO'):
+
+        with caplog.at_level("INFO"):
             await controller.start()
             await asyncio.sleep(0.8)
             await controller.stop()
-        
+
         # Verify partial recovery logging
         log_messages = [record.message for record in caplog.records]
         improving_logs = [msg for msg in log_messages if "IMPROVING" in msg]
         assert len(improving_logs) > 0, "Should log partial recovery progress"
 
     @pytest.mark.asyncio
-    async def test_health_check_backoff_calculation(self, controller_fast_timers, mock_session):
+    async def test_health_check_backoff_calculation(
+        self, controller_fast_timers, mock_session
+    ):
         """Test exponential backoff calculation with configurable parameters."""
         controller = controller_fast_timers
         controller._session = mock_session
-        
+
         # Mock failing health checks
         mock_session.get.side_effect = aiohttp.ClientError("Connection refused")
-        
+
         # Record sleep intervals to verify backoff
         sleep_intervals = []
         original_sleep = asyncio.sleep
-        
+
         async def mock_sleep(interval):
             sleep_intervals.append(interval)
             # Use very short actual sleep for test speed
             await original_sleep(0.001)
-        
-        with patch('asyncio.sleep', side_effect=mock_sleep):
+
+        with patch("asyncio.sleep", side_effect=mock_sleep):
             await controller.start()
             await asyncio.sleep(0.1)  # Let some checks run
             await controller.stop()
-        
+
         # Verify backoff intervals increase exponentially
         if len(sleep_intervals) >= 2:
             # Should see increasing intervals (allowing for circuit breaker waits)
-            health_check_intervals = [interval for interval in sleep_intervals 
-                                    if interval >= controller._health_check_interval]
+            health_check_intervals = [
+                interval
+                for interval in sleep_intervals
+                if interval >= controller._health_check_interval
+            ]
             if len(health_check_intervals) >= 2:
                 assert health_check_intervals[1] > health_check_intervals[0]
 
     @pytest.mark.asyncio
-    async def test_health_state_transition_logging(self, controller_fast_timers, mock_session, caplog):
+    async def test_health_state_transition_logging(
+        self, controller_fast_timers, mock_session, caplog
+    ):
         """Test health state transitions are logged with context."""
         controller = controller_fast_timers
         controller._session = mock_session
-        
+
         # Mock transition from failure to success
         failure_response = self._mock_response(500, text_data="Service Unavailable")
-        success_response = self._mock_response(200, {
-            "serverVersion": "1.0.0", 
-            "serverUptime": 1200
-        })
-        
-        mock_session.get.side_effect = [failure_response, success_response, success_response]
-        
-        with caplog.at_level('INFO'):
+        success_response = self._mock_response(
+            200, {"serverVersion": "1.0.0", "serverUptime": 1200}
+        )
+
+        mock_session.get.side_effect = [
+            failure_response,
+            success_response,
+            success_response,
+        ]
+
+        with caplog.at_level("INFO"):
             await controller.start()
             await asyncio.sleep(0.3)  # Let health checks run
             await controller.stop()
-        
+
         # Verify transition logging
         log_messages = [record.message for record in caplog.records]
-        
+
         # Should see health degradation and recovery messages
         degraded_logs = [msg for msg in log_messages if "DEGRADED" in msg]
         recovered_logs = [msg for msg in log_messages if "RECOVERED" in msg]
-        
+
         assert len(degraded_logs) > 0, "Should log health degradation"
         # Recovery may not occur in short test time, but degradation should be logged
 
     @pytest.mark.asyncio
     async def test_configurable_recovery_confirmation_threshold(self):
         """Test recovery confirmation threshold is configurable, not hardcoded."""
         # Test with different recovery confirmation thresholds
         controller1 = MediaMTXController(
-            host="localhost", api_port=9997, rtsp_port=8554,
-            webrtc_port=8889, hls_port=8888, config_path="/tmp/config.yml",
-            recordings_path="/tmp/recordings", snapshots_path="/tmp/snapshots",
-            health_recovery_confirmation_threshold=1  # Immediate recovery (old behavior)
-        )
-        
+            host="localhost",
+            api_port=9997,
+            rtsp_port=8554,
+            webrtc_port=8889,
+            hls_port=8888,
+            config_path="/tmp/config.yml",
+            recordings_path="/tmp/recordings",
+            snapshots_path="/tmp/snapshots",
+            health_recovery_confirmation_threshold=1,  # Immediate recovery (old behavior)
+        )
+
         controller2 = MediaMTXController(
-            host="localhost", api_port=9997, rtsp_port=8554,
-            webrtc_port=8889, hls_port=8888, config_path="/tmp/config.yml", 
-            recordings_path="/tmp/recordings", snapshots_path="/tmp/snapshots",
-            health_recovery_confirmation_threshold=5  # Conservative recovery
-        )
-        
+            host="localhost",
+            api_port=9997,
+            rtsp_port=8554,
+            webrtc_port=8889,
+            hls_port=8888,
+            config_path="/tmp/config.yml",
+            recordings_path="/tmp/recordings",
+            snapshots_path="/tmp/snapshots",
+            health_recovery_confirmation_threshold=5,  # Conservative recovery
+        )
+
         # Verify different controllers use their configured values
         assert controller1._health_recovery_confirmation_threshold == 1
         assert controller2._health_recovery_confirmation_threshold == 5
-        
+
         # Verify initial state includes recovery confirmation tracking
-        assert 'consecutive_successes_during_recovery' in controller1._health_state
-        assert 'consecutive_successes_during_recovery' in controller2._health_state
-
-    @pytest.mark.asyncio 
-    async def test_health_check_success_resets_failure_count(self, controller_fast_timers, mock_session):
+        assert "consecutive_successes_during_recovery" in controller1._health_state
+        assert "consecutive_successes_during_recovery" in controller2._health_state
+
+    @pytest.mark.asyncio
+    async def test_health_check_success_resets_failure_count(
+        self, controller_fast_timers, mock_session
+    ):
         """Test successful health check resets consecutive failure count."""
         controller = controller_fast_timers
         controller._session = mock_session
-        
+
         # Mock pattern: fail, fail, succeed, fail
         responses = [
             self._mock_response(500, text_data="Error 1"),
-            self._mock_response(500, text_data="Error 2"), 
+            self._mock_response(500, text_data="Error 2"),
             self._mock_response(200, {"serverVersion": "1.0.0"}),
-            self._mock_response(500, text_data="Error 3")
+            self._mock_response(500, text_data="Error 3"),
         ]
         mock_session.get.side_effect = responses
-        
+
         await controller.start()
         await asyncio.sleep(0.4)  # Let health checks run
         await controller.stop()
-        
+
         # After success, failure count should have been reset
         # Final state depends on timing, but we can check that success was registered
-        assert controller._health_state['last_success_time'] > 0
-
-    @pytest.mark.asyncio
-    async def test_health_monitor_cleanup_on_stop(self, controller_fast_timers, mock_session):
+        assert controller._health_state["last_success_time"] > 0
+
+    @pytest.mark.asyncio
+    async def test_health_monitor_cleanup_on_stop(
+        self, controller_fast_timers, mock_session
+    ):
         """Test health monitoring task is properly cancelled on stop."""
         controller = controller_fast_timers
         controller._session = mock_session
-        
+
         # Mock successful responses
-        mock_session.get.return_value = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+        mock_session.get.return_value = self._mock_response(
+            200, {"serverVersion": "1.0.0"}
+        )
+
         await controller.start()
-        
+
         # Verify health check task is running
         assert controller._health_check_task is not None
         assert not controller._health_check_task.done()
-        
+
         await controller.stop()
-        
+
         # Verify task is cancelled/completed
         assert controller._health_check_task.done()
 
     @pytest.mark.asyncio
-    async def test_health_check_correlation_id_propagation(self, controller_fast_timers, mock_session):
+    async def test_health_check_correlation_id_propagation(
+        self, controller_fast_timers, mock_session
+    ):
         """Test correlation IDs are set for health check operations."""
         controller = controller_fast_timers
         controller._session = mock_session
-        
-        mock_session.get.return_value = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+
+        mock_session.get.return_value = self._mock_response(
+            200, {"serverVersion": "1.0.0"}
+        )
+
         # Mock correlation ID functions to capture calls
         correlation_ids = []
-        
+
         def mock_set_correlation_id(cid):
             correlation_ids.append(cid)
-        
-        with patch('src.mediamtx_wrapper.controller.set_correlation_id', side_effect=mock_set_correlation_id):
+
+        with patch(
+            "src.mediamtx_wrapper.controller.set_correlation_id",
+            side_effect=mock_set_correlation_id,
+        ):
             await controller.start()
             await asyncio.sleep(0.2)  # Let health checks run
             await controller.stop()
-        
+
         # Verify correlation IDs were set
         assert len(correlation_ids) > 0
         # Each correlation ID should be a short string
         for cid in correlation_ids:
             assert isinstance(cid, str)
@@ -303,24 +359,34 @@
     @pytest.mark.asyncio
     async def test_jitter_configuration_affects_backoff(self):
         """Test that jitter configuration affects backoff calculation."""
         # Controller with no jitter
         controller_no_jitter = MediaMTXController(
-            host="localhost", api_port=9997, rtsp_port=8554,
-            webrtc_port=8889, hls_port=8888, config_path="/tmp/config.yml",
-            recordings_path="/tmp/recordings", snapshots_path="/tmp/snapshots",
-            backoff_jitter_range=(1.0, 1.0)  # No jitter
-        )
-        
+            host="localhost",
+            api_port=9997,
+            rtsp_port=8554,
+            webrtc_port=8889,
+            hls_port=8888,
+            config_path="/tmp/config.yml",
+            recordings_path="/tmp/recordings",
+            snapshots_path="/tmp/snapshots",
+            backoff_jitter_range=(1.0, 1.0),  # No jitter
+        )
+
         # Controller with wide jitter
         controller_wide_jitter = MediaMTXController(
-            host="localhost", api_port=9997, rtsp_port=8554,
-            webrtc_port=8889, hls_port=8888, config_path="/tmp/config.yml",
-            recordings_path="/tmp/recordings", snapshots_path="/tmp/snapshots", 
-            backoff_jitter_range=(0.5, 1.5)  # ±50% jitter
-        )
-        
+            host="localhost",
+            api_port=9997,
+            rtsp_port=8554,
+            webrtc_port=8889,
+            hls_port=8888,
+            config_path="/tmp/config.yml",
+            recordings_path="/tmp/recordings",
+            snapshots_path="/tmp/snapshots",
+            backoff_jitter_range=(0.5, 1.5),  # ±50% jitter
+        )
+
         # Verify jitter configuration is stored
         assert controller_no_jitter._backoff_jitter_range == (1.0, 1.0)
         assert controller_wide_jitter._backoff_jitter_range == (0.5, 1.5)
 
 
@@ -330,6 +396,6 @@
 # - Mock asyncio.sleep to capture backoff intervals
 # - Use caplog fixture to verify logging behavior
 # - Mock correlation ID functions to verify propagation
 # - Test both circuit breaker activation and recovery
 # - Verify configurable parameters are respected, not hardcoded values
-# - Test proper task cleanup on controller stop
\ No newline at end of file
+# - Test proper task cleanup on controller stop
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_controller_snapshot_capture.py	2025-08-03 19:27:38.812956+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_controller_snapshot_capture.py	2025-08-04 15:35:25.814288+00:00
@@ -31,11 +31,11 @@
                 hls_port=8888,
                 config_path="/tmp/test_config.yml",
                 recordings_path=os.path.join(temp_dir, "recordings"),
                 snapshots_path=os.path.join(temp_dir, "snapshots"),
                 process_termination_timeout=1.0,  # Short timeout for testing
-                process_kill_timeout=0.5
+                process_kill_timeout=0.5,
             )
             # Mock session to avoid HTTP calls
             controller._session = Mock()
             yield controller
 
@@ -46,70 +46,79 @@
         mock_process = Mock()
         mock_process.returncode = None  # Process still running
         mock_process.communicate = AsyncMock(side_effect=asyncio.TimeoutError())
         mock_process.terminate = Mock()
         mock_process.kill = Mock()
-        mock_process.wait = AsyncMock(side_effect=asyncio.TimeoutError())  # Doesn't respond to signals
-        
-        with patch('asyncio.create_subprocess_exec', return_value=mock_process):
-            result = await controller.take_snapshot("test_stream", "test_snapshot.jpg")
-            
+        mock_process.wait = AsyncMock(
+            side_effect=asyncio.TimeoutError()
+        )  # Doesn't respond to signals
+
+        with patch("asyncio.create_subprocess_exec", return_value=mock_process):
+            result = await controller.take_snapshot("test_stream", "test_snapshot.jpg")
+
             # Verify graceful termination then force kill was attempted
             mock_process.terminate.assert_called_once()
             mock_process.kill.assert_called_once()
-            
+
             # Verify error context includes process cleanup information
             assert result["status"] == "failed"
             assert "timeout" in result["error"].lower()
             assert "killed" in result["error"] or "terminated" in result["error"]
 
-    @pytest.mark.asyncio  
+    @pytest.mark.asyncio
     async def test_snapshot_file_size_error_handling(self, controller):
         """Test handling when file exists but size cannot be determined."""
         # Mock successful FFmpeg execution
         mock_process = Mock()
         mock_process.returncode = 0
         mock_process.communicate = AsyncMock(return_value=(b"success", b""))
-        
+
         # Mock file that exists but raises OSError on getsize
-        with patch('asyncio.create_subprocess_exec', return_value=mock_process), \
-             patch('os.path.exists', return_value=True), \
-             patch('os.path.getsize', side_effect=OSError("Permission denied")):
-            
-            result = await controller.take_snapshot("test_stream", "test_snapshot.jpg")
-            
+        with (
+            patch("asyncio.create_subprocess_exec", return_value=mock_process),
+            patch("os.path.exists", return_value=True),
+            patch("os.path.getsize", side_effect=OSError("Permission denied")),
+        ):
+
+            result = await controller.take_snapshot("test_stream", "test_snapshot.jpg")
+
             # Verify successful completion with warning about file size
             assert result["status"] == "completed"
             assert result["file_size"] == 0
             assert "warning" in result
             assert "Could not determine file size" in result["warning"]
 
     @pytest.mark.asyncio
     async def test_snapshot_directory_permission_error(self, controller):
         """Test handling when snapshots directory cannot be created or written to."""
         # Mock permission error when creating directory
-        with patch('os.makedirs', side_effect=PermissionError("Access denied")):
-            result = await controller.take_snapshot("test_stream", "test_snapshot.jpg")
-            
+        with patch("os.makedirs", side_effect=PermissionError("Access denied")):
+            result = await controller.take_snapshot("test_stream", "test_snapshot.jpg")
+
             # Verify graceful error handling
             assert result["status"] == "failed"
             assert "Cannot write to snapshots directory" in result["error"]
-            assert "Permission denied" in result["error"] or "Access denied" in result["error"]
+            assert (
+                "Permission denied" in result["error"]
+                or "Access denied" in result["error"]
+            )
 
     @pytest.mark.asyncio
     async def test_snapshot_ffmpeg_nonzero_exit_code(self, controller):
         """Test handling when FFmpeg exits with error code."""
         # Mock FFmpeg process that fails
         mock_process = Mock()
         mock_process.returncode = 1  # Error exit code
         mock_process.communicate = AsyncMock(return_value=(b"", b"Input/output error"))
-        
-        with patch('asyncio.create_subprocess_exec', return_value=mock_process), \
-             patch('os.path.exists', return_value=False):  # No output file created
-            
-            result = await controller.take_snapshot("test_stream", "test_snapshot.jpg")
-            
+
+        with (
+            patch("asyncio.create_subprocess_exec", return_value=mock_process),
+            patch("os.path.exists", return_value=False),
+        ):  # No output file created
+
+            result = await controller.take_snapshot("test_stream", "test_snapshot.jpg")
+
             # Verify error is properly captured and reported
             assert result["status"] == "failed"
             assert "FFmpeg capture failed" in result["error"]
             assert "Input/output error" in result["error"]
 
@@ -118,21 +127,23 @@
         """Test successful snapshot capture returns accurate metadata."""
         # Mock successful FFmpeg execution
         mock_process = Mock()
         mock_process.returncode = 0
         mock_process.communicate = AsyncMock(return_value=(b"success", b""))
-        
+
         test_file_size = 12345
         test_file_path = os.path.join(controller._snapshots_path, "test_snapshot.jpg")
-        
-        with patch('asyncio.create_subprocess_exec', return_value=mock_process), \
-             patch('os.path.exists', return_value=True), \
-             patch('os.path.getsize', return_value=test_file_size), \
-             patch('os.makedirs'):
-            
-            result = await controller.take_snapshot("test_stream", "test_snapshot.jpg")
-            
+
+        with (
+            patch("asyncio.create_subprocess_exec", return_value=mock_process),
+            patch("os.path.exists", return_value=True),
+            patch("os.path.getsize", return_value=test_file_size),
+            patch("os.makedirs"),
+        ):
+
+            result = await controller.take_snapshot("test_stream", "test_snapshot.jpg")
+
             # Verify accurate metadata
             assert result["status"] == "completed"
             assert result["filename"] == "test_snapshot.jpg"
             assert result["file_size"] == test_file_size
             assert result["file_path"] == test_file_path
@@ -140,36 +151,40 @@
 
     @pytest.mark.asyncio
     async def test_snapshot_process_creation_timeout(self, controller):
         """Test timeout during FFmpeg process creation."""
         # Mock process creation that times out
-        with patch('asyncio.create_subprocess_exec', side_effect=asyncio.TimeoutError()):
-            result = await controller.take_snapshot("test_stream", "test_snapshot.jpg")
-            
+        with patch(
+            "asyncio.create_subprocess_exec", side_effect=asyncio.TimeoutError()
+        ):
+            result = await controller.take_snapshot("test_stream", "test_snapshot.jpg")
+
             # Verify timeout is handled gracefully
             assert result["status"] == "failed"
             assert "timeout" in result["error"].lower()
 
     def test_cleanup_ffmpeg_process_escalation_path(self, controller):
         """Test FFmpeg process cleanup escalation: SIGTERM → SIGKILL."""
         # This would be a unit test of the _cleanup_ffmpeg_process method
         # Testing the escalation logic without actual process creation
-        
+
         # Mock process that doesn't respond to SIGTERM but does to SIGKILL
         mock_process = Mock()
         mock_process.returncode = None
         mock_process.terminate = Mock()
         mock_process.kill = Mock()
-        
+
         # First wait (after SIGTERM) times out, second wait (after SIGKILL) succeeds
         mock_process.wait = AsyncMock(side_effect=[asyncio.TimeoutError(), None])
-        
+
         # Test the cleanup method directly
         cleanup_result = asyncio.run(
-            controller._cleanup_ffmpeg_process(mock_process, "test_stream", "test_correlation")
+            controller._cleanup_ffmpeg_process(
+                mock_process, "test_stream", "test_correlation"
+            )
         )
-        
+
         # Verify escalation path was followed
         mock_process.terminate.assert_called_once()
         mock_process.kill.assert_called_once()
         assert "terminated" in cleanup_result
         assert "killed" in cleanup_result
@@ -180,6 +195,6 @@
 # - Mock asyncio.create_subprocess_exec for FFmpeg process control
 # - Mock os.path.exists and os.path.getsize for file system operations
 # - Mock os.makedirs for directory creation testing
 # - Use temporary directories for file system tests
 # - Test both success and failure paths for robustness
-# - Verify correlation IDs are properly set in error logging
\ No newline at end of file
+# - Verify correlation IDs are properly set in error logging
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_controller_recording_duration.py	2025-08-03 19:27:38.812956+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_controller_recording_duration.py	2025-08-04 15:35:25.894315+00:00
@@ -1,10 +1,10 @@
 # tests/unit/test_mediamtx_wrapper/test_controller_recording_duration.py
 """
 Test recording lifecycle duration calculation and file handling robustness.
 
-Test policy: Verify accurate duration computation, graceful handling of 
+Test policy: Verify accurate duration computation, graceful handling of
 missing files, permission errors, and proper session management.
 """
 
 import pytest
 import asyncio
@@ -30,46 +30,52 @@
                 rtsp_port=8554,
                 webrtc_port=8889,
                 hls_port=8888,
                 config_path="/tmp/test_config.yml",
                 recordings_path=os.path.join(temp_dir, "recordings"),
-                snapshots_path=os.path.join(temp_dir, "snapshots")
+                snapshots_path=os.path.join(temp_dir, "snapshots"),
             )
             # Mock session for HTTP calls
             controller._session = Mock()
             yield controller
 
     @pytest.fixture
     def mock_http_success(self):
         """Mock successful HTTP responses."""
+
         def _mock_response(status=200, json_data=None):
             response = Mock()
             response.status = status
             response.json = AsyncMock(return_value=json_data or {})
             response.text = AsyncMock(return_value="")
             return response
+
         return _mock_response
 
     @pytest.mark.asyncio
-    async def test_recording_duration_calculation_precision(self, controller, mock_http_success):
+    async def test_recording_duration_calculation_precision(
+        self, controller, mock_http_success
+    ):
         """Test accurate duration calculation using session timestamps."""
         # Mock successful HTTP responses for start and stop
         controller._session.post = AsyncMock(return_value=mock_http_success())
-        
+
         # Start recording and capture start time
         start_time = time.time()
         await controller.start_recording("test_stream", duration=3600, format="mp4")
-        
+
         # Simulate passage of time
         test_duration = 123  # 123 seconds
-        with patch('time.time', return_value=start_time + test_duration):
+        with patch("time.time", return_value=start_time + test_duration):
             # Mock file exists and has size
-            with patch('os.path.exists', return_value=True), \
-                 patch('os.path.getsize', return_value=1024000):
-                
+            with (
+                patch("os.path.exists", return_value=True),
+                patch("os.path.getsize", return_value=1024000),
+            ):
+
                 result = await controller.stop_recording("test_stream")
-        
+
         # Verify duration calculation is accurate
         assert result["duration"] == test_duration
         assert result["status"] == "completed"
         assert abs(result["duration"] - test_duration) <= 1  # Allow 1 second tolerance
 
@@ -77,15 +83,15 @@
     async def test_recording_missing_file_handling(self, controller, mock_http_success):
         """Test stop_recording when file doesn't exist on disk."""
         # Setup recording session
         controller._session.post = AsyncMock(return_value=mock_http_success())
         await controller.start_recording("test_stream", format="mp4")
-        
+
         # Mock file doesn't exist
-        with patch('os.path.exists', return_value=False):
+        with patch("os.path.exists", return_value=False):
             result = await controller.stop_recording("test_stream")
-        
+
         # Verify graceful handling of missing file
         assert result["status"] == "completed"
         assert result["file_exists"] is False
         assert result["file_size"] == 0
         assert "file_warning" in result
@@ -95,56 +101,69 @@
     async def test_recording_file_permission_error(self, controller, mock_http_success):
         """Test handling when file exists but cannot be accessed due to permissions."""
         # Setup recording session
         controller._session.post = AsyncMock(return_value=mock_http_success())
         await controller.start_recording("test_stream", format="mp4")
-        
+
         # Mock file exists but permission error on getsize
-        with patch('os.path.exists', return_value=True), \
-             patch('os.path.getsize', side_effect=PermissionError("Access denied")):
-            
+        with (
+            patch("os.path.exists", return_value=True),
+            patch("os.path.getsize", side_effect=PermissionError("Access denied")),
+        ):
+
             result = await controller.stop_recording("test_stream")
-        
+
         # Verify graceful error handling
         assert result["status"] == "completed"
         assert result["file_exists"] is True
         assert result["file_size"] == 0
         assert "file_warning" in result
         assert "Permission denied" in result["file_warning"]
 
     @pytest.mark.asyncio
-    async def test_recording_directory_creation_permission_error(self, controller, mock_http_success):
+    async def test_recording_directory_creation_permission_error(
+        self, controller, mock_http_success
+    ):
         """Test handling when recordings directory cannot be created."""
         # Mock permission error when creating directory
-        with patch('os.makedirs', side_effect=PermissionError("Access denied")), \
-             patch('tempfile.NamedTemporaryFile', side_effect=PermissionError("Access denied")):
-            
+        with (
+            patch("os.makedirs", side_effect=PermissionError("Access denied")),
+            patch(
+                "tempfile.NamedTemporaryFile",
+                side_effect=PermissionError("Access denied"),
+            ),
+        ):
+
             # Attempt to start recording
-            with pytest.raises(ValueError, match="Cannot write to recordings directory"):
+            with pytest.raises(
+                ValueError, match="Cannot write to recordings directory"
+            ):
                 await controller.start_recording("test_stream", format="mp4")
 
     @pytest.mark.asyncio
     async def test_recording_session_management(self, controller, mock_http_success):
         """Test recording session tracking and cleanup."""
         controller._session.post = AsyncMock(return_value=mock_http_success())
-        
+
         # Start recording - should create session
         await controller.start_recording("test_stream", format="mp4")
         assert "test_stream" in controller._recording_sessions
-        
+
         # Verify session contains required fields
         session = controller._recording_sessions["test_stream"]
         assert "start_time" in session
         assert "filename" in session
         assert "record_path" in session
         assert "correlation_id" in session
-        
+
         # Stop recording - should clean up session
-        with patch('os.path.exists', return_value=True), \
-             patch('os.path.getsize', return_value=1024):
+        with (
+            patch("os.path.exists", return_value=True),
+            patch("os.path.getsize", return_value=1024),
+        ):
             await controller.stop_recording("test_stream")
-        
+
         # Session should be cleaned up
         assert "test_stream" not in controller._recording_sessions
 
     @pytest.mark.asyncio
     async def test_recording_api_failure_preserves_session(self, controller):
@@ -152,65 +171,67 @@
         # Start recording successfully
         success_response = Mock()
         success_response.status = 200
         controller._session.post = AsyncMock(return_value=success_response)
         await controller.start_recording("test_stream", format="mp4")
-        
+
         # Mock API failure during stop
         failure_response = Mock()
         failure_response.status = 500
         failure_response.text = AsyncMock(return_value="Internal Server Error")
         controller._session.post = AsyncMock(return_value=failure_response)
-        
+
         # Attempt to stop recording
         with pytest.raises(ValueError, match="Failed to stop recording"):
             await controller.stop_recording("test_stream")
-        
+
         # Session should still exist for retry
         assert "test_stream" in controller._recording_sessions
 
     @pytest.mark.asyncio
     async def test_recording_duplicate_start_error(self, controller, mock_http_success):
         """Test error when trying to start recording on already recording stream."""
         controller._session.post = AsyncMock(return_value=mock_http_success())
-        
+
         # Start first recording
         await controller.start_recording("test_stream", format="mp4")
-        
+
         # Attempt to start second recording on same stream
         with pytest.raises(ValueError, match="Recording already active"):
             await controller.start_recording("test_stream", format="mp4")
 
     @pytest.mark.asyncio
     async def test_recording_stop_without_start_error(self, controller):
         """Test error when trying to stop recording that was never started."""
         # Mock session for stop request
         controller._session = Mock()
-        
+
         # Attempt to stop recording without starting
         with pytest.raises(ValueError, match="No active recording session found"):
             await controller.stop_recording("test_stream")
 
     @pytest.mark.asyncio
     async def test_recording_format_validation(self, controller):
         """Test validation of recording format parameter."""
         # Test invalid format
         with pytest.raises(ValueError, match="Invalid format.*Must be one of"):
             await controller.start_recording("test_stream", format="invalid")
-        
+
         # Test valid formats
         valid_formats = ["mp4", "mkv", "avi"]
         for format_type in valid_formats:
             # This would normally require mocking the HTTP call
             # but we're just testing the validation doesn't raise
             try:
                 controller._session = Mock()
                 success_response = Mock()
                 success_response.status = 200
                 controller._session.post = AsyncMock(return_value=success_response)
-                
-                await controller.start_recording(f"test_stream_{format_type}", format=format_type)
+
+                await controller.start_recording(
+                    f"test_stream_{format_type}", format=format_type
+                )
                 # Clean up for next iteration
                 await controller.stop_recording(f"test_stream_{format_type}")
             except ValueError as e:
                 if "Invalid format" in str(e):
                     pytest.fail(f"Valid format {format_type} was rejected")
@@ -222,6 +243,6 @@
 # - Mock os.path.exists and os.path.getsize for file operations
 # - Mock os.makedirs for directory creation testing
 # - Use temporary directories for file system tests
 # - Test session management and cleanup
 # - Verify correlation IDs in session data
-# - Test both success and failure scenarios
\ No newline at end of file
+# - Test both success and failure scenarios
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_controller_stream_operations.py	2025-08-03 19:27:38.812956+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_controller_stream_operations.py	2025-08-04 15:35:26.156382+00:00
@@ -1,10 +1,10 @@
 # tests/unit/test_mediamtx_wrapper/test_controller_stream_operations.py
 """
 Test stream creation/deletion idempotent behavior and error handling.
 
-Test policy: Verify idempotent operations, clear error contexts, and 
+Test policy: Verify idempotent operations, clear error contexts, and
 reliability under transient failures.
 """
 
 import pytest
 import asyncio
@@ -26,24 +26,20 @@
             rtsp_port=8554,
             webrtc_port=8889,
             hls_port=8888,
             config_path="/tmp/test_config.yml",
             recordings_path="/tmp/recordings",
-            snapshots_path="/tmp/snapshots"
+            snapshots_path="/tmp/snapshots",
         )
         # Mock session
         controller._session = Mock()
         return controller
 
     @pytest.fixture
     def sample_stream_config(self):
         """Create sample stream configuration."""
-        return StreamConfig(
-            name="test_stream",
-            source="/dev/video0",
-            record=False
-        )
+        return StreamConfig(name="test_stream", source="/dev/video0", record=False)
 
     def _mock_response(self, status, json_data=None, text_data=""):
         """Helper to create mock HTTP response."""
         response = Mock()
         response.status = status
@@ -55,80 +51,92 @@
     async def test_create_stream_success(self, controller, sample_stream_config):
         """Test successful stream creation returns correct URLs."""
         # Mock successful response
         success_response = self._mock_response(200)
         controller._session.post = AsyncMock(return_value=success_response)
-        
+
         # Mock get_stream_status to return stream doesn't exist (for idempotency check)
-        controller.get_stream_status = AsyncMock(side_effect=ValueError("Stream not found"))
-        
+        controller.get_stream_status = AsyncMock(
+            side_effect=ValueError("Stream not found")
+        )
+
         result = await controller.create_stream(sample_stream_config)
-        
+
         # Verify URLs are correctly generated
         expected_urls = {
             "rtsp": "rtsp://localhost:8554/test_stream",
             "webrtc": "http://localhost:8889/test_stream",
-            "hls": "http://localhost:8888/test_stream"
+            "hls": "http://localhost:8888/test_stream",
         }
         assert result == expected_urls
 
     @pytest.mark.asyncio
-    async def test_create_stream_idempotent_behavior(self, controller, sample_stream_config):
+    async def test_create_stream_idempotent_behavior(
+        self, controller, sample_stream_config
+    ):
         """Test that creating existing stream returns URLs without error."""
         # Mock get_stream_status to return existing stream
         controller.get_stream_status = AsyncMock(return_value={"name": "test_stream"})
-        
+
         result = await controller.create_stream(sample_stream_config)
-        
+
         # Should return URLs without making create API call
         expected_urls = {
             "rtsp": "rtsp://localhost:8554/test_stream",
-            "webrtc": "http://localhost:8889/test_stream", 
-            "hls": "http://localhost:8888/test_stream"
+            "webrtc": "http://localhost:8889/test_stream",
+            "hls": "http://localhost:8888/test_stream",
         }
         assert result == expected_urls
 
     @pytest.mark.asyncio
-    async def test_create_stream_conflict_409_idempotent(self, controller, sample_stream_config):
+    async def test_create_stream_conflict_409_idempotent(
+        self, controller, sample_stream_config
+    ):
         """Test 409 conflict response is handled idempotently."""
         # Mock get_stream_status to indicate stream doesn't exist initially
-        controller.get_stream_status = AsyncMock(side_effect=ValueError("Stream not found"))
-        
+        controller.get_stream_status = AsyncMock(
+            side_effect=ValueError("Stream not found")
+        )
+
         # Mock 409 conflict response from create call
         conflict_response = self._mock_response(409, text_data="Path already exists")
         controller._session.post = AsyncMock(return_value=conflict_response)
-        
+
         result = await controller.create_stream(sample_stream_config)
-        
+
         # Should return URLs despite 409 conflict
         assert "rtsp" in result
         assert "test_stream" in result["rtsp"]
 
     @pytest.mark.asyncio
     async def test_create_stream_validation_errors(self, controller):
         """Test stream configuration validation."""
         # Test missing name
         with pytest.raises(ValueError, match="Stream name and source are required"):
             await controller.create_stream(StreamConfig(name="", source="/dev/video0"))
-        
+
         # Test missing source
         with pytest.raises(ValueError, match="Stream name and source are required"):
             await controller.create_stream(StreamConfig(name="test", source=""))
 
     @pytest.mark.asyncio
-    async def test_create_stream_api_error_with_context(self, controller, sample_stream_config):
+    async def test_create_stream_api_error_with_context(
+        self, controller, sample_stream_config
+    ):
         """Test API error includes detailed context information."""
         # Mock get_stream_status to indicate stream doesn't exist
-        controller.get_stream_status = AsyncMock(side_effect=ValueError("Stream not found"))
-        
+        controller.get_stream_status = AsyncMock(
+            side_effect=ValueError("Stream not found")
+        )
+
         # Mock API error response
         error_response = self._mock_response(500, text_data="Internal Server Error")
         controller._session.post = AsyncMock(return_value=error_response)
-        
+
         with pytest.raises(ConnectionError) as exc_info:
             await controller.create_stream(sample_stream_config)
-        
+
         # Verify error context includes stream details
         error_msg = str(exc_info.value)
         assert "test_stream" in error_msg
         assert "/dev/video0" in error_msg
         assert "record=False" in error_msg
@@ -136,38 +144,42 @@
 
     @pytest.mark.asyncio
     async def test_create_stream_network_error(self, controller, sample_stream_config):
         """Test network connectivity error handling."""
         # Mock get_stream_status to indicate stream doesn't exist
-        controller.get_stream_status = AsyncMock(side_effect=ValueError("Stream not found"))
-        
+        controller.get_stream_status = AsyncMock(
+            side_effect=ValueError("Stream not found")
+        )
+
         # Mock network error
-        controller._session.post = AsyncMock(side_effect=aiohttp.ClientError("Connection refused"))
-        
+        controller._session.post = AsyncMock(
+            side_effect=aiohttp.ClientError("Connection refused")
+        )
+
         with pytest.raises(ConnectionError, match="MediaMTX unreachable"):
             await controller.create_stream(sample_stream_config)
 
     @pytest.mark.asyncio
     async def test_delete_stream_success(self, controller):
         """Test successful stream deletion."""
         # Mock successful deletion response
         success_response = self._mock_response(200)
         controller._session.post = AsyncMock(return_value=success_response)
-        
+
         result = await controller.delete_stream("test_stream")
-        
+
         assert result is True
 
     @pytest.mark.asyncio
     async def test_delete_stream_idempotent_404(self, controller):
         """Test 404 response is handled idempotently (stream already deleted)."""
         # Mock 404 not found response
         not_found_response = self._mock_response(404, text_data="Path not found")
         controller._session.post = AsyncMock(return_value=not_found_response)
-        
+
         result = await controller.delete_stream("nonexistent_stream")
-        
+
         # Should return True (idempotent - stream already doesn't exist)
         assert result is True
 
     @pytest.mark.asyncio
     async def test_delete_stream_validation_error(self, controller):
@@ -179,71 +191,77 @@
     async def test_delete_stream_api_error(self, controller):
         """Test API error during deletion."""
         # Mock API error response
         error_response = self._mock_response(500, text_data="Internal Server Error")
         controller._session.post = AsyncMock(return_value=error_response)
-        
+
         result = await controller.delete_stream("test_stream")
-        
+
         # Should return False on API error (not idempotent case)
         assert result is False
 
     @pytest.mark.asyncio
     async def test_delete_stream_network_error(self, controller):
         """Test network error during deletion."""
         # Mock network error
-        controller._session.post = AsyncMock(side_effect=aiohttp.ClientError("Connection refused"))
-        
+        controller._session.post = AsyncMock(
+            side_effect=aiohttp.ClientError("Connection refused")
+        )
+
         with pytest.raises(ConnectionError, match="MediaMTX unreachable"):
             await controller.delete_stream("test_stream")
 
     @pytest.mark.asyncio
-    async def test_stream_operations_without_session(self, controller, sample_stream_config):
+    async def test_stream_operations_without_session(
+        self, controller, sample_stream_config
+    ):
         """Test operations fail gracefully when controller not started."""
         # Remove session to simulate unstarted controller
         controller._session = None
-        
+
         with pytest.raises(ConnectionError, match="MediaMTX controller not started"):
             await controller.create_stream(sample_stream_config)
-        
+
         with pytest.raises(ConnectionError, match="MediaMTX controller not started"):
             await controller.delete_stream("test_stream")
 
     @pytest.mark.asyncio
     async def test_stream_config_with_recording(self, controller):
         """Test stream configuration with recording enabled."""
         # Mock get_stream_status to indicate stream doesn't exist
-        controller.get_stream_status = AsyncMock(side_effect=ValueError("Stream not found"))
-        
+        controller.get_stream_status = AsyncMock(
+            side_effect=ValueError("Stream not found")
+        )
+
         # Mock successful response
         success_response = self._mock_response(200)
         controller._session.post = AsyncMock(return_value=success_response)
-        
+
         recording_config = StreamConfig(
             name="recording_stream",
             source="/dev/video1",
             record=True,
-            record_path="/tmp/recordings/test.mp4"
-        )
-        
+            record_path="/tmp/recordings/test.mp4",
+        )
+
         await controller.create_stream(recording_config)
-        
+
         # Verify API call was made with correct recording configuration
         call_args = controller._session.post.call_args
         assert call_args is not None
-        json_data = call_args.kwargs.get('json', {})
-        assert json_data.get('record') is True
-        assert json_data.get('recordPath') == "/tmp/recordings/test.mp4"
+        json_data = call_args.kwargs.get("json", {})
+        assert json_data.get("record") is True
+        assert json_data.get("recordPath") == "/tmp/recordings/test.mp4"
 
     def test_generate_stream_urls_format(self, controller):
         """Test stream URL generation format."""
         urls = controller._generate_stream_urls("test_stream")
-        
+
         expected_urls = {
             "rtsp": "rtsp://localhost:8554/test_stream",
             "webrtc": "http://localhost:8889/test_stream",
-            "hls": "http://localhost:8888/test_stream"
+            "hls": "http://localhost:8888/test_stream",
         }
         assert urls == expected_urls
 
 
 # Test configuration expectations:
@@ -251,6 +269,6 @@
 # - Mock get_stream_status method for idempotency testing
 # - Test both successful and error response scenarios
 # - Verify error messages include relevant context
 # - Test validation of input parameters
 # - Test idempotent behavior for both operations
-# - Verify correlation IDs are set for logging
\ No newline at end of file
+# - Verify correlation IDs are set for logging
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_health_monitor_circuit_breaker_flapping.py	2025-08-04 14:34:56.058706+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_health_monitor_circuit_breaker_flapping.py	2025-08-04 15:35:26.219021+00:00
@@ -1,10 +1,10 @@
 # tests/unit/test_mediamtx_wrapper/test_health_monitor_circuit_breaker_flapping.py
 """
 Test health monitoring circuit breaker flapping scenarios and edge cases.
 
-Test policy: Verify circuit breaker stability under alternating success/failure 
+Test policy: Verify circuit breaker stability under alternating success/failure
 patterns to prevent oscillation and ensure proper recovery confirmation logic.
 """
 
 import pytest
 import asyncio
@@ -35,14 +35,14 @@
             health_failure_threshold=3,
             health_circuit_breaker_timeout=0.3,
             health_max_backoff_interval=1.0,
             health_recovery_confirmation_threshold=3,
             backoff_base_multiplier=2.0,
-            backoff_jitter_range=(1.0, 1.0)  # No jitter for predictable testing
+            backoff_jitter_range=(1.0, 1.0),  # No jitter for predictable testing
         )
 
-    @pytest.fixture  
+    @pytest.fixture
     def mock_session(self):
         """Create mock aiohttp session."""
         session = Mock()
         session.get = AsyncMock()
         session.close = AsyncMock()
@@ -55,66 +55,75 @@
         response.json = AsyncMock(return_value=json_data or {})
         response.text = AsyncMock(return_value=text_data)
         return response
 
     @pytest.mark.asyncio
-    async def test_circuit_breaker_activation_threshold(self, controller_fast_timers, mock_session):
+    async def test_circuit_breaker_activation_threshold(
+        self, controller_fast_timers, mock_session
+    ):
         """Test circuit breaker opens exactly at configured failure threshold."""
         controller = controller_fast_timers
         controller._session = mock_session
-        
+
         failure_response = self._mock_response(500, text_data="Service Error")
         success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+
         # Pattern: 2 failures (below threshold) → 1 success → 3 failures (trigger CB)
         responses = [
-            failure_response, failure_response,  # 2 failures - should not trigger CB
-            success_response,                    # Reset consecutive failures
-            failure_response, failure_response, failure_response  # 3 failures - should trigger CB
-        ]
-        mock_session.get.side_effect = responses
-        
+            failure_response,
+            failure_response,  # 2 failures - should not trigger CB
+            success_response,  # Reset consecutive failures
+            failure_response,
+            failure_response,
+            failure_response,  # 3 failures - should trigger CB
+        ]
+        mock_session.get.side_effect = responses
+
         await controller.start()
         await asyncio.sleep(0.4)  # Let sequence run
         await controller.stop()
-        
+
         # Verify circuit breaker activated exactly once
-        assert controller._health_state['circuit_breaker_activations'] == 1
-        assert controller._health_state['consecutive_failures'] >= 3
-
-    @pytest.mark.asyncio
-    async def test_flapping_resistance_during_recovery(self, controller_fast_timers, mock_session, caplog):
+        assert controller._health_state["circuit_breaker_activations"] == 1
+        assert controller._health_state["consecutive_failures"] >= 3
+
+    @pytest.mark.asyncio
+    async def test_flapping_resistance_during_recovery(
+        self, controller_fast_timers, mock_session, caplog
+    ):
         """Test circuit breaker resists flapping during recovery phase."""
         controller = controller_fast_timers
         controller._session = mock_session
         controller._health_recovery_confirmation_threshold = 3
-        
-        failure_response = self._mock_response(500, text_data="Error")
-        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+
+        failure_response = self._mock_response(500, text_data="Error")
+        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
+
         # Pattern: failures → CB → timeout → alternating success/failure (should not fully recover)
         responses = [
-            failure_response, failure_response, failure_response,  # Trigger CB
-            success_response,   # 1st success during recovery
-            failure_response,   # Reset confirmation counter
-            success_response,   # 1st success again  
-            failure_response,   # Reset confirmation counter again
-            success_response,   # 1st success yet again
-            success_response,   # 2nd consecutive success
-            success_response    # 3rd consecutive success - should fully recover
-        ]
-        mock_session.get.side_effect = responses
-        
-        with caplog.at_level('INFO'):
+            failure_response,
+            failure_response,
+            failure_response,  # Trigger CB
+            success_response,  # 1st success during recovery
+            failure_response,  # Reset confirmation counter
+            success_response,  # 1st success again
+            failure_response,  # Reset confirmation counter again
+            success_response,  # 1st success yet again
+            success_response,  # 2nd consecutive success
+            success_response,  # 3rd consecutive success - should fully recover
+        ]
+        mock_session.get.side_effect = responses
+
+        with caplog.at_level("INFO"):
             await controller.start()
             await asyncio.sleep(0.6)  # Let recovery sequence run
             await controller.stop()
-        
+
         # Verify circuit breaker eventually recovered after stable successes
-        assert controller._health_state['circuit_breaker_activations'] == 1
-        assert controller._health_state['recovery_count'] == 1
-        
+        assert controller._health_state["circuit_breaker_activations"] == 1
+        assert controller._health_state["recovery_count"] == 1
+
         # Verify intermediate "IMPROVING" logs during partial recovery
         log_messages = [record.message for record in caplog.records]
         improving_logs = [msg for msg in log_messages if "IMPROVING" in msg]
         assert len(improving_logs) >= 2, "Should log multiple partial recovery attempts"
 
@@ -122,116 +131,146 @@
     async def test_rapid_flapping_scenario(self, controller_fast_timers, mock_session):
         """Test circuit breaker behavior under rapid success/failure alternation."""
         controller = controller_fast_timers
         controller._session = mock_session
         controller._health_recovery_confirmation_threshold = 2
-        
+
         failure_response = self._mock_response(503, text_data="Unavailable")
         success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+
         # Pattern: failures → CB → rapid alternation → eventual stable recovery
         responses = [
-            failure_response, failure_response, failure_response,  # Trigger CB
+            failure_response,
+            failure_response,
+            failure_response,  # Trigger CB
             # Rapid alternation during recovery (10 cycles)
-            success_response, failure_response,  # Reset
-            success_response, failure_response,  # Reset
-            success_response, failure_response,  # Reset
-            success_response, failure_response,  # Reset
-            success_response, failure_response,  # Reset
+            success_response,
+            failure_response,  # Reset
+            success_response,
+            failure_response,  # Reset
+            success_response,
+            failure_response,  # Reset
+            success_response,
+            failure_response,  # Reset
+            success_response,
+            failure_response,  # Reset
             # Finally stable recovery
-            success_response, success_response   # Should fully recover
-        ]
-        mock_session.get.side_effect = responses
-        
+            success_response,
+            success_response,  # Should fully recover
+        ]
+        mock_session.get.side_effect = responses
+
         await controller.start()
         await asyncio.sleep(0.8)  # Extended time for rapid sequence
         await controller.stop()
-        
+
         # Verify circuit breaker stayed stable during flapping
-        assert controller._health_state['circuit_breaker_activations'] == 1
-        assert controller._health_state['recovery_count'] == 1
+        assert controller._health_state["circuit_breaker_activations"] == 1
+        assert controller._health_state["recovery_count"] == 1
         # Consecutive successes should be reset to 0 after recovery
-        assert controller._health_state['consecutive_successes_during_recovery'] == 0
-
-    @pytest.mark.asyncio
-    async def test_multiple_circuit_breaker_cycles(self, controller_fast_timers, mock_session):
+        assert controller._health_state["consecutive_successes_during_recovery"] == 0
+
+    @pytest.mark.asyncio
+    async def test_multiple_circuit_breaker_cycles(
+        self, controller_fast_timers, mock_session
+    ):
         """Test multiple circuit breaker activation/recovery cycles."""
         controller = controller_fast_timers
         controller._session = mock_session
         controller._health_recovery_confirmation_threshold = 2
-        
-        failure_response = self._mock_response(500, text_data="Error")
-        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+
+        failure_response = self._mock_response(500, text_data="Error")
+        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
+
         # Pattern: CB cycle 1 → recovery → CB cycle 2 → recovery
         responses = [
             # First CB cycle
-            failure_response, failure_response, failure_response,  # Trigger CB #1
-            success_response, success_response,                    # Recover from CB #1
-            # Second CB cycle  
-            failure_response, failure_response, failure_response,  # Trigger CB #2
-            success_response, success_response                     # Recover from CB #2
-        ]
-        mock_session.get.side_effect = responses
-        
+            failure_response,
+            failure_response,
+            failure_response,  # Trigger CB #1
+            success_response,
+            success_response,  # Recover from CB #1
+            # Second CB cycle
+            failure_response,
+            failure_response,
+            failure_response,  # Trigger CB #2
+            success_response,
+            success_response,  # Recover from CB #2
+        ]
+        mock_session.get.side_effect = responses
+
         await controller.start()
         await asyncio.sleep(0.8)  # Extended time for two cycles
         await controller.stop()
-        
+
         # Verify both circuit breaker cycles occurred
-        assert controller._health_state['circuit_breaker_activations'] == 2
-        assert controller._health_state['recovery_count'] == 2
-
-    @pytest.mark.asyncio
-    async def test_recovery_confirmation_boundary_conditions(self, controller_fast_timers, mock_session):
+        assert controller._health_state["circuit_breaker_activations"] == 2
+        assert controller._health_state["recovery_count"] == 2
+
+    @pytest.mark.asyncio
+    async def test_recovery_confirmation_boundary_conditions(
+        self, controller_fast_timers, mock_session
+    ):
         """Test recovery confirmation at exactly the threshold boundary."""
         controller = controller_fast_timers
         controller._session = mock_session
         controller._health_recovery_confirmation_threshold = 4  # Higher threshold
-        
-        failure_response = self._mock_response(500, text_data="Error")
-        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+
+        failure_response = self._mock_response(500, text_data="Error")
+        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
+
         # Pattern: failures → CB → exactly N-1 successes → failure → N successes
         responses = [
-            failure_response, failure_response, failure_response,  # Trigger CB
-            success_response, success_response, success_response,  # 3/4 successes (not enough)
-            failure_response,                                      # Reset confirmation counter
-            success_response, success_response, success_response, success_response  # 4/4 successes (should recover)
-        ]
-        mock_session.get.side_effect = responses
-        
+            failure_response,
+            failure_response,
+            failure_response,  # Trigger CB
+            success_response,
+            success_response,
+            success_response,  # 3/4 successes (not enough)
+            failure_response,  # Reset confirmation counter
+            success_response,
+            success_response,
+            success_response,
+            success_response,  # 4/4 successes (should recover)
+        ]
+        mock_session.get.side_effect = responses
+
         await controller.start()
         await asyncio.sleep(0.7)
         await controller.stop()
-        
+
         # Verify recovery required exactly the configured threshold
-        assert controller._health_state['circuit_breaker_activations'] == 1
-        assert controller._health_state['recovery_count'] == 1
-
-    @pytest.mark.asyncio
-    async def test_no_premature_circuit_breaker_reset(self, controller_fast_timers, mock_session):
+        assert controller._health_state["circuit_breaker_activations"] == 1
+        assert controller._health_state["recovery_count"] == 1
+
+    @pytest.mark.asyncio
+    async def test_no_premature_circuit_breaker_reset(
+        self, controller_fast_timers, mock_session
+    ):
         """Test that transient successes don't prematurely reset circuit breaker state."""
         controller = controller_fast_timers
         controller._session = mock_session
         controller._health_recovery_confirmation_threshold = 3
-        
-        failure_response = self._mock_response(500, text_data="Error")
-        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+
+        failure_response = self._mock_response(500, text_data="Error")
+        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
+
         # Pattern: failures → CB → single success → more failures (should not reset CB state)
         responses = [
-            failure_response, failure_response, failure_response,  # Trigger CB
-            success_response,                                      # Single success (insufficient)
-            failure_response, failure_response                     # More failures during recovery
-        ]
-        mock_session.get.side_effect = responses
-        
+            failure_response,
+            failure_response,
+            failure_response,  # Trigger CB
+            success_response,  # Single success (insufficient)
+            failure_response,
+            failure_response,  # More failures during recovery
+        ]
+        mock_session.get.side_effect = responses
+
         await controller.start()
         await asyncio.sleep(0.5)
         await controller.stop()
-        
+
         # Verify circuit breaker activated but did not recover
-        assert controller._health_state['circuit_breaker_activations'] == 1
-        assert controller._health_state['recovery_count'] == 0
+        assert controller._health_state["circuit_breaker_activations"] == 1
+        assert controller._health_state["recovery_count"] == 0
         # Consecutive successes should be reset to 0 after failure
-        assert controller._health_state['consecutive_successes_during_recovery'] == 0
\ No newline at end of file
+        assert controller._health_state["consecutive_successes_during_recovery"] == 0
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_health_monitor_backoff_jitter.py	2025-08-04 14:36:15.434574+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_health_monitor_backoff_jitter.py	2025-08-04 15:35:26.353948+00:00
@@ -35,14 +35,14 @@
             health_check_interval=0.1,  # Base interval
             health_failure_threshold=5,  # Higher threshold to avoid CB during backoff test
             health_max_backoff_interval=2.0,  # Max backoff cap
             backoff_base_multiplier=2.0,  # Double each time
             backoff_jitter_range=(1.0, 1.0),  # No jitter for predictable testing
-            health_circuit_breaker_timeout=10.0  # Long timeout to focus on backoff
-        )
-
-    @pytest.fixture  
+            health_circuit_breaker_timeout=10.0,  # Long timeout to focus on backoff
+        )
+
+    @pytest.fixture
     def mock_session(self):
         """Create mock aiohttp session."""
         session = Mock()
         session.get = AsyncMock()
         session.close = AsyncMock()
@@ -55,265 +55,319 @@
         response.json = AsyncMock(return_value=json_data or {})
         response.text = AsyncMock(return_value=text_data)
         return response
 
     @pytest.mark.asyncio
-    async def test_exponential_backoff_calculation(self, controller_backoff_test, mock_session):
+    async def test_exponential_backoff_calculation(
+        self, controller_backoff_test, mock_session
+    ):
         """Test exponential backoff interval calculation."""
         controller = controller_backoff_test
         controller._session = mock_session
-        
+
         failure_response = self._mock_response(500, text_data="Service Error")
-        
+
         # All failures to trigger exponential backoff
         responses = [failure_response] * 10
         mock_session.get.side_effect = responses
-        
+
         # Mock asyncio.sleep to capture sleep intervals
         sleep_intervals = []
-        
+
         async def mock_sleep(interval):
             sleep_intervals.append(interval)
             await asyncio.sleep(0.01)  # Short actual sleep for test speed
-        
-        with patch('asyncio.sleep', side_effect=mock_sleep):
+
+        with patch("asyncio.sleep", side_effect=mock_sleep):
             await controller.start()
             await asyncio.sleep(0.3)  # Let several failures occur
             await controller.stop()
-        
+
         # Verify exponential backoff pattern
         # Expected intervals: 0.1, 0.2, 0.4, 0.8, 1.6, 2.0 (capped), 2.0, ...
         # Note: The sleep intervals include both health check intervals and error backoffs
         error_backoffs = [interval for interval in sleep_intervals if interval > 0.1]
-        
+
         assert len(error_backoffs) >= 3, "Should have multiple error backoff intervals"
-        
+
         # Verify exponential growth up to the cap
         if len(error_backoffs) >= 3:
             # First few should show exponential growth
-            assert error_backoffs[1] > error_backoffs[0], "Second backoff should be longer than first"
+            assert (
+                error_backoffs[1] > error_backoffs[0]
+            ), "Second backoff should be longer than first"
             # Later intervals should be capped
             max_intervals = [interval for interval in error_backoffs if interval >= 2.0]
             assert len(max_intervals) > 0, "Should hit maximum backoff interval cap"
 
     @pytest.mark.asyncio
     async def test_backoff_with_jitter(self, mock_session):
         """Test backoff calculation with jitter applied."""
         controller = MediaMTXController(
-            host="localhost", api_port=9997, rtsp_port=8554,
-            webrtc_port=8889, hls_port=8888, config_path="/tmp/config.yml",
-            recordings_path="/tmp/recordings", snapshots_path="/tmp/snapshots",
+            host="localhost",
+            api_port=9997,
+            rtsp_port=8554,
+            webrtc_port=8889,
+            hls_port=8888,
+            config_path="/tmp/config.yml",
+            recordings_path="/tmp/recordings",
+            snapshots_path="/tmp/snapshots",
             health_check_interval=0.1,
             health_failure_threshold=5,
             health_max_backoff_interval=2.0,
             backoff_base_multiplier=2.0,
             backoff_jitter_range=(0.8, 1.2),  # ±20% jitter
-            health_circuit_breaker_timeout=10.0
-        )
-        controller._session = mock_session
-        
+            health_circuit_breaker_timeout=10.0,
+        )
+        controller._session = mock_session
+
         failure_response = self._mock_response(500, text_data="Error")
         responses = [failure_response] * 8
         mock_session.get.side_effect = responses
-        
-        sleep_intervals = []
-        
-        async def mock_sleep(interval):
-            sleep_intervals.append(interval)
-            await asyncio.sleep(0.01)
-        
-        with patch('asyncio.sleep', side_effect=mock_sleep):
+
+        sleep_intervals = []
+
+        async def mock_sleep(interval):
+            sleep_intervals.append(interval)
+            await asyncio.sleep(0.01)
+
+        with patch("asyncio.sleep", side_effect=mock_sleep):
             await controller.start()
             await asyncio.sleep(0.25)
             await controller.stop()
-        
+
         # Verify jitter is applied (intervals should vary within expected ranges)
         error_backoffs = [interval for interval in sleep_intervals if interval > 0.1]
-        
+
         if len(error_backoffs) >= 4:
             # With jitter, consecutive intervals should not be identical
             # (except when hitting the maximum cap)
             unique_intervals = set(error_backoffs[:4])  # First 4 backoffs
-            assert len(unique_intervals) >= 2, "Jitter should create variation in backoff intervals"
-
-    @pytest.mark.asyncio
-    async def test_backoff_maximum_cap_enforcement(self, controller_backoff_test, mock_session):
+            assert (
+                len(unique_intervals) >= 2
+            ), "Jitter should create variation in backoff intervals"
+
+    @pytest.mark.asyncio
+    async def test_backoff_maximum_cap_enforcement(
+        self, controller_backoff_test, mock_session
+    ):
         """Test that backoff intervals are capped at maximum value."""
         controller = controller_backoff_test
         controller._session = mock_session
         controller._health_max_backoff_interval = 1.0  # Lower cap for testing
-        
+
         failure_response = self._mock_response(500, text_data="Error")
         responses = [failure_response] * 15  # Many failures to exceed cap
         mock_session.get.side_effect = responses
-        
-        sleep_intervals = []
-        
-        async def mock_sleep(interval):
-            sleep_intervals.append(interval)
-            await asyncio.sleep(0.01)
-        
-        with patch('asyncio.sleep', side_effect=mock_sleep):
+
+        sleep_intervals = []
+
+        async def mock_sleep(interval):
+            sleep_intervals.append(interval)
+            await asyncio.sleep(0.01)
+
+        with patch("asyncio.sleep", side_effect=mock_sleep):
             await controller.start()
             await asyncio.sleep(0.4)
             await controller.stop()
-        
+
         # Verify no interval exceeds the maximum cap
         max_interval = max(sleep_intervals)
-        assert max_interval <= 1.1, f"No interval should exceed cap (max: {max_interval})"
-        
+        assert (
+            max_interval <= 1.1
+        ), f"No interval should exceed cap (max: {max_interval})"
+
         # Verify cap is actually reached
         capped_intervals = [interval for interval in sleep_intervals if interval >= 1.0]
         assert len(capped_intervals) > 0, "Should reach maximum backoff cap"
 
     @pytest.mark.asyncio
-    async def test_backoff_reset_on_success(self, controller_backoff_test, mock_session):
+    async def test_backoff_reset_on_success(
+        self, controller_backoff_test, mock_session
+    ):
         """Test that backoff resets when health check succeeds."""
         controller = controller_backoff_test
         controller._session = mock_session
-        
+
         failure_response = self._mock_response(500, text_data="Error")
         success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+
         # Pattern: failures (build up backoff) → success (reset) → failures again
         responses = [
-            failure_response, failure_response, failure_response,  # Build backoff
-            success_response,                                      # Reset backoff
-            failure_response, failure_response                     # Fresh backoff sequence
+            failure_response,
+            failure_response,
+            failure_response,  # Build backoff
+            success_response,  # Reset backoff
+            failure_response,
+            failure_response,  # Fresh backoff sequence
         ]
         mock_session.get.side_effect = responses
-        
-        sleep_intervals = []
-        
-        async def mock_sleep(interval):
-            sleep_intervals.append(interval)
-            await asyncio.sleep(0.01)
-        
-        with patch('asyncio.sleep', side_effect=mock_sleep):
+
+        sleep_intervals = []
+
+        async def mock_sleep(interval):
+            sleep_intervals.append(interval)
+            await asyncio.sleep(0.01)
+
+        with patch("asyncio.sleep", side_effect=mock_sleep):
             await controller.start()
             await asyncio.sleep(0.4)
             await controller.stop()
-        
+
         # Verify backoff was reset after success
         # Should see: increasing intervals → success (normal interval) → increasing again
         error_backoffs = [interval for interval in sleep_intervals if interval > 0.15]
-        normal_intervals = [interval for interval in sleep_intervals if 0.08 <= interval <= 0.12]
-        
+        normal_intervals = [
+            interval for interval in sleep_intervals if 0.08 <= interval <= 0.12
+        ]
+
         assert len(error_backoffs) >= 2, "Should have error backoff intervals"
         assert len(normal_intervals) >= 1, "Should have normal intervals after success"
 
     @pytest.mark.asyncio
     async def test_configurable_backoff_multiplier(self, mock_session):
         """Test different backoff multiplier configurations."""
         # Controller with aggressive backoff (multiplier = 3.0)
         controller_aggressive = MediaMTXController(
-            host="localhost", api_port=9997, rtsp_port=8554,
-            webrtc_port=8889, hls_port=8888, config_path="/tmp/config.yml",
-            recordings_path="/tmp/recordings", snapshots_path="/tmp/snapshots",
-            health_check_interval=0.1, health_failure_threshold=5,
-            health_max_backoff_interval=5.0, backoff_base_multiplier=3.0,
-            backoff_jitter_range=(1.0, 1.0), health_circuit_breaker_timeout=10.0
-        )
-        
+            host="localhost",
+            api_port=9997,
+            rtsp_port=8554,
+            webrtc_port=8889,
+            hls_port=8888,
+            config_path="/tmp/config.yml",
+            recordings_path="/tmp/recordings",
+            snapshots_path="/tmp/snapshots",
+            health_check_interval=0.1,
+            health_failure_threshold=5,
+            health_max_backoff_interval=5.0,
+            backoff_base_multiplier=3.0,
+            backoff_jitter_range=(1.0, 1.0),
+            health_circuit_breaker_timeout=10.0,
+        )
+
         # Controller with conservative backoff (multiplier = 1.5)
         controller_conservative = MediaMTXController(
-            host="localhost", api_port=9997, rtsp_port=8554,
-            webrtc_port=8889, hls_port=8888, config_path="/tmp/config.yml",
-            recordings_path="/tmp/recordings", snapshots_path="/tmp/snapshots",
-            health_check_interval=0.1, health_failure_threshold=5,
-            health_max_backoff_interval=5.0, backoff_base_multiplier=1.5,
-            backoff_jitter_range=(1.0, 1.0), health_circuit_breaker_timeout=10.0
-        )
-        
+            host="localhost",
+            api_port=9997,
+            rtsp_port=8554,
+            webrtc_port=8889,
+            hls_port=8888,
+            config_path="/tmp/config.yml",
+            recordings_path="/tmp/recordings",
+            snapshots_path="/tmp/snapshots",
+            health_check_interval=0.1,
+            health_failure_threshold=5,
+            health_max_backoff_interval=5.0,
+            backoff_base_multiplier=1.5,
+            backoff_jitter_range=(1.0, 1.0),
+            health_circuit_breaker_timeout=10.0,
+        )
+
         # Verify multipliers are configured correctly
         assert controller_aggressive._backoff_base_multiplier == 3.0
         assert controller_conservative._backoff_base_multiplier == 1.5
 
-    @pytest.mark.asyncio 
+    @pytest.mark.asyncio
     async def test_jitter_range_configuration(self, mock_session):
         """Test different jitter range configurations."""
         # No jitter
         controller_no_jitter = MediaMTXController(
-            host="localhost", api_port=9997, rtsp_port=8554,
-            webrtc_port=8889, hls_port=8888, config_path="/tmp/config.yml",
-            recordings_path="/tmp/recordings", snapshots_path="/tmp/snapshots",
-            backoff_jitter_range=(1.0, 1.0)  # No variation
-        )
-        
+            host="localhost",
+            api_port=9997,
+            rtsp_port=8554,
+            webrtc_port=8889,
+            hls_port=8888,
+            config_path="/tmp/config.yml",
+            recordings_path="/tmp/recordings",
+            snapshots_path="/tmp/snapshots",
+            backoff_jitter_range=(1.0, 1.0),  # No variation
+        )
+
         # Wide jitter range
         controller_wide_jitter = MediaMTXController(
-            host="localhost", api_port=9997, rtsp_port=8554,
-            webrtc_port=8889, hls_port=8888, config_path="/tmp/config.yml",
-            recordings_path="/tmp/recordings", snapshots_path="/tmp/snapshots",
-            backoff_jitter_range=(0.5, 2.0)  # Wide variation
-        )
-        
+            host="localhost",
+            api_port=9997,
+            rtsp_port=8554,
+            webrtc_port=8889,
+            hls_port=8888,
+            config_path="/tmp/config.yml",
+            recordings_path="/tmp/recordings",
+            snapshots_path="/tmp/snapshots",
+            backoff_jitter_range=(0.5, 2.0),  # Wide variation
+        )
+
         # Verify jitter configurations
         assert controller_no_jitter._backoff_jitter_range == (1.0, 1.0)
         assert controller_wide_jitter._backoff_jitter_range == (0.5, 2.0)
 
     @pytest.mark.asyncio
-    async def test_circuit_breaker_backoff_interaction(self, controller_backoff_test, mock_session):
+    async def test_circuit_breaker_backoff_interaction(
+        self, controller_backoff_test, mock_session
+    ):
         """Test backoff behavior when circuit breaker is active."""
         controller = controller_backoff_test
         controller._session = mock_session
         controller._health_failure_threshold = 2  # Low threshold to trigger CB quickly
-        
+
         failure_response = self._mock_response(500, text_data="Error")
         responses = [failure_response] * 8  # Enough to trigger CB and continue
         mock_session.get.side_effect = responses
-        
-        sleep_intervals = []
-        
-        async def mock_sleep(interval):
-            sleep_intervals.append(interval)
-            await asyncio.sleep(0.01)
-        
-        with patch('asyncio.sleep', side_effect=mock_sleep):
+
+        sleep_intervals = []
+
+        async def mock_sleep(interval):
+            sleep_intervals.append(interval)
+            await asyncio.sleep(0.01)
+
+        with patch("asyncio.sleep", side_effect=mock_sleep):
             await controller.start()
             await asyncio.sleep(0.3)
             await controller.stop()
-        
+
         # Verify circuit breaker was activated
-        assert controller._health_state['circuit_breaker_activations'] > 0
-        
+        assert controller._health_state["circuit_breaker_activations"] > 0
+
         # Verify there are different types of sleep intervals:
         # - Normal health check intervals
-        # - Error backoff intervals  
+        # - Error backoff intervals
         # - Circuit breaker wait intervals
         unique_intervals = set(sleep_intervals)
-        assert len(unique_intervals) >= 2, "Should have varied sleep intervals during CB and backoff"
-
-    @pytest.mark.asyncio
-    async def test_deterministic_backoff_with_no_jitter(self, controller_backoff_test, mock_session):
+        assert (
+            len(unique_intervals) >= 2
+        ), "Should have varied sleep intervals during CB and backoff"
+
+    @pytest.mark.asyncio
+    async def test_deterministic_backoff_with_no_jitter(
+        self, controller_backoff_test, mock_session
+    ):
         """Test that backoff is deterministic when jitter is disabled."""
         controller = controller_backoff_test
         controller._session = mock_session
         controller._backoff_jitter_range = (1.0, 1.0)  # No jitter
-        
+
         failure_response = self._mock_response(500, text_data="Error")
         responses = [failure_response] * 6
         mock_session.get.side_effect = responses
-        
-        sleep_intervals = []
-        
-        async def mock_sleep(interval):
-            sleep_intervals.append(interval)
-            await asyncio.sleep(0.01)
-        
-        with patch('asyncio.sleep', side_effect=mock_sleep):
+
+        sleep_intervals = []
+
+        async def mock_sleep(interval):
+            sleep_intervals.append(interval)
+            await asyncio.sleep(0.01)
+
+        with patch("asyncio.sleep", side_effect=mock_sleep):
             await controller.start()
             await asyncio.sleep(0.25)
             await controller.stop()
-        
+
         # With no jitter, consecutive error backoffs at same failure count should be identical
         error_backoffs = [interval for interval in sleep_intervals if interval > 0.1]
-        
+
         # Verify predictable exponential progression
         if len(error_backoffs) >= 3:
             # Expected: 0.2, 0.4, 0.8, ... (base_interval * multiplier^failures)
             expected_first = 0.1 * 2.0  # First error backoff
             expected_second = 0.1 * 4.0  # Second error backoff
-            
+
             # Allow small floating point tolerance
             assert abs(error_backoffs[0] - expected_first) < 0.01
-            assert abs(error_backoffs[1] - expected_second) < 0.01
\ No newline at end of file
+            assert abs(error_backoffs[1] - expected_second) < 0.01
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_service_manager.py	2025-08-03 19:27:38.812956+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_service_manager.py	2025-08-04 15:35:26.378777+00:00
@@ -5,11 +5,19 @@
 
 import pytest
 import asyncio
 from unittest.mock import Mock, patch
 
-from camera_service.config import Config, ServerConfig, MediaMTXConfig, CameraConfig, LoggingConfig, RecordingConfig, SnapshotConfig
+from camera_service.config import (
+    Config,
+    ServerConfig,
+    MediaMTXConfig,
+    CameraConfig,
+    LoggingConfig,
+    RecordingConfig,
+    SnapshotConfig,
+)
 from camera_service.service_manager import ServiceManager
 
 
 class TestServiceManager:
     """Test cases for ServiceManager class."""
@@ -18,14 +26,14 @@
     def mock_config(self):
         """Create a mock configuration for testing."""
         return Config(
             server=ServerConfig(),
             mediamtx=MediaMTXConfig(),
-            camera=CameraConfig(),  
+            camera=CameraConfig(),
             logging=LoggingConfig(),
             recording=RecordingConfig(),
-            snapshots=SnapshotConfig()
+            snapshots=SnapshotConfig(),
         )
 
     def test_instantiation(self, mock_config):
         """Test ServiceManager can be instantiated with valid config."""
         service_manager = ServiceManager(mock_config)
@@ -48,11 +56,11 @@
         # TODO: Verify startup sequence
         service_manager = ServiceManager(mock_config)
         # Test implementation needed when start() is implemented
         pass
 
-    @pytest.mark.asyncio  
+    @pytest.mark.asyncio
     async def test_stop(self, mock_config):
         """Test ServiceManager stop method."""
         # TODO: Implement test for stop() method
         # TODO: Mock component shutdown
         # TODO: Verify shutdown sequence
@@ -71,11 +79,11 @@
 
     def test_get_status(self, mock_config):
         """Test ServiceManager get_status method."""
         service_manager = ServiceManager(mock_config)
         status = service_manager.get_status()
-        
+
         assert isinstance(status, dict)
         assert "running" in status
         assert status["running"] is False
 
     @pytest.mark.asyncio
@@ -164,6 +172,6 @@
     async def test_double_start_raises_error(self, mock_config):
         """Test that starting an already running service raises RuntimeError."""
         # TODO: Implement test for double start prevention
         service_manager = ServiceManager(mock_config)
         # Test implementation needed when start() is implemented
-        pass
\ No newline at end of file
+        pass
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_health_monitor_recovery_confirmation.py	2025-08-04 14:35:31.813096+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_mediamtx_wrapper/test_health_monitor_recovery_confirmation.py	2025-08-04 15:35:26.424323+00:00
@@ -1,10 +1,10 @@
 # tests/unit/test_mediamtx_wrapper/test_health_monitor_recovery_confirmation.py
 """
 Test health monitoring recovery confirmation logic and circuit breaker state transitions.
 
-Test policy: Verify that circuit breaker recovery requires exactly N consecutive 
+Test policy: Verify that circuit breaker recovery requires exactly N consecutive
 successful health checks and that any failure during recovery resets the confirmation progress.
 """
 
 import pytest
 import asyncio
@@ -35,14 +35,14 @@
             health_failure_threshold=3,
             health_circuit_breaker_timeout=0.2,
             health_max_backoff_interval=1.0,
             health_recovery_confirmation_threshold=3,
             backoff_base_multiplier=2.0,
-            backoff_jitter_range=(1.0, 1.0)  # No jitter for predictable testing
-        )
-
-    @pytest.fixture  
+            backoff_jitter_range=(1.0, 1.0),  # No jitter for predictable testing
+        )
+
+    @pytest.fixture
     def mock_session(self):
         """Create mock aiohttp session."""
         session = Mock()
         session.get = AsyncMock()
         session.close = AsyncMock()
@@ -55,232 +55,303 @@
         response.json = AsyncMock(return_value=json_data or {})
         response.text = AsyncMock(return_value=text_data)
         return response
 
     @pytest.mark.asyncio
-    async def test_exact_consecutive_success_requirement(self, controller_fast_timers, mock_session):
+    async def test_exact_consecutive_success_requirement(
+        self, controller_fast_timers, mock_session
+    ):
         """Test that recovery requires exactly the configured number of consecutive successes."""
         controller = controller_fast_timers
         controller._session = mock_session
-        controller._health_recovery_confirmation_threshold = 4  # Require 4 consecutive successes
-        
+        controller._health_recovery_confirmation_threshold = (
+            4  # Require 4 consecutive successes
+        )
+
         failure_response = self._mock_response(500, text_data="Service Error")
-        success_response = self._mock_response(200, {"serverVersion": "1.0.0", "serverUptime": 1200})
-        
+        success_response = self._mock_response(
+            200, {"serverVersion": "1.0.0", "serverUptime": 1200}
+        )
+
         # Pattern: failures → CB timeout → exactly 4 consecutive successes
         responses = [
-            failure_response, failure_response, failure_response,  # Trigger CB
-            success_response, success_response, success_response, success_response  # Exactly 4 successes
-        ]
-        mock_session.get.side_effect = responses
-        
+            failure_response,
+            failure_response,
+            failure_response,  # Trigger CB
+            success_response,
+            success_response,
+            success_response,
+            success_response,  # Exactly 4 successes
+        ]
+        mock_session.get.side_effect = responses
+
         await controller.start()
         await asyncio.sleep(0.5)  # Let recovery sequence complete
         await controller.stop()
-        
+
         # Verify recovery occurred after exactly 4 consecutive successes
-        assert controller._health_state['circuit_breaker_activations'] == 1
-        assert controller._health_state['recovery_count'] == 1
-        assert controller._health_state['consecutive_successes_during_recovery'] == 0  # Reset after recovery
-
-    @pytest.mark.asyncio
-    async def test_insufficient_consecutive_successes(self, controller_fast_timers, mock_session):
+        assert controller._health_state["circuit_breaker_activations"] == 1
+        assert controller._health_state["recovery_count"] == 1
+        assert (
+            controller._health_state["consecutive_successes_during_recovery"] == 0
+        )  # Reset after recovery
+
+    @pytest.mark.asyncio
+    async def test_insufficient_consecutive_successes(
+        self, controller_fast_timers, mock_session
+    ):
         """Test that N-1 consecutive successes do not trigger recovery."""
         controller = controller_fast_timers
         controller._session = mock_session
         controller._health_recovery_confirmation_threshold = 3
-        
-        failure_response = self._mock_response(500, text_data="Error")
-        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+
+        failure_response = self._mock_response(500, text_data="Error")
+        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
+
         # Pattern: failures → CB timeout → only 2 successes (insufficient for threshold of 3)
         responses = [
-            failure_response, failure_response, failure_response,  # Trigger CB
-            success_response, success_response  # Only 2/3 required successes
-        ]
-        mock_session.get.side_effect = responses
-        
+            failure_response,
+            failure_response,
+            failure_response,  # Trigger CB
+            success_response,
+            success_response,  # Only 2/3 required successes
+        ]
+        mock_session.get.side_effect = responses
+
         await controller.start()
         await asyncio.sleep(0.4)
         await controller.stop()
-        
+
         # Verify circuit breaker did not recover
-        assert controller._health_state['circuit_breaker_activations'] == 1
-        assert controller._health_state['recovery_count'] == 0
-        assert controller._health_state['consecutive_successes_during_recovery'] == 2  # Partial progress
-
-    @pytest.mark.asyncio
-    async def test_failure_resets_confirmation_progress(self, controller_fast_timers, mock_session, caplog):
+        assert controller._health_state["circuit_breaker_activations"] == 1
+        assert controller._health_state["recovery_count"] == 0
+        assert (
+            controller._health_state["consecutive_successes_during_recovery"] == 2
+        )  # Partial progress
+
+    @pytest.mark.asyncio
+    async def test_failure_resets_confirmation_progress(
+        self, controller_fast_timers, mock_session, caplog
+    ):
         """Test that any failure during recovery resets the confirmation counter."""
         controller = controller_fast_timers
         controller._session = mock_session
         controller._health_recovery_confirmation_threshold = 3
-        
+
         failure_response = self._mock_response(503, text_data="Service Unavailable")
         success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+
         # Pattern: failures → CB → 2 successes → failure (reset) → 3 successes (recover)
         responses = [
-            failure_response, failure_response, failure_response,  # Trigger CB
-            success_response, success_response,  # 2/3 successes
-            failure_response,                    # Reset confirmation progress
-            success_response, success_response, success_response  # 3 consecutive successes
-        ]
-        mock_session.get.side_effect = responses
-        
-        with caplog.at_level('INFO'):
+            failure_response,
+            failure_response,
+            failure_response,  # Trigger CB
+            success_response,
+            success_response,  # 2/3 successes
+            failure_response,  # Reset confirmation progress
+            success_response,
+            success_response,
+            success_response,  # 3 consecutive successes
+        ]
+        mock_session.get.side_effect = responses
+
+        with caplog.at_level("INFO"):
             await controller.start()
             await asyncio.sleep(0.6)
             await controller.stop()
-        
+
         # Verify eventual recovery after reset
-        assert controller._health_state['circuit_breaker_activations'] == 1
-        assert controller._health_state['recovery_count'] == 1
-        
+        assert controller._health_state["circuit_breaker_activations"] == 1
+        assert controller._health_state["recovery_count"] == 1
+
         # Verify reset was logged
         log_messages = [record.message for record in caplog.records]
-        degraded_logs = [msg for msg in log_messages if "DEGRADED" in msg and "IMPROVING" not in msg]
-        assert len(degraded_logs) > 0, "Should log health degradation that resets recovery"
-
-    @pytest.mark.asyncio
-    async def test_circuit_breaker_timeout_behavior(self, controller_fast_timers, mock_session):
+        degraded_logs = [
+            msg for msg in log_messages if "DEGRADED" in msg and "IMPROVING" not in msg
+        ]
+        assert (
+            len(degraded_logs) > 0
+        ), "Should log health degradation that resets recovery"
+
+    @pytest.mark.asyncio
+    async def test_circuit_breaker_timeout_behavior(
+        self, controller_fast_timers, mock_session
+    ):
         """Test circuit breaker timeout behavior before recovery attempts."""
         controller = controller_fast_timers
         controller._session = mock_session
         controller._health_circuit_breaker_timeout = 0.3  # Short timeout for testing
-        
-        failure_response = self._mock_response(500, text_data="Error")
-        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+
+        failure_response = self._mock_response(500, text_data="Error")
+        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
+
         # Pattern: failures → CB → wait for timeout → immediate recovery
         responses = [
-            failure_response, failure_response, failure_response,  # Trigger CB
-            success_response, success_response, success_response   # Immediate recovery after timeout
-        ]
-        mock_session.get.side_effect = responses
-        
+            failure_response,
+            failure_response,
+            failure_response,  # Trigger CB
+            success_response,
+            success_response,
+            success_response,  # Immediate recovery after timeout
+        ]
+        mock_session.get.side_effect = responses
+
         start_time = time.time()
         await controller.start()
         await asyncio.sleep(0.6)  # Wait for timeout + recovery
         await controller.stop()
         elapsed = time.time() - start_time
-        
+
         # Verify circuit breaker timeout was respected
         assert elapsed >= 0.3, "Should respect circuit breaker timeout"
-        assert controller._health_state['circuit_breaker_activations'] == 1
-        assert controller._health_state['recovery_count'] == 1
+        assert controller._health_state["circuit_breaker_activations"] == 1
+        assert controller._health_state["recovery_count"] == 1
 
     @pytest.mark.asyncio
     async def test_recovery_state_tracking(self, controller_fast_timers, mock_session):
         """Test internal state tracking during recovery process."""
         controller = controller_fast_timers
         controller._session = mock_session
         controller._health_recovery_confirmation_threshold = 2
-        
-        failure_response = self._mock_response(500, text_data="Error")
-        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+
+        failure_response = self._mock_response(500, text_data="Error")
+        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
+
         # Pattern: failures → CB → track progress through recovery
         responses = [
-            failure_response, failure_response, failure_response,  # Trigger CB
+            failure_response,
+            failure_response,
+            failure_response,  # Trigger CB
             success_response,  # 1/2 successes
-            success_response   # 2/2 successes (full recovery)
-        ]
-        mock_session.get.side_effect = responses
-        
+            success_response,  # 2/2 successes (full recovery)
+        ]
+        mock_session.get.side_effect = responses
+
         await controller.start()
         await asyncio.sleep(0.4)
-        
+
         # Check intermediate state before full recovery
         # Note: We can't easily check intermediate state during execution,
         # so we verify final state after completion
         await controller.stop()
-        
+
         # Verify state was properly tracked and reset
-        assert controller._health_state['circuit_breaker_activations'] == 1
-        assert controller._health_state['recovery_count'] == 1
-        assert controller._health_state['consecutive_successes_during_recovery'] == 0  # Reset after recovery
+        assert controller._health_state["circuit_breaker_activations"] == 1
+        assert controller._health_state["recovery_count"] == 1
+        assert (
+            controller._health_state["consecutive_successes_during_recovery"] == 0
+        )  # Reset after recovery
 
     @pytest.mark.asyncio
     async def test_configurable_confirmation_threshold(self, mock_session):
         """Test different confirmation threshold configurations."""
         # Test with threshold = 1 (immediate recovery)
         controller_fast = MediaMTXController(
-            host="localhost", api_port=9997, rtsp_port=8554,
-            webrtc_port=8889, hls_port=8888, config_path="/tmp/config.yml",
-            recordings_path="/tmp/recordings", snapshots_path="/tmp/snapshots",
-            health_check_interval=0.05, health_failure_threshold=2,
-            health_circuit_breaker_timeout=0.1, health_recovery_confirmation_threshold=1
+            host="localhost",
+            api_port=9997,
+            rtsp_port=8554,
+            webrtc_port=8889,
+            hls_port=8888,
+            config_path="/tmp/config.yml",
+            recordings_path="/tmp/recordings",
+            snapshots_path="/tmp/snapshots",
+            health_check_interval=0.05,
+            health_failure_threshold=2,
+            health_circuit_breaker_timeout=0.1,
+            health_recovery_confirmation_threshold=1,
         )
         controller_fast._session = mock_session
-        
+
         # Test with threshold = 5 (slow recovery)
         controller_slow = MediaMTXController(
-            host="localhost", api_port=9997, rtsp_port=8554,
-            webrtc_port=8889, hls_port=8888, config_path="/tmp/config.yml",
-            recordings_path="/tmp/recordings", snapshots_path="/tmp/snapshots",
-            health_check_interval=0.05, health_failure_threshold=2,
-            health_circuit_breaker_timeout=0.1, health_recovery_confirmation_threshold=5
+            host="localhost",
+            api_port=9997,
+            rtsp_port=8554,
+            webrtc_port=8889,
+            hls_port=8888,
+            config_path="/tmp/config.yml",
+            recordings_path="/tmp/recordings",
+            snapshots_path="/tmp/snapshots",
+            health_check_interval=0.05,
+            health_failure_threshold=2,
+            health_circuit_breaker_timeout=0.1,
+            health_recovery_confirmation_threshold=5,
         )
         controller_slow._session = mock_session
-        
+
         # Verify configuration is applied
         assert controller_fast._health_recovery_confirmation_threshold == 1
         assert controller_slow._health_recovery_confirmation_threshold == 5
-        
-        failure_response = self._mock_response(500, text_data="Error")
-        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+
+        failure_response = self._mock_response(500, text_data="Error")
+        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
+
         # Test fast recovery (1 success)
-        mock_session.get.side_effect = [failure_response, failure_response, success_response]
+        mock_session.get.side_effect = [
+            failure_response,
+            failure_response,
+            success_response,
+        ]
         await controller_fast.start()
         await asyncio.sleep(0.3)
         await controller_fast.stop()
-        
+
         # Fast controller should recover immediately
-        assert controller_fast._health_state['recovery_count'] == 1
-        
+        assert controller_fast._health_state["recovery_count"] == 1
+
         # Reset mock for slow controller test
         mock_session.get.side_effect = [
-            failure_response, failure_response,  # Trigger CB
-            success_response, success_response, success_response, success_response  # Only 4/5 needed
+            failure_response,
+            failure_response,  # Trigger CB
+            success_response,
+            success_response,
+            success_response,
+            success_response,  # Only 4/5 needed
         ]
         await controller_slow.start()
         await asyncio.sleep(0.4)
         await controller_slow.stop()
-        
+
         # Slow controller should not recover with only 4/5 successes
-        assert controller_slow._health_state['recovery_count'] == 0
-
-    @pytest.mark.asyncio
-    async def test_partial_recovery_logging(self, controller_fast_timers, mock_session, caplog):
+        assert controller_slow._health_state["recovery_count"] == 0
+
+    @pytest.mark.asyncio
+    async def test_partial_recovery_logging(
+        self, controller_fast_timers, mock_session, caplog
+    ):
         """Test that partial recovery progress is properly logged."""
         controller = controller_fast_timers
         controller._session = mock_session
         controller._health_recovery_confirmation_threshold = 4
-        
-        failure_response = self._mock_response(500, text_data="Error")
-        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
-        
+
+        failure_response = self._mock_response(500, text_data="Error")
+        success_response = self._mock_response(200, {"serverVersion": "1.0.0"})
+
         # Pattern: failures → CB → partial recovery → reset → full recovery
         responses = [
-            failure_response, failure_response, failure_response,  # Trigger CB
-            success_response, success_response,  # 2/4 successes (partial)
-            failure_response,                    # Reset
-            success_response, success_response, success_response, success_response  # Full recovery
-        ]
-        mock_session.get.side_effect = responses
-        
-        with caplog.at_level('INFO'):
+            failure_response,
+            failure_response,
+            failure_response,  # Trigger CB
+            success_response,
+            success_response,  # 2/4 successes (partial)
+            failure_response,  # Reset
+            success_response,
+            success_response,
+            success_response,
+            success_response,  # Full recovery
+        ]
+        mock_session.get.side_effect = responses
+
+        with caplog.at_level("INFO"):
             await controller.start()
             await asyncio.sleep(0.7)
             await controller.stop()
-        
+
         # Verify different recovery states were logged
         log_messages = [record.message for record in caplog.records]
         improving_logs = [msg for msg in log_messages if "IMPROVING" in msg]
         recovered_logs = [msg for msg in log_messages if "FULLY RECOVERED" in msg]
         degraded_logs = [msg for msg in log_messages if "DEGRADED" in msg]
-        
+
         assert len(improving_logs) >= 1, "Should log partial recovery progress"
         assert len(recovered_logs) == 1, "Should log full recovery"
-        assert len(degraded_logs) >= 1, "Should log degradation that resets recovery"
\ No newline at end of file
+        assert len(degraded_logs) >= 1, "Should log degradation that resets recovery"
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_websocket_server/test_server_method_handlers.py	2025-08-03 19:27:38.812956+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_websocket_server/test_server_method_handlers.py	2025-08-04 15:35:26.653669+00:00
@@ -19,124 +19,131 @@
 
     @pytest.fixture
     def server(self):
         """Create WebSocket server instance for testing."""
         return WebSocketJsonRpcServer(
-            host="localhost",
-            port=8002,
-            websocket_path="/ws",
-            max_connections=100
+            host="localhost", port=8002, websocket_path="/ws", max_connections=100
         )
 
     def test_method_registration_and_versioning(self, server):
         """Test method registration with version tracking."""
         # Test built-in method registration
         server._register_builtin_methods()
-        
+
         # Verify core methods are registered
-        expected_methods = ["ping", "get_camera_list", "get_camera_status", 
-                          "take_snapshot", "start_recording", "stop_recording"]
-        
+        expected_methods = [
+            "ping",
+            "get_camera_list",
+            "get_camera_status",
+            "take_snapshot",
+            "start_recording",
+            "stop_recording",
+        ]
+
         for method in expected_methods:
             assert method in server._method_handlers
             assert method in server._method_versions
             assert server._method_versions[method] == "1.0"
 
     def test_custom_method_registration(self, server):
         """Test registration of custom methods with versions."""
+
         async def custom_handler(params=None):
             return {"result": "custom"}
-        
+
         # Register custom method with version
         server.register_method("custom_method", custom_handler, version="2.1")
-        
+
         # Verify method is registered
         assert "custom_method" in server._method_handlers
         assert server.get_method_version("custom_method") == "2.1"
-        
+
         # Test method unregistration
         server.unregister_method("custom_method")
         assert "custom_method" not in server._method_handlers
 
     @pytest.mark.asyncio
     async def test_ping_method(self, server):
         """Test ping method for health checks."""
         result = await server._method_ping()
         assert result == "pong"
-        
+
         # Test with parameters (should be ignored)
         result = await server._method_ping({"test": "value"})
         assert result == "pong"
 
     def test_parameter_validation(self, server):
         """Test parameter validation in method handlers."""
         # Test methods requiring device parameter
         with pytest.raises(ValueError, match="device parameter is required"):
             asyncio.run(server._method_get_camera_status({}))
-        
+
         with pytest.raises(ValueError, match="device parameter is required"):
             asyncio.run(server._method_take_snapshot(None))
-        
+
         with pytest.raises(ValueError, match="device parameter is required"):
             asyncio.run(server._method_start_recording({}))
-        
+
         with pytest.raises(ValueError, match="device parameter is required"):
             asyncio.run(server._method_stop_recording({}))
 
     @pytest.mark.asyncio
     async def test_take_snapshot_parameter_handling(self, server):
         """Test snapshot method parameter processing."""
         # Mock MediaMTX controller
         mock_controller = Mock()
-        mock_controller.take_snapshot = AsyncMock(return_value={
-            "filename": "test_snapshot.jpg",
-            "file_size": 12345,
-            "file_path": "/opt/snapshots/test_snapshot.jpg"
-        })
+        mock_controller.take_snapshot = AsyncMock(
+            return_value={
+                "filename": "test_snapshot.jpg",
+                "file_size": 12345,
+                "file_path": "/opt/snapshots/test_snapshot.jpg",
+            }
+        )
         server._mediamtx_controller = mock_controller
-        
+
         # Test with device parameter only
         result = await server._method_take_snapshot({"device": "/dev/video0"})
         assert result["device"] == "/dev/video0"
         assert result["status"] == "completed"
         assert "filename" in result
-        
+
         # Test with custom filename
-        result = await server._method_take_snapshot({
-            "device": "/dev/video0",
-            "filename": "custom_snapshot.jpg"
-        })
+        result = await server._method_take_snapshot(
+            {"device": "/dev/video0", "filename": "custom_snapshot.jpg"}
+        )
         assert result["filename"] == "test_snapshot.jpg"  # From mock controller
 
     @pytest.mark.asyncio
     async def test_recording_methods_parameter_handling(self, server):
         """Test recording method parameter processing."""
         # Mock MediaMTX controller
         mock_controller = Mock()
-        mock_controller.start_recording = AsyncMock(return_value={
-            "filename": "test_recording.mp4",
-            "start_time": "2025-08-03T12:00:00Z"
-        })
-        mock_controller.stop_recording = AsyncMock(return_value={
-            "filename": "test_recording.mp4",
-            "start_time": "2025-08-03T12:00:00Z",
-            "duration": 3600,
-            "file_size": 1073741824
-        })
+        mock_controller.start_recording = AsyncMock(
+            return_value={
+                "filename": "test_recording.mp4",
+                "start_time": "2025-08-03T12:00:00Z",
+            }
+        )
+        mock_controller.stop_recording = AsyncMock(
+            return_value={
+                "filename": "test_recording.mp4",
+                "start_time": "2025-08-03T12:00:00Z",
+                "duration": 3600,
+                "file_size": 1073741824,
+            }
+        )
         server._mediamtx_controller = mock_controller
-        
+
         # Test start_recording with parameters
-        result = await server._method_start_recording({
-            "device": "/dev/video0",
-            "duration": 3600,
-            "format": "mp4"
-        })
+        result = await server._method_start_recording(
+            {"device": "/dev/video0", "duration": 3600, "format": "mp4"}
+        )
         assert result["device"] == "/dev/video0"
         assert result["status"] == "STARTED"
         assert result["duration"] == 3600
         assert result["format"] == "mp4"
-        
+
         # Test stop_recording
         result = await server._method_stop_recording({"device": "/dev/video0"})
         assert result["device"] == "/dev/video0"
         assert result["status"] == "STOPPED"
         assert result["duration"] == 3600
@@ -144,21 +151,21 @@
     @pytest.mark.asyncio
     async def test_method_error_handling_no_mediamtx(self, server):
         """Test method error handling when MediaMTX controller unavailable."""
         # Ensure no MediaMTX controller
         server._mediamtx_controller = None
-        
+
         # Test snapshot without MediaMTX
         result = await server._method_take_snapshot({"device": "/dev/video0"})
         assert result["status"] == "FAILED"
         assert "MediaMTX controller not available" in result["error"]
-        
+
         # Test start_recording without MediaMTX
         result = await server._method_start_recording({"device": "/dev/video0"})
         assert result["status"] == "FAILED"
         assert "MediaMTX controller not available" in result["error"]
-        
+
         # Test stop_recording without MediaMTX
         result = await server._method_stop_recording({"device": "/dev/video0"})
         assert result["status"] == "FAILED"
         assert "MediaMTX controller not available" in result["error"]
 
@@ -166,100 +173,105 @@
         """Test filename generation for recordings and snapshots."""
         # Test default filename generation
         filename = server._generate_filename("/dev/video0", "jpg")
         assert filename.startswith("camera0_")
         assert filename.endswith(".jpg")
-        
+
         # Test custom filename without extension
         filename = server._generate_filename("/dev/video0", "mp4", "custom_recording")
         assert filename == "custom_recording.mp4"
-        
+
         # Test custom filename with extension
-        filename = server._generate_filename("/dev/video0", "jpg", "custom_snapshot.jpg")
+        filename = server._generate_filename(
+            "/dev/video0", "jpg", "custom_snapshot.jpg"
+        )
         assert filename == "custom_snapshot.jpg"
 
     def test_stream_name_extraction(self, server):
         """Test stream name extraction from device paths."""
         # Test standard video device paths
         assert server._get_stream_name_from_device_path("/dev/video0") == "camera0"
         assert server._get_stream_name_from_device_path("/dev/video15") == "camera15"
-        
+
         # Test non-standard device paths
         stream_name = server._get_stream_name_from_device_path("/custom/device")
         assert stream_name.startswith("camera_")
         assert stream_name != "camera_unknown"  # Should generate deterministic hash
-        
+
         # Test empty/invalid paths
         assert server._get_stream_name_from_device_path("") == "camera_unknown"
 
     @pytest.mark.asyncio
     async def test_method_exception_handling(self, server):
         """Test exception handling in method execution."""
         # Mock MediaMTX controller that raises exceptions
         mock_controller = Mock()
-        mock_controller.take_snapshot = AsyncMock(side_effect=Exception("MediaMTX error"))
+        mock_controller.take_snapshot = AsyncMock(
+            side_effect=Exception("MediaMTX error")
+        )
         server._mediamtx_controller = mock_controller
-        
+
         # Test that exceptions are caught and return error responses
         result = await server._method_take_snapshot({"device": "/dev/video0"})
         assert result["status"] == "FAILED"
         assert "error" in result
 
     def test_method_version_tracking(self, server):
         """Test method version tracking functionality."""
+
         # Register methods with different versions
         async def handler_v1():
             return {"version": "1.0"}
-        
+
         async def handler_v2():
             return {"version": "2.0"}
-        
+
         server.register_method("test_method", handler_v1, "1.0")
         assert server.get_method_version("test_method") == "1.0"
-        
+
         # Update to new version
         server.register_method("test_method", handler_v2, "2.0")
         assert server.get_method_version("test_method") == "2.0"
-        
+
         # Test non-existent method
         assert server.get_method_version("nonexistent_method") is None
 
     def test_server_stats_and_status(self, server):
         """Test server statistics and status reporting."""
         # Test initial stats
         stats = server.get_server_stats()
         assert stats["running"] is False
         assert stats["connected_clients"] == 0
         assert stats["max_connections"] == 100
-        
+
         # Test after method registration
         server._register_builtin_methods()
         stats = server.get_server_stats()
         assert stats["registered_methods"] >= 6  # At least the built-in methods
-        
+
         # Test connection count
         assert server.get_connection_count() == 0
-        
+
         # Test is_running property
         assert server.is_running is False
 
     @pytest.mark.asyncio
     async def test_method_handlers_with_mock_dependencies(self, server):
         """Test method handlers with properly mocked dependencies."""
         # Setup mock camera monitor
         mock_camera_monitor = Mock()
         mock_camera_monitor.get_connected_cameras = AsyncMock(return_value={})
         server._camera_monitor = mock_camera_monitor
-        
-        # Setup mock MediaMTX controller  
+
+        # Setup mock MediaMTX controller
         mock_mediamtx = Mock()
         mock_mediamtx.get_stream_status = AsyncMock(return_value={"status": "inactive"})
         server._mediamtx_controller = mock_mediamtx
-        
+
         # Test get_camera_list with mocked dependencies
         result = await server._method_get_camera_list()
         assert result["cameras"] == []
         assert result["total"] == 0
         assert result["connected"] == 0
-        
+
         # Verify dependencies were called
-        mock_camera_monitor.get_connected_cameras.assert_called_once()
\ No newline at end of file
+        mock_camera_monitor.get_connected_cameras.assert_called_once()
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_websocket_server/test_server_notifications.py	2025-08-03 19:27:38.813970+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_websocket_server/test_server_notifications.py	2025-08-04 15:35:26.676282+00:00
@@ -19,23 +19,20 @@
 
     @pytest.fixture
     def server(self):
         """Create WebSocket server instance for testing."""
         return WebSocketJsonRpcServer(
-            host="localhost",
-            port=8002,
-            websocket_path="/ws",
-            max_connections=100
+            host="localhost", port=8002, websocket_path="/ws", max_connections=100
         )
 
     @pytest.fixture
     def mock_client(self):
         """Create mock WebSocket client."""
         mock_websocket = Mock()
         mock_websocket.open = True
         mock_websocket.send = AsyncMock()
-        
+
         client = ClientConnection(mock_websocket, "test-client-123")
         client.authenticated = True
         return client
 
     def test_camera_status_notification_field_filtering(self, server):
@@ -50,33 +47,45 @@
             "streams": {"rtsp": "rtsp://localhost:8554/camera0"},
             # Fields that should be filtered out:
             "internal_id": "camera-internal-123",
             "debug_info": {"probe_count": 5},
             "raw_capability_data": {"driver": "uvcvideo"},
-            "validation_status": "confirmed"
+            "validation_status": "confirmed",
         }
-        
+
         # Test notification filtering
-        with patch.object(server, 'broadcast_notification') as mock_broadcast:
+        with patch.object(server, "broadcast_notification") as mock_broadcast:
             asyncio.run(server.notify_camera_status_update(input_params))
-            
+
             # Verify broadcast_notification called with filtered parameters
             mock_broadcast.assert_called_once()
             args, kwargs = mock_broadcast.call_args
-            
+
             assert kwargs["method"] == "camera_status_update"
             filtered_params = kwargs["params"]
-            
+
             # Verify only allowed fields are present
-            expected_fields = {"device", "status", "name", "resolution", "fps", "streams"}
+            expected_fields = {
+                "device",
+                "status",
+                "name",
+                "resolution",
+                "fps",
+                "streams",
+            }
             assert set(filtered_params.keys()) == expected_fields
-            
+
             # Verify filtered out fields are not present
-            forbidden_fields = {"internal_id", "debug_info", "raw_capability_data", "validation_status"}
+            forbidden_fields = {
+                "internal_id",
+                "debug_info",
+                "raw_capability_data",
+                "validation_status",
+            }
             for field in forbidden_fields:
                 assert field not in filtered_params
-            
+
             # Verify values are preserved for allowed fields
             assert filtered_params["device"] == "/dev/video0"
             assert filtered_params["status"] == "CONNECTED"
             assert filtered_params["resolution"] == "1920x1080"
             assert filtered_params["fps"] == 30
@@ -92,75 +101,80 @@
             # Fields that should be filtered out:
             "session_internal_id": "session-abc-123",
             "file_path": "/opt/recordings/camera0_recording.mp4",
             "process_id": 12345,
             "encoding_settings": {"bitrate": "2M"},
-            "correlation_id": "req-xyz-789"
+            "correlation_id": "req-xyz-789",
         }
-        
-        # Test notification filtering  
-        with patch.object(server, 'broadcast_notification') as mock_broadcast:
+
+        # Test notification filtering
+        with patch.object(server, "broadcast_notification") as mock_broadcast:
             asyncio.run(server.notify_recording_status_update(input_params))
-            
+
             # Verify broadcast_notification called with filtered parameters
             mock_broadcast.assert_called_once()
             args, kwargs = mock_broadcast.call_args
-            
+
             assert kwargs["method"] == "recording_status_update"
             filtered_params = kwargs["params"]
-            
+
             # Verify only allowed fields are present
             expected_fields = {"device", "status", "filename", "duration"}
             assert set(filtered_params.keys()) == expected_fields
-            
+
             # Verify filtered out fields are not present
-            forbidden_fields = {"session_internal_id", "file_path", "process_id", "encoding_settings", "correlation_id"}
+            forbidden_fields = {
+                "session_internal_id",
+                "file_path",
+                "process_id",
+                "encoding_settings",
+                "correlation_id",
+            }
             for field in forbidden_fields:
                 assert field not in filtered_params
-            
+
             # Verify values are preserved for allowed fields
             assert filtered_params["device"] == "/dev/video0"
             assert filtered_params["status"] == "STARTED"
             assert filtered_params["filename"] == "camera0_recording.mp4"
             assert filtered_params["duration"] == 3600
 
     def test_notification_required_field_validation(self, server):
         """Test validation of required fields in notifications."""
         # Test camera notification without required device field
-        with patch.object(server, 'broadcast_notification') as mock_broadcast:
+        with patch.object(server, "broadcast_notification") as mock_broadcast:
             asyncio.run(server.notify_camera_status_update({"status": "CONNECTED"}))
             mock_broadcast.assert_not_called()  # Should not broadcast invalid notification
-        
+
         # Test camera notification without required status field
-        with patch.object(server, 'broadcast_notification') as mock_broadcast:
+        with patch.object(server, "broadcast_notification") as mock_broadcast:
             asyncio.run(server.notify_camera_status_update({"device": "/dev/video0"}))
             mock_broadcast.assert_not_called()  # Should not broadcast invalid notification
-        
+
         # Test recording notification without required fields
-        with patch.object(server, 'broadcast_notification') as mock_broadcast:
+        with patch.object(server, "broadcast_notification") as mock_broadcast:
             asyncio.run(server.notify_recording_status_update({"filename": "test.mp4"}))
             mock_broadcast.assert_not_called()  # Should not broadcast invalid notification
 
     @pytest.mark.asyncio
     async def test_broadcast_notification_to_clients(self, server, mock_client):
         """Test broadcasting notifications to connected clients."""
         # Add mock client to server
         server._clients["test-client-123"] = mock_client
-        
+
         # Test broadcasting notification
         await server.broadcast_notification(
-            method="test_notification",
-            params={"key": "value", "device": "/dev/video0"}
-        )
-        
+            method="test_notification", params={"key": "value", "device": "/dev/video0"}
+        )
+
         # Verify notification was sent to client
         mock_client.websocket.send.assert_called_once()
-        
+
         # Verify notification structure
         sent_data = mock_client.websocket.send.call_args[0][0]
         notification = json.loads(sent_data)
-        
+
         assert notification["jsonrpc"] == "2.0"
         assert notification["method"] == "test_notification"
         assert notification["params"]["key"] == "value"
         assert "id" not in notification  # Notifications don't have IDs
 
@@ -168,37 +182,36 @@
     async def test_notification_client_cleanup_on_failure(self, server, mock_client):
         """Test cleanup of disconnected clients during notification."""
         # Setup client with failing websocket
         mock_client.websocket.send.side_effect = Exception("Connection broken")
         server._clients["test-client-123"] = mock_client
-        
+
         # Test broadcasting notification
         await server.broadcast_notification(
-            method="test_notification", 
-            params={"device": "/dev/video0"}
-        )
-        
+            method="test_notification", params={"device": "/dev/video0"}
+        )
+
         # Verify failed client was removed
         assert "test-client-123" not in server._clients
 
     @pytest.mark.asyncio
     async def test_send_notification_to_specific_client(self, server, mock_client):
         """Test sending notification to specific client."""
         # Add mock client to server
         server._clients["test-client-123"] = mock_client
-        
+
         # Send notification to specific client
         result = await server.send_notification_to_client(
             client_id="test-client-123",
             method="camera_connected",
-            params={"device": "/dev/video0", "status": "CONNECTED"}
-        )
-        
+            params={"device": "/dev/video0", "status": "CONNECTED"},
+        )
+
         # Verify notification was sent successfully
         assert result is True
         mock_client.websocket.send.assert_called_once()
-        
+
         # Verify notification content
         sent_data = mock_client.websocket.send.call_args[0][0]
         notification = json.loads(sent_data)
         assert notification["method"] == "camera_connected"
         assert notification["params"]["device"] == "/dev/video0"
@@ -208,86 +221,82 @@
         """Test notification handling for non-existent client."""
         # Try to send notification to non-existent client
         result = await server.send_notification_to_client(
             client_id="nonexistent-client",
             method="test_notification",
-            params={"key": "value"}
-        )
-        
+            params={"key": "value"},
+        )
+
         # Should return False for non-existent client
         assert result is False
 
     @pytest.mark.asyncio
     async def test_notification_serialization_error_handling(self, server):
         """Test handling of notification serialization errors."""
+
         # Create params that can't be serialized to JSON
         class NonSerializable:
             pass
-        
-        with patch.object(server, '_clients', {"test": Mock()}):
+
+        with patch.object(server, "_clients", {"test": Mock()}):
             # Test notification with non-serializable params
             await server.broadcast_notification(
-                method="test_notification",
-                params={"object": NonSerializable()}
+                method="test_notification", params={"object": NonSerializable()}
             )
             # Should handle gracefully without crashing
 
     def test_empty_notification_params_handling(self, server):
         """Test handling of empty or None notification parameters."""
         # Test camera notification with None params
-        with patch.object(server, 'broadcast_notification') as mock_broadcast:
+        with patch.object(server, "broadcast_notification") as mock_broadcast:
             asyncio.run(server.notify_camera_status_update(None))
             mock_broadcast.assert_not_called()
-        
+
         # Test recording notification with empty params
-        with patch.object(server, 'broadcast_notification') as mock_broadcast:
+        with patch.object(server, "broadcast_notification") as mock_broadcast:
             asyncio.run(server.notify_recording_status_update({}))
             mock_broadcast.assert_not_called()
 
     @pytest.mark.asyncio
     async def test_notification_correlation_id_handling(self, server, mock_client):
         """Test correlation ID inclusion in notifications."""
         server._clients["test-client-123"] = mock_client
-        
+
         # Test notification with correlation ID in params
         await server.broadcast_notification(
             method="test_notification",
-            params={"device": "/dev/video0", "correlation_id": "test-correlation-123"}
-        )
-        
+            params={"device": "/dev/video0", "correlation_id": "test-correlation-123"},
+        )
+
         # Verify notification was sent (correlation ID handling tested in logging layer)
         mock_client.websocket.send.assert_called_once()
 
-    @pytest.mark.asyncio 
+    @pytest.mark.asyncio
     async def test_targeted_notification_broadcast(self, server):
         """Test broadcasting notifications to specific client list."""
         # Setup multiple mock clients
         client1 = Mock()
         client1.websocket.open = True
         client1.websocket.send = AsyncMock()
-        
+
         client2 = Mock()
-        client2.websocket.open = True  
+        client2.websocket.open = True
         client2.websocket.send = AsyncMock()
-        
+
         client3 = Mock()
         client3.websocket.open = True
         client3.websocket.send = AsyncMock()
-        
-        server._clients = {
-            "client1": client1,
-            "client2": client2, 
-            "client3": client3
-        }
-        
+
+        server._clients = {"client1": client1, "client2": client2, "client3": client3}
+
         # Broadcast to specific clients only
         await server.broadcast_notification(
             method="targeted_notification",
             params={"message": "test"},
-            target_clients=["client1", "client3"]
-        )
-        
+            target_clients=["client1", "client3"],
+        )
+
         # Verify only targeted clients received notification
         client1.websocket.send.assert_called_once()
         client2.websocket.send.assert_not_called()  # Should not receive
         client3.websocket.send.assert_called_once()
 
@@ -295,9 +304,9 @@
         """Verify notification methods document API compliance."""
         # Test that notification methods have proper docstrings referencing API spec
         camera_notify_doc = server.notify_camera_status_update.__doc__
         assert "docs/api/json-rpc-methods.md" in camera_notify_doc
         assert "device, status, name, resolution, fps, streams" in camera_notify_doc
-        
+
         recording_notify_doc = server.notify_recording_status_update.__doc__
         assert "docs/api/json-rpc-methods.md" in recording_notify_doc
-        assert "device, status, filename, duration" in recording_notify_doc
\ No newline at end of file
+        assert "device, status, filename, duration" in recording_notify_doc
--- /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_websocket_server/test_server_status_aggregation.py	2025-08-03 19:27:38.813970+00:00
+++ /home/carlossprekelsen/Documentos/CameraRecorder/mediamtx-camera-service/tests/unit/test_websocket_server/test_server_status_aggregation.py	2025-08-04 15:35:26.803098+00:00
@@ -50,264 +50,286 @@
             host="localhost",
             port=8002,
             websocket_path="/ws",
             max_connections=100,
             mediamtx_controller=mock_mediamtx_controller,
-            camera_monitor=mock_camera_monitor
-        )
-
-    @pytest.mark.asyncio
-    async def test_get_camera_status_uses_real_capability_data(self, server, mock_camera_monitor, mock_mediamtx_controller):
+            camera_monitor=mock_camera_monitor,
+        )
+
+    @pytest.mark.asyncio
+    async def test_get_camera_status_uses_real_capability_data(
+        self, server, mock_camera_monitor, mock_mediamtx_controller
+    ):
         """Verify get_camera_status integrates real capability metadata when available."""
         # Setup connected cameras with real capability data
         mock_camera_device = CameraDevice(
-            device="/dev/video0",
-            name="Test Camera 0",
-            status="CONNECTED"
-        )
-        
+            device="/dev/video0", name="Test Camera 0", status="CONNECTED"
+        )
+
         mock_camera_monitor.get_connected_cameras.return_value = {
             "/dev/video0": mock_camera_device
         }
-        
+
         # Mock capability metadata with provisional/confirmed data
         mock_capability_metadata = {
             "resolution": "1280x720",
             "fps": 25,
             "validation_status": "confirmed",
             "formats": ["YUYV", "MJPEG"],
             "all_resolutions": ["1920x1080", "1280x720", "640x480"],
-            "consecutive_successes": 3
-        }
-        mock_camera_monitor.get_effective_capability_metadata.return_value = mock_capability_metadata
-        
+            "consecutive_successes": 3,
+        }
+        mock_camera_monitor.get_effective_capability_metadata.return_value = (
+            mock_capability_metadata
+        )
+
         # Mock MediaMTX stream status
         mock_mediamtx_controller.get_stream_status.return_value = {
             "status": "active",
             "bytes_sent": 12345,
-            "readers": 2
-        }
-        
+            "readers": 2,
+        }
+
         # Test get_camera_status method
         result = await server._method_get_camera_status({"device": "/dev/video0"})
-        
+
         # Verify real capability data is used, not defaults
         assert result["resolution"] == "1280x720"  # From capability detection
-        assert result["fps"] == 25                 # From capability detection
+        assert result["fps"] == 25  # From capability detection
         assert result["status"] == "CONNECTED"
         assert result["name"] == "Test Camera 0"
-        
+
         # Verify capabilities section populated with real data
         assert result["capabilities"]["formats"] == ["YUYV", "MJPEG"]
-        assert result["capabilities"]["resolutions"] == ["1920x1080", "1280x720", "640x480"]
-        
+        assert result["capabilities"]["resolutions"] == [
+            "1920x1080",
+            "1280x720",
+            "640x480",
+        ]
+
         # Verify MediaMTX integration
         assert result["metrics"]["bytes_sent"] == 12345
         assert result["metrics"]["readers"] == 2
         assert result["streams"]["rtsp"] == "rtsp://localhost:8554/camera0"
-        
+
         # Verify method calls
         mock_camera_monitor.get_connected_cameras.assert_called_once()
-        mock_camera_monitor.get_effective_capability_metadata.assert_called_once_with("/dev/video0")
+        mock_camera_monitor.get_effective_capability_metadata.assert_called_once_with(
+            "/dev/video0"
+        )
         mock_mediamtx_controller.get_stream_status.assert_called_once_with("camera0")
 
     @pytest.mark.asyncio
-    async def test_get_camera_status_fallback_to_defaults(self, server, mock_camera_monitor):
+    async def test_get_camera_status_fallback_to_defaults(
+        self, server, mock_camera_monitor
+    ):
         """Verify graceful fallback when capability detection unavailable."""
         # Setup camera without capability detection support
         mock_camera_device = CameraDevice(
-            device="/dev/video0",
-            name="Test Camera 0", 
-            status="CONNECTED"
-        )
-        
+            device="/dev/video0", name="Test Camera 0", status="CONNECTED"
+        )
+
         mock_camera_monitor.get_connected_cameras.return_value = {
             "/dev/video0": mock_camera_device
         }
-        
+
         # Remove capability detection method to simulate unavailability
-        if hasattr(mock_camera_monitor, 'get_effective_capability_metadata'):
-            delattr(mock_camera_monitor, 'get_effective_capability_metadata')
-        
+        if hasattr(mock_camera_monitor, "get_effective_capability_metadata"):
+            delattr(mock_camera_monitor, "get_effective_capability_metadata")
+
         # Test get_camera_status method
         result = await server._method_get_camera_status({"device": "/dev/video0"})
-        
+
         # Verify architecture defaults are used
         assert result["resolution"] == "1920x1080"  # Architecture default
-        assert result["fps"] == 30                   # Architecture default
+        assert result["fps"] == 30  # Architecture default
         assert result["status"] == "CONNECTED"
         assert result["name"] == "Test Camera 0"
-        
+
         # Verify empty capabilities when detection unavailable
         assert result["capabilities"]["formats"] == []
         assert result["capabilities"]["resolutions"] == []
 
     @pytest.mark.asyncio
-    async def test_get_camera_list_capability_integration(self, server, mock_camera_monitor, mock_mediamtx_controller):
+    async def test_get_camera_list_capability_integration(
+        self, server, mock_camera_monitor, mock_mediamtx_controller
+    ):
         """Verify get_camera_list uses real capability data for resolution/fps."""
         # Setup multiple connected cameras with capability data
         mock_cameras = {
             "/dev/video0": CameraDevice("/dev/video0", "Camera 0", "CONNECTED"),
             "/dev/video1": CameraDevice("/dev/video1", "Camera 1", "CONNECTED"),
-            "/dev/video2": CameraDevice("/dev/video2", "Camera 2", "DISCONNECTED")
+            "/dev/video2": CameraDevice("/dev/video2", "Camera 2", "DISCONNECTED"),
         }
         mock_camera_monitor.get_connected_cameras.return_value = mock_cameras
-        
+
         # Mock capability metadata for different cameras
         def mock_get_capability_metadata(device_path):
             metadata_map = {
                 "/dev/video0": {
                     "resolution": "1920x1080",
                     "fps": 30,
-                    "validation_status": "confirmed"
+                    "validation_status": "confirmed",
                 },
                 "/dev/video1": {
-                    "resolution": "1280x720", 
+                    "resolution": "1280x720",
                     "fps": 15,
-                    "validation_status": "provisional"
+                    "validation_status": "provisional",
                 },
                 "/dev/video2": {
                     "resolution": "640x480",
                     "fps": 10,
-                    "validation_status": "none"
-                }
+                    "validation_status": "none",
+                },
             }
             return metadata_map.get(device_path, {})
-        
-        mock_camera_monitor.get_effective_capability_metadata.side_effect = mock_get_capability_metadata
-        
+
+        mock_camera_monitor.get_effective_capability_metadata.side_effect = (
+            mock_get_capability_metadata
+        )
+
         # Mock MediaMTX stream status
         mock_mediamtx_controller.get_stream_status.return_value = {"status": "active"}
-        
+
         # Test get_camera_list method
         result = await server._method_get_camera_list()
-        
+
         # Verify real capability data used in camera list
         cameras = result["cameras"]
         assert len(cameras) == 3
-        
+
         # Camera 0 - confirmed capability data
         camera0 = next(c for c in cameras if c["device"] == "/dev/video0")
         assert camera0["resolution"] == "1920x1080"
         assert camera0["fps"] == 30
         assert camera0["status"] == "CONNECTED"
-        
-        # Camera 1 - provisional capability data  
+
+        # Camera 1 - provisional capability data
         camera1 = next(c for c in cameras if c["device"] == "/dev/video1")
         assert camera1["resolution"] == "1280x720"
         assert camera1["fps"] == 15
         assert camera1["status"] == "CONNECTED"
-        
+
         # Camera 2 - disconnected camera
         camera2 = next(c for c in cameras if c["device"] == "/dev/video2")
         assert camera2["status"] == "DISCONNECTED"
-        
+
         # Verify summary counts
         assert result["total"] == 3
         assert result["connected"] == 2
 
-    @pytest.mark.asyncio 
-    async def test_get_camera_status_provisional_vs_confirmed_logic(self, server, mock_camera_monitor):
+    @pytest.mark.asyncio
+    async def test_get_camera_status_provisional_vs_confirmed_logic(
+        self, server, mock_camera_monitor
+    ):
         """Test that provisional and confirmed capability data are handled correctly."""
         # Setup camera with provisional capability data
         mock_camera_device = CameraDevice("/dev/video0", "Test Camera", "CONNECTED")
-        mock_camera_monitor.get_connected_cameras.return_value = {"/dev/video0": mock_camera_device}
-        
+        mock_camera_monitor.get_connected_cameras.return_value = {
+            "/dev/video0": mock_camera_device
+        }
+
         # Test provisional capability data
         provisional_metadata = {
             "resolution": "1280x720",
             "fps": 25,
             "validation_status": "provisional",
             "consecutive_successes": 1,
-            "formats": ["YUYV"]
-        }
-        mock_camera_monitor.get_effective_capability_metadata.return_value = provisional_metadata
-        
+            "formats": ["YUYV"],
+        }
+        mock_camera_monitor.get_effective_capability_metadata.return_value = (
+            provisional_metadata
+        )
+
         result = await server._method_get_camera_status({"device": "/dev/video0"})
         assert result["resolution"] == "1280x720"
         assert result["fps"] == 25
-        
+
         # Test confirmed capability data
         confirmed_metadata = {
-            "resolution": "1920x1080", 
+            "resolution": "1920x1080",
             "fps": 30,
             "validation_status": "confirmed",
             "consecutive_successes": 5,
-            "formats": ["YUYV", "MJPEG"]
-        }
-        mock_camera_monitor.get_effective_capability_metadata.return_value = confirmed_metadata
-        
+            "formats": ["YUYV", "MJPEG"],
+        }
+        mock_camera_monitor.get_effective_capability_metadata.return_value = (
+            confirmed_metadata
+        )
+
         result = await server._method_get_camera_status({"device": "/dev/video0"})
         assert result["resolution"] == "1920x1080"
         assert result["fps"] == 30
 
     @pytest.mark.asyncio
     async def test_graceful_degradation_missing_camera_monitor(self, server):
         """Verify methods handle missing camera_monitor gracefully."""
         # Remove camera monitor to simulate unavailability
         server._camera_monitor = None
-        
+
         # Test get_camera_list with missing camera monitor
         result = await server._method_get_camera_list()
-        assert result == {
-            "cameras": [],
-            "total": 0,
-            "connected": 0
-        }
-        
+        assert result == {"cameras": [], "total": 0, "connected": 0}
+
         # Test get_camera_status with missing camera monitor
         result = await server._method_get_camera_status({"device": "/dev/video0"})
         assert result["status"] == "DISCONNECTED"
         assert result["resolution"] == "1920x1080"  # Architecture default
-        assert result["fps"] == 30                   # Architecture default
-
-    @pytest.mark.asyncio
-    async def test_graceful_degradation_missing_mediamtx_controller(self, server, mock_camera_monitor):
+        assert result["fps"] == 30  # Architecture default
+
+    @pytest.mark.asyncio
+    async def test_graceful_degradation_missing_mediamtx_controller(
+        self, server, mock_camera_monitor
+    ):
         """Verify methods handle missing MediaMTX controller gracefully."""
         # Setup camera monitor but remove MediaMTX controller
         mock_camera_device = CameraDevice("/dev/video0", "Test Camera", "CONNECTED")
-        mock_camera_monitor.get_connected_cameras.return_value = {"/dev/video0": mock_camera_device}
+        mock_camera_monitor.get_connected_cameras.return_value = {
+            "/dev/video0": mock_camera_device
+        }
         server._mediamtx_controller = None
-        
+
         # Test get_camera_status without MediaMTX controller
         result = await server._method_get_camera_status({"device": "/dev/video0"})
-        
+
         # Should still return camera data without stream info
         assert result["status"] == "CONNECTED"
         assert result["streams"] == {}  # No stream URLs without MediaMTX
         assert result["metrics"]["bytes_sent"] == 0
         assert result["metrics"]["readers"] == 0
 
     @pytest.mark.asyncio
     async def test_camera_status_error_handling(self, server, mock_camera_monitor):
         """Test error handling in camera status aggregation."""
         # Setup camera monitor to raise exception
-        mock_camera_monitor.get_connected_cameras.side_effect = Exception("Camera monitor error")
-        
+        mock_camera_monitor.get_connected_cameras.side_effect = Exception(
+            "Camera monitor error"
+        )
+
         # Test get_camera_status with exception
         result = await server._method_get_camera_status({"device": "/dev/video0"})
         assert result["status"] == "ERROR"
         assert result["device"] == "/dev/video0"
 
     def test_missing_device_parameter_validation(self, server):
         """Test validation of required device parameter."""
         # Test get_camera_status without device parameter
         with pytest.raises(ValueError, match="device parameter is required"):
             asyncio.run(server._method_get_camera_status({}))
-        
+
         with pytest.raises(ValueError, match="device parameter is required"):
             asyncio.run(server._method_get_camera_status(None))
 
     @pytest.mark.asyncio
     async def test_stream_name_generation_from_device_path(self, server):
         """Test stream name generation for various device path formats."""
         # Test standard device paths
         assert server._get_stream_name_from_device_path("/dev/video0") == "camera0"
         assert server._get_stream_name_from_device_path("/dev/video15") == "camera15"
-        
+
         # Test non-standard paths
         stream_name = server._get_stream_name_from_device_path("/custom/device/path")
         assert stream_name.startswith("camera_")  # Should generate hash-based name
-        
+
         # Test error handling
         stream_name = server._get_stream_name_from_device_path("")
-        assert stream_name == "camera_unknown"
\ No newline at end of file
+        assert stream_name == "camera_unknown"
